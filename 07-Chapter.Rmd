# Interpolating spatial data

**"Compassion isn't about being nice, it's about reducing suffering for others."** -- April Wensel

## Introduction

Observations of the natural world are made at specific locations in space (and time). But we often want estimates of the observed values everywhere. This is the case when the observations are taken from a continuous field (surface). Data observed or measured at locations across a continuous field are called geostatistical data. Examples: concentrations of heavy metals across a farm field, surface air pressures measured by barometers at cities across the country, minimum air temperature values across the city on a clear, calm night.

Local averaging or spline functions are typically used for interpolation. If it is 20C here and 30C ten miles to the south, then it is 25C five miles to the south. That is a reasonable first-order assumption. But, these methods do not (1) take into account spatial autocorrelation and (2) do not estimate uncertainty about the interpolated values.

Kriging is statistical interpolation (usually spatial). It is the centerpiece of what is called 'geostatistics.' The resulting surface (kriged surface) is composed of three components. (1) Spatial trend: an increase or decrease in the values that depends on direction or a covariate (co-kriging); (2) Local spatial autocorrelation. (3) Random variation. Together the three components provide a model that is used to estimate values at any point in a specified domain.

Geostatistics is used to (1) quantify spatial correlation, (2) predict values at specific locations, (3) provide an estimate of uncertainty on the predicted values, and (4) simulation.

As we've done with areal averaged data and point pattern data (Moran's I, Ripley's K, etc), we begin with understanding how to quantify spatial autocorrelation. In geostatistics, this involves the covariance function, the correlogram function, and the variogram.

### Continuity and stationarity

* Statistical interpolation assumes the observed values are spatially homogeneous. This implies stationarity and continuity.
* Stationarity implies that the average difference in values between pairs of observations separated by a given distance (lag) is constant across the domain. 
* Continuity implies that the spatial autocorrelation depends only on the lag (and orientation) between observations. That is; spatial autocorrelation is independent of location.
* Stationarity and continuity allow different parts of the domain to be treated as "independent" samples. 
* Spatial autocorrelation can be described by a single parametric function. 

Stationarity can be weak or intrinsic. Both assume the average of the difference in values at observations separated by a lag distance $h$ is zero. That is, E$[z_i - z_j]$ = 0, where location $i$ and location $j$ are a (lag) distance $h$ apart. This implies that the interpolated surface $Z(s)$ is a random function with a constant mean ($m$) and a residual ($\varepsilon$).
$$
Z(s) = m + \varepsilon(s).
$$
The expected value (average across all values) in the domain is $m$.

Weak stationarity assumes the covariance is a function of the lag distance $h$.
$$
\hbox{cov}(z_i, z_j) = \hbox{cov}(h)
$$
where cov($h$) is called the covariogram.

Intrinsic stationarity assumes the variance of the difference in values is a function of the lag: 
$$
\hbox{var}(z_i - z_j) = \gamma(h),
$$
where $\gamma(h)$ is called the variogram. This means that the variance of $Z$ is constant and that spatial correlation is independent of location.

These assumptions are needed to get started with statistical interpolation.

### Covariogram and correlogram

Our interest will be on a parametric model for the variogram $\gamma(h)$. But to help understand the variogram, let us first consider the covariogram. 

To make things simple but with no loss in generality, we start with a 4 x 6 map of surface air temperatures in degrees C.

  21  21  20  19  18  19 
  
  26  25  26  27  29  28 
  
  32  33  34  35  30  28   
  
  34  35  35  36  32  31   

Put the values into a data vector and determine the mean and variance.
```{r chapter7}
temps <- c(21, 21, 20, 19, 18, 19, 
           26, 25, 26, 27, 29, 28, 
           32, 33, 34, 35, 30, 28, 
           34, 35, 35, 36, 32, 31)
mean(temps); var(temps)
```

To start, we focus only on the north-south direction. To compute the sample covariance function we first compute the covariance between the observed values one distance unit apart.

Mathematically
$$
\hbox{cov}(0, 1) = 1/|N(1)| \sum (z_i - Z)(z_j - Z)
$$
where $|N(1)|$ is the number of distinct observation pairs with a distance separation of one unit in the north-south direction and where $Z$ is the average over all observations. Here we let zero in the cov(0, 1) refer to the direction and the one to the distance of one unit apart. Here $|N(1)|$ = 18.

The equation for the covariance can be simplified to:
$$
\hbox{cov}(0, 1) = 1/|N(1)| \sum z_i z_j - m_{-1} m_{+1}
$$
where $m_{-1}$ is the average temperature over all rows except the first (northern most) and $m_{+1}$ is the average temperature over all rows except the last (southern most).

To simplify the notation we re-index the grid of temperatures using lexicographic (reading) order.

 1   2   3   4   5   6
 
 7   8   9   10  11  12 
 
 13  14  15  16  17  18  
  
 19  20  21  22  23  24 

Then
```{r}
mp1 <- mean(temps[1:18])
mm1 <- mean(temps[7:24])
cc <- sum(temps[1:18] * temps[7:24])/18
cc - mm1 * mp1
```

Or more generally
```{r}
N <- 18
k <- 1:N
1/N * sum(temps[k] * temps[k + 6]) - mean(temps[k]) * mean(temps[k + 6])
```

The covariance has units of the field variable squared (here $^\circ C^2$).

We also have observation pairs two units of distance apart. So we compute the cov(0, 2) in a similar way. 
$$
\hbox{cov}(0, 2) = 1/|N(2)| \sum z_i z_j - m_{-2} m_{+2}
$$
where $m_{-2}$ is the average temperature over all rows except the first two and $m_{+2}$ is the average temperature over all rows except the last two. $|N(2)|$ is the number of pairs two units apart.
```{r}
N <- 12
k <- 1:N
1/N * sum(temps[k] * temps[k + 12]) - mean(temps[k]) * mean(temps[k + 12])
```

Similarly we have observation pairs three units apart so we compute cov(0, 3) as
$$
\hbox{cov}(0, 3) = 1/|N(3)| \sum z_i z_j - m_{-3} m_{+3}
$$
```{r}
N <- 6
k <- 1:N
1/N * sum(temps[k] * temps[k + 18]) - mean(temps[k]) * mean(temps[k + 18])
```

There are no observation pairs four units apart in the north-south direction so we are finished. The covariogram is a plot of the covariance values as a function of lagged distance. Let h be the lagged distance, then

h      |  cov(h)  
-------|--------  
(0, 1) |  15  
(0, 2) |   3  
(0, 3) |   1  

It is convenient to have a measure of co-variability that is dimensionless. This is obtained by dividing the covariance at lagged distance $h$ by the covariance at lag zero. This is the correlogram. Values of the correlogram range from 0 to +1.

### Variogram

The covariogram is a decreasing function of lag. The variogram is the multiplicative inverse of the covariogram. 

Mathematically: var($z_i - z_j$) for locations i and j. The semivariogram is 1/2 the variogram. If location i is near location j, the difference in the values will be small and so too will the variance of their differences, in general. If location i is far from location j, the difference in values will be large and so too will the variance of their differences.

In practice we have a set of observations and we compute a variogram. We call this the sample (or empirical) variogram. Let $t_i = (x_i, y_i)$  be the ith location and $h_{i,j} = t_j - t_i$ be the vector connecting location $t_i$ with location $t_j$. Then the sample variogram is defined as
$$
\gamma(h) = \frac{1}{2N(h)} \sum^{N(h)} (z_i - z_j)^2
$$
where $N(h)$ is the number of observation pairs a distance of $h$ units apart.

The variogram assumes intrinsic stationarity so the values need to be detrended first.

The sample variogram is characterized by a set of points the values of which generally increase as $h$ increases before leveling off (reaching a plateau).

### Terminology

Let's begin with a plot with labels. Make sure the {geoR} package is installed. The code is done using the base graphics commands and the plot method from the package.
```{r}
library(geoR)
plot(variog(s100, max.dist = 1), 
     xlab = "Lagged Distance (h)",
     ylab = expression(paste(gamma,"(h)")), 
     las = 1, pch = 16)
abline(h = 0)
abline(h = .15, col = "red")
arrows(0, 0, x1 = 0, y1 = .15, length = .1)
text(0, y = .05, labels = "nugget", pos=4)
abline(h = .9, col = "red")
arrows(0, .15, x1 = 0, y1 = .9, length = .1)
text(0, y = .8, labels="sill (partial sill)", pos=4)
abline(v = .6, col = "red")
arrows(0, 1, x1 = .6, y1 = 1, length = .1)
text(.4, y = 1.04, labels = "range")
```

* Lag (lag distance): Relative distance between observation locations.
* Nugget (nugget, nugget variance, or nugget effect): The height of the variogram at zero lag. The nugget is the variation in the values at the observation locations without regard to spatial variation. Related to the observation (or measurement) precision.
* Sill: The height of the variogram at which the values are uncorrelated.
* Relative nugget effect: The ratio of the nugget to the sill expressed as a percentage.
* Range: The distance beyond which the values are uncorrelated. The range is indicated on the empirical variogram as the position along the horizontal axis where values of the variogram reach a constant height.

Additional terms.
* Isotropy: The condition in which spatial correlation is the same in all directions.
* Anisotropy: (an-I-so-trop-y) spatial correlation is stronger or more persistent in some directions.
* Directional variogram: Distance and direction are important in characterizing the spatial correlations. Otherwise the variogram is called omni-directional.
* Azimuth ($\theta$): Defines the direction of the variogram in degrees.The azimuth is measured clockwise from north.
* Lag spacing: The distance between successive lags is called the lag spacing or lag increment.
* Lag tolerance: The distance allowable for observational pairs at a specified lag. With arbitrary observation locations there will be no observations exactly a lag distance from any observation. Lag tolerance provides a range of distances to be used for computing values of the variogram at a specified lag.

### Variogram Models

Computing the sample variogram is the first step in modeling geostatistical data. The next step is to fit a model to these variogram estimates. We replace a scatter of points with a statistical model. 

The model is important since the sample variogram is made only at specified lag distances (with specified lag tolerance and azimuth). We need a continuous function that varies smoothly across all lags.

Variogram models come from different families. The decision includes (1) choosing the model family and (2) determining the parameters: nugget, sill, and range.

The exponential model family reaches the sill asymptotically. The range (a) is defined as the lag distance at which gamma reaches 95% of the sill.
```{r}
c0 <- .1
c1 <- 2.1
a <- 1.3
curve(c0 + c1*(1 - exp(-3*x/a)), 
      from = .01, to = 3, 
      xlab = "h", 
      ylab = expression(paste(hat(gamma), "(h)")), 
      las = 1)
```

The spherical model family is piece wise reaching the sill at x = 1 (here).
```{r}
curve(c0 + c1*(3*x/2 - x^3/2),
      from = .01, to = 1,
      xlab = "h",
      ylab = expression(paste(hat(gamma), "(h)")), 
      las = 1)
```

The Gaussian model family is sigmoidal. It is used when the data exhibit strong correlations at the shortest lag distances.  The inflection point of the model occurs at $\sqrt{a/6}$.
```{r}
curve(c0 + c1*(1 - exp(-3*x^2/a^2)),
      from = .01, to = 3, 
      xlab = "h", 
      ylab = expression(paste(hat(gamma), "(h)")),
      las = 1)
```

Other families include

* Linear models: $\hat \gamma(h)$ = c0 + b * h.
* Power models:  $\hat \gamma(h)$ = c0 + b * h$^\lambda$.

These models have no sill.

Choosing a variogram family is largely done by eyeballing the shape of the sample variogram. Then, given a sample variogram computed from a set of spatial observations and a choice of family, the parameters of the variogram model are determined by a weighted least-squares (WLS) algorithm. There are other ways to determine the parameters including by eye, and by the method of maximum likelihoods, but WLS is less erratic than other methods and it requires fewer assumptions about the distribution of the data.

### Kriging

The final step is kriging. Kriging interpolates the observed data using the variogram model. It was developed by a South African miner (D.G. Krige) as a way to improve estimates of where ore reserves might be located. Extraction costs are reduced substantially if good predictions can be made of where the ore resides given samples taken across the mine.

A kriged estimate is a weighted average of the observations where the weights are based on the variogram model. The kriged estimates are optimal in the sense that they minimize the error variance. The type of kriging depends on the characteristics of the observations and the purpose of interpolation.

* Simple kriging assumes a known constant mean for the domain.  
* Ordinary kriging assumes an unknown constant mean.  
* Universal kriging assumes an unknown linear or nonlinear trend in the mean.  

The steps are:

1. Examine the observations for trends and isotropy.
2. Compute an empirical variogram.
3. Fit a variogram model to the empirical variogram.
4. Create an interpolated surface using kriging.

## Spatial statistical interpolation using functions from the {geoR} package

The {geoR} package has functions for doing geostatistics. There are others but it was one of the first. It is useful for learning how things work. 

Suppose we have the following set of observations (`zobs`) at locations (`sx`, `sy`).
```{r}
sx <- c(1.1, 3.2, 2.1, 4.9, 5.5, 7, 7.8, 9, 2.3, 6.9)
sy <- c(3, 3.5, 6, 1.5, 5.5, 3.2, 1, 4.5, 1, 7)
zobs <- c(-0.6117, -2.4232, -0.42, -0.2522, -2.0362, 0.9814, 1.842,
         0.1723, -0.0811, -0.3896)
```

Create a data frame and plot the observed values at the locations using the `geom_text()` function.
```{r}
df <- data.frame(sx, sy, zobs)

library(ggplot2)

ggplot(df, aes(x = sx, y = sy, label = zobs)) +
  geom_text() +
  theme_minimal()
```

Lag distance (distance between locations) is the independent variable in the variogram function. We get all pairwise distances by applying the `dist()` function to a matrix of spatial coordinates.
```{r}
dist(cbind(sx, sy))
max(dist(cbind(sx, sy)))
min(dist(cbind(sx, sy)))
```

The function computes a pairwise distance matrix. The distance between the first and second observation is 2.16 units and so on. The largest lag distance is 8.04 units and the smallest lag distance is 2.05 units.

The functions in the {geoR} package work with objects of class `geodata`. Thus we first need to convert the data frame of observations and locations into a `geodata` object.

This is done with the `as.geodata()` function. The default for the function is to assume that the first two columns of the data frame contain the coordinates and the third column contains the observed data values. This is the way we constructed the data frame so we can use the defaults.
```{r}
library(geoR)

gdf <- as.geodata(df)
str(gdf)
```

An object of class `geodata` contains two lists: the coordinates of the locations and the observed values as `data`. It may contain other elements like coordinate boundaries but unlike working with `ppp` objects with functions from the {spatstat} package a boundary defining the domain is not required.

For an arbitrary data frame we can specify where the coordinate and data columns by column numbers. For example, if the location coordinates are in columns five and six and the observed values are in column eight then use `coords.col = c(5, 6)` and `data.col = 8`.

With the class set to `geodata` methods like `summary` and `plot` provide attribute and spatial information about the observations. For example the `summary()` method outputs the number of observations, a summary of the coordinates and a summary of the observed values.
```{r}
summary(gdf)
```

We access the coordinates and the data using the `$` operator.
```{r}
gdf$coords
gdf$data
```

The `plot()` method produces a four panel plot.
```{r}
plot(gdf)
```

The upper left panel is a map with the locations of the observations given by colors and symbols according to their values. Green triangles have the smallest values and red crosses have the largest values. We can see an upward trend in values from northwest to southeast.

This trend is decomposed in the upper right and lower left panels. The upper-right panel shows the north-south (Y Coord) coordinate plotted against the data values. As the data values increase to the right the north-south coordinate decreases (smaller data values are in the north). The lower-left panel shows the data plotted against the east-west (X Coord). Moving from west to east we see the data values tend to increase.

The lower right panel is a non-spatial distribution of the data values shown with a histogram, and density curve, and a rug plot.

To plot the trend in the east-west direction, type
```{r}
ggplot(df, aes(x = sx, y = zobs)) + 
  geom_point() +
  geom_smooth(method = lm, se = FALSE) +
  theme_minimal()
```

The data can be plotted with the trend removed. The `trend = "1st"` refers to a linear (first-order) trend.
```{r}
plot(gdf, trend = "1st")
```

Now the plots show the residuals from the first-order trend model.

As another example, consider the dataset called `topo` from the {MASS} package. The data are topographic heights (feet) within a 310 sq ft domain.
```{r}
library(MASS)

data(topo)
topo.gdf <- as.geodata(topo)
plot(topo.gdf)
```

Note the trend in the north-south direction and the skewness in the observed values. 

Examine the residuals after removing a first-order trend from the observations.
```{r}
plot(topo.gdf, trend = "1st")
```

The north-south trend is removed and the observations have a more symmetric distribution. There appears to be some non-linear trend in the east-west direction. 

Examine the residuals after removing a second-order trend.
```{r}
plot(topo.gdf, trend = "2nd")
```

The residuals from a second-order polynomial fit are symmetric and the trends are gone. However, the residuals appear to show spatial autocorrelations (areas with above and below residuals). 

### Empirical variograms

Consider the dataset `s100` available from the {geoR} package. The `points.geodata()` function produces a bubble plot showing the locations of the observations and the relative magnitude of the `z` variable.
```{r}
data(s100)
points.geodata(s100) 
```

The `variog()` function computes the empirical variogram. Here we save it in the object `s100.v`.
```{r}
s100.v <- variog(s100)
str(s100.v)
```

Information in the variogram object includes the lag distances (`u`), the values of the variogram at those distances (`v`), the number of distance pairs used to compute the variogram values at each lag (`n`), the standard deviation of the variogram values, and the coefficients of the trend surface (with no trend, the value is the overall mean of the data) (`beta.ols`) among other information.

Note: Mathematically lag distance is denoted with $h$ but in the {geoR} package it is `u`.

Verify the mean value.
```{r}
mean(s100$data)
```

Plot the variogram.
```{r}
plot(s100.v)
```

The semivariance ($\gamma(u)$) is plotted against lag distance ($u$). Values increase with increasing lag until a lag distance of about 1. 

At large lags there are fewer estimates so the values have greater variance. A model for the semivariance is fit only for the the increasing portion of the graph.

Another example: variogram of the `topo` dataset.
```{r}
topo.v <- variog(topo.gdf)
plot(topo.v, xlab = "Lagged Distance", 
     ylab = expression(paste(gamma, "(u) [ft", {}^2, "]")),
     las = 1, pch = 16)
```

The variogram values have units of square feet and are calculated using point pairs at lag distances within a lag tolerance. The number of point pairs depends on the lag so the variogram values are less precise at large distance.

Plot the number of point pairs used as a function of lag distance.
```{r}
ggplot(data.frame(u = topo.v$u, n = topo.v$n), aes(x = u, y = n)) +
  geom_point() +
  xlab("Lag Distance") + ylab("Number of Observation Pairs") +
  theme_minimal()
```

### Wolfcamp aquifer data

Some years ago there were three nuclear waste repository sites being proposed in Nevada, Texas, and Washington. The site needs to be larger enough for more than 68,000 high-level waste containers placed underground, about 9 m (~30 feet) apart, in trenches surrounded by salt. In July of 2002 the Congress approved [Yucca Mountain](https://en.wikipedia.org/wiki/Yucca_Mountain_nuclear_waste_repository), Nevada, as the nation’s first long-term geological repository for spent nuclear fuel and high-level radioactive waste.

The site must isolate the waste for 10,000 years. Leaks could occur, however, or radioactive heat could cause tiny quantities of water in the salt to migrate toward the heat until eventually each canister is surrounded by 22.5 liters of water (~6 gallons). A chemical reaction of salt and water can create hydrochloric acid that might corrode the canisters.

The piezometric-head data at the site were obtained by drilling a narrow pipe into the aquifer and letting water seeks its own level in the pipe (piezometer). 

The head measurements, given in units of feet above sea level, are from drill stem tests and indicate the total energy of the water in units of height. The higher the head height, the greater the potential energy. 

Water flows away from areas of high potential energy so aquifer discharge is proportional to the gradient of the piezometric head.

The data are in `wolfcamp.csv` on my website.

#### Step 1: Examine the observed data for trends, check for normality

Get the data.
```{r}
L <- "http://myweb.fsu.edu/jelsner/temp/data/wolfcamp.csv"
wca.df <- read.csv(L, header = TRUE)
```

Create a simple feature data frame and then map the locations and head heights.
```{r}
library(sf)
wca.sf <- st_as_sf(x = wca.df, 
                   coords = c("lon", "lat"),
                   crs = "+proj=longlat +datum=WGS84")

library(mapview)
mapView(wca.sf, 
        zcol = "head")
```

Convert the data frame to a `geodata` object. Note there is no method to do this from a simple feature.
```{r}
wca.gdf <- as.geodata(wca.df, 
                      coords.col = 1:2, 
                      data.col = 3)
```

Find the duplicate location(s).
```{r}
dup.coords(wca.gdf)
```

The locations are the same, but the data values are different. This may represent an error or multiple measurements at this one location.  

We can average the values or we can exclude a row. Here we remove the row 30 observation.
```{r}
wca.gdf <- as.geodata(wca.df[-30, ], 
                      coords.col = 1:2, 
                      data.col = 3)
summary(wca.gdf)
```

There are 84 well sites. The spatial bounding box is given under coordinates summary. The minimum distance between sites is .01 degrees and the maximum distance is 4.6 degrees.

The data values are summarized. The values are piezometric head heights in units of feet.

The `plot()` method for `geodata` provides a panel of plots with information about the data useful for modeling them.
```{r}
plot(wca.gdf)
```

The upper left panel is a graph of the observed locations with symbols reflecting quartiles of the observed piezometric head heights. There is a clear trend in the data with the highest potential energy over the southwest (red crosses) and lowest over the northeast (blue circles). 

The nature of this trend can be seen in the upper right and lower left panels. The upper right panel plots the data against the y coordinate and the lower left panel plots the data against the x coordinate. Both graphs indicate a linear trend.

A histogram of the head heights is shown in the lower right panel. The rug locates the values along the data axis. The data are bi-modal and skewed to the right.

Repeat the plot after removing the 1st order trend. What happens?
```{r}
plot(wca.gdf, trend = "1st")
```

With the trend removed the variation of values appears to be roughly symmetric and the high and low values are mixed. However, there is some spatial grouping to the residuals.

#### Step 2: Compute empirical variograms

We noted the maximum distance between any two locations is 4.6 degrees. It is a good idea to plot the variogram values for distances only between 0 and about 1/2 the maximum distance.

Since there is a linear trend in the data over the spatial domain it is removed (trend argument) before computing the variogram values.
```{r}
plot(variog(wca.gdf, 
            trend = "1st", 
            max.dist = 2.3))
```

Here we see an increase in the variance with distance until about one degree, then the values fluctuate about a variance of about 41000 (ft$^2$).

What does the variogram look like if we do not first remove the trend?

This continuously increasing set of variances with little fluctuation about a best fit curve indicates a trend in the data that must be removed before the variogram is modeled. 

There are two sources of variation in the field values: trend and spatial correlation. Trends are modeled with smooth curves and correlations are modeled with the variogram.

The `variog()` function has options for specifying a classical or modulus estimator. By default the function uses the classical estimator. To override this, include the `estimator.type = "modulus"` argument. The modulus estimator of the variogram is more resistant to outliers in the data.

Compare the classical with the modulus variograms for the Wolfcamp aquifer data.
```{r}
par(mfrow = c(1, 2))
plot(variog(wca.gdf, trend = "1st", 
            max.dist = 2.3), 
     main = "Classical")
plot(variog(wca.gdf, trend  ="1st", 
            max.dist = 2.3, 
            estimator.type = "modulus"),
     main = "Modulus")
```

The two variograms are nearly identical.

### Variogram cloud

We can examine the makeup of the variogram in more detail using a 'variogram cloud'. Consider again the simulated data set `s100`.
```{r}
par(mfrow = c(1, 1))
plot(s100)
```

There appears to be a trend in the east-west direction with higher values in the east. This is seen clearly in the bottom left panel.

Remove this first-order trend.
```{r}
plot(s100, 
     trend = "1st")
```

We can see that the trend is adequately modeled with a 1st-order surface. Residuals appear symmetric about zero. 

A summary of the data indicates the maximum lagged distance of 1.3 so the `max.dist` argument is set to .6. 
```{r}
summary(s100)
```

Here we specify the eleven variogram estimates between 0 and .6 using the `uvec` argument.
```{r}
plot(variog(s100, 
            trend = "1st", 
            uvec = 11, 
            max.dist = .6))
```

We 'decompose' the variogram estimates with the `option = "cloud"`.
```{r}
c1 <- variog(s100, trend = "1st", 
             option = "cloud", 
             max.dist = .6)
plot(c1)
```

Each point on the plot is the difference (absolute value) between two observed values (y-axis) a lag distance apart (x-axis). There are 100 observations so there are 100 * 99/2 = 4950 possible points on this plot. Actually here only 3131 since we limit the distance to less than .6 degrees.

There is a tendency for more large differences as the lag increases but most differences are small for a given lag. The variability in observational differences is quite large especially for longer lag distances.

The variogram computes the sum of the squared differences as a function of lag distance grouped by a lag tolerance. Here we indirectly specify the lag tolerance with the `uvec` argument and compare the variogram with the variogram cloud.
```{r}
par(mfrow = c(1, 2))
v1 <- variog(s100, trend = "1st", 
            uvec = seq(0, .6, l = 11))
layout(matrix(c(1, 2), byrow = TRUE, ncol = 2), 
       respect = TRUE)
plot(v1); plot(c1)
```

Note the difference in the scale on the y-axis between the variogram cloud and the variogram is a factor of 10.

The variogram cloud can be grouped into lag distance classes (bins) and displayed with a box plot. This gives an idea of the general shape the variogram model should take.
```{r}
par(mfrow = c(1, 1))
b1 <- variog(s100, trend = "1st", 
            max.dist = .6, 
            bin.cloud = TRUE, 
            estimator.type = "classical")
plot(b1, bin.cloud = TRUE)
```

The box plot summarizes the squared differences for a given lag (and lag tolerance) with the median. There is a box plot for each lag but the lag distance is given as a bin number. 

This summary information helps us anticipate the type of variogram model. As an aside, note how to use the expression and paste functions to include symbols as part of the axis label.
```{r}
df <- data.frame(u = b1$u, v = b1$v)
ggplot(df, aes(u, v)) + 
  geom_point() + 
  geom_smooth(span = .8) +
  scale_y_continuous(limits = c(0, .6)) +
  ylab(expression(paste("Variogram [", gamma,"(h)]"))) +
  xlab("Lagged distance (h)")
```

The blue line is a least-squares regression smoother through the variogram estimates. The fact that it is not horizontal indicates spatial autocorrelation in the values that is separate from the first-order trend. The shape of the blue line gives an idea of the type of variogram family of models we should consider.

Now we can guess at a family for the variogram model and eyeball the parameters. Recall the plot from Lesson 18. This is called variography. We will discuss this in Lesson 20.

```{r}
plot(variog(s100, max.dist = 1), 
     xlab = "Lagged Distance (h)",
     ylab = expression(paste(gamma,"(h)")), 
     las = 1, pch = 16)
abline(h = 0)
abline(h = .15, col = "red")
arrows(0, 0, x1 = 0, y1 = .15, length = .1)
text(0, y = .05, labels = "nugget", pos=4)
abline(h = .9, col = "red")
arrows(0, .15, x1 = 0, y1 = .9, length = .1)
text(0, y = .8, labels="sill (partial sill)", pos=4)
abline(v = .6, col = "red")
arrows(0, 1, x1 = .6, y1 = 1, length = .1)
text(.4, y = 1.04, labels = "range")
```

* Lag (lag distance): Relative distance between observation locations.
* Nugget (nugget, nugget variance, or nugget effect): The height of the variogram at zero lag. The nugget is the variation in the values at the observation locations without regard to spatial variation. Related to the observation (or measurement) precision.
* Sill: The height of the variogram at which the values are uncorrelated.
* Relative nugget effect: The ratio of the nugget to the sill expressed as a percentage.
* Range: The distance beyond which the values are uncorrelated. The range is indicated on the empirical variogram as the position along the horizontal axis where values of the variogram reach a constant height.

### Directional variograms

Recall that the assumption of isotropy implies the same spatial autocorrelation function regardless of direction.

To examine the assumption of isotropy we compute variograms using observational pairs located along the same orientation. Instead of considering all observational pairs within a lag distance $h$ and lag tolerance $\delta h$, we consider only pairs within a wedge-shaped segment of this annulus.

This is done with the `variog()` function and specifying a `direction =` argument in units of radians. Radians are fractions of `pi`. We also need to specify the angle tolerance.

For example to compute and plot the variogram for the direction of 45 degrees (NE to SW) with a default tolerance of 22.5 degrees (default), type
```{r}
v45 <- variog(wca.gdf, 
              trend = "1st", 
              max.dist = 2.3, 
              direction = pi/4)
plot(v45)
```

Repeat for the other three quadrants (increments of $\pi$/4).
```{r}
par(mfrow = c(2, 2))
plot(variog(wca.gdf, trend = "1st", 
            max.dist = 2.3, direction = 0),
     main = expression(0 * degree))
plot(variog(wca.gdf, trend = "1st", 
            max.dist = 2.3, direction = pi/4),
     main = expression(45 * degree))
plot(variog(wca.gdf, trend = "1st", 
            max.dist = 2.3, direction = pi/2),
     main = expression(90 * degree))
plot(variog(wca.gdf, trend = "1st", 
            max.dist = 2.3, direction = 3 * pi/4),
     main = expression(135 * degree))
```

The variograms appear similar but it's difficult to tell since the vertical scales are not all the same. The `variog4()` function makes the comparison easier by plotting them on the same graph. It works like `variog()` but takes a vector of directions. The default directions are 0, 45, 90, and 135 degrees.
```{r}
par(mfrow = c(1, 1))
v4 <- variog4(wca.gdf, 
              trend = "1st",
              max.dist = 2.3)
plot(v4)
```

This plot makes it clear that the assumption of isotropy is reasonable.

#### Step 3: Fit a variogram model to the empirical variogram

Next we fit a curve through the set of points that make up the empirical variogram. The curve is called the variogram model. 

Since the `s100` data were generated using a variogram model, we already know the answer. The model is exponential with a sill (partial) of 1, a range of .3, and a nugget of 0.

Plot the empirical variogram values and overlay the curve corresponding to the exponential model with the `lines.variomodel()` function. The sill and range parameters are given in order using the `cov.pars` argument.
```{r}
plot(variog(s100, uvec = seq(0, 1, l = 21)))
lines.variomodel(cov.model = "exp", 
                 cov.pars = c(1, .3), nugget = 0, 
                 max.dist = 1.2, lwd = 2, col = "red")
```

Note that the practical range for the exponential model is the distance along the horizontal axis at which the curve reaches 95% of the sill which is 3 times the range specified in the `cov.pars` argument.

The `uvec =` argument allows control over the range of distance values and the number of variogram estimates. We specify the maximum distance in the `lines.variomodel()` function.

With observed data we don't know the model so we estimate the parameters using information from the empirical variogram.

* By eye: Trial and error over several models and parameter values. The `lines.variomodel()` function can help.  
* By least squares fit:  Using ordinary least squares (OLS) or weighted least squares (WLS) methods available through the `variofit()` function.  
* By maximum likelihood methods: Options for maximum likelihood (ML) and restricted maximum likelihood (REML) are available through the `likfit()` function.  

Try various curves and choosing the one that looks the best. Here we plot three models on top of the `s100` empirical variogram.
```{r}
plot(variog(s100, max.dist = 1))
lines.variomodel(cov.model = "exp", 
                 cov.pars = c(1, .3), 
                 nug = 0, max.dist = 1)
lines.variomodel(cov.model = "exp", 
                 cov.pars = c(1, .5),
                 nug = .1, max.dist = 1, 
                 col = "red")
lines.variomodel(cov.model = "sph", 
                 cov.pars = c(.8, .8),
                 nug = .1, max.dist = 1, 
                 col = "blue")
```

The black line is the model used to generate the `s100` dataset. The red line is based on the same exponential (exp) function but has a range of .5 and a nugget of .1. The blue line is based on a spherical function with a sill and range of .8 and a nugget of .1. All three models fit the points reasonably well. This is important: In practice the choice often makes little difference in the quality of the spatial interpolation.

A function to help with the fitting is `eyefit`. This opens a widget with radio buttons and slider bars making it easier to see how the model changes through various parameter sets. CAUTION: Mac users must have X11 installed. X11 is no longer included in OSX.
```{r, eval=FALSE}
eyefit(variog(s100))
```

#### Variogram model for the Wolfcamp aquifer data

Let's fit a variogram model to the Wolfcamp aquifer data. First plot the empirical variogram. We've seen that directional variograms are not needed.
```{r}
wca.v <- variog(wca.gdf, 
                trend = "1st", 
                max.dist = 2.3)
plot(wca.v)
```

Begin with a spherical model with a range of .8, a sill of 40000, and a nugget of 10000.  
```{r}
plot(wca.v)
lines.variomodel(cov.model = "sph", 
                 cov.pars = c(30000, .8),
                 nug = 10000, max.dist = 2.3)
```
 
In the `cov.pars` argument the first value is the "partial" sill and the second is the range. With the nugget set at 10000 and partial sill at 30000, the sill is 400000. The sill and nugget are variance measures so they have units of square feet. The range is distance so it has the corresponding spatial units.

The model looks reasonable. Perhaps the sill should be a bit higher, say 43000. We increase the partial sill accordingly,
```{r}
plot(wca.v)
lines.variomodel(cov.model="sph", 
                 cov.pars = c(33000, .8),
                 nug = 10000, max.dist = 2.3,
                 col = "red")
```

Better still. We try an exponential model with the same sill but the same range.
```{r}
plot(wca.v)
lines.variomodel(cov.model = "exp", 
                 cov.pars = c(33000, .8/3),
                 nug = 10000, max.dist = 2.3,
                 col = "green")
```

Not as good. The exponential model does not have the sharp turn at the sill. We settle on the spherical model with a sill of 43000, a range of .8, and a nugget of 10000.

#### Fine tune the parameter values

Next we tune the parameter estimates using the `variofit()` function. The function takes a set of initial parameter values and improves upon them using the method of weighted least squares. Alternatively we can use the `likfit()` function to fine tune the parameters. Here we use it to adjust the variogram model parameters estimated above. We save the model in the object `wca.vm`.
```{r}
wca.vm <- likfit(wca.gdf, 
                 trend = "1st", 
                 ini = c(33000, .8),
                 nug = 10000, 
                 cov.model = "spherical")
wca.vm
```

The function improves on the initial set of model parameters until the log-likelihood value is maximized. Any other set of parameters will produce a log likelihood value smaller than -553.  

The output includes values for the trend surface. It is a linear trend in two dimensions so it's represented by a plane with a single z-intercept value (`beta0`) and two slope values corresponding to the x (`beta1`) and y (`beta2`) directions.  

The units on the slope parameters are data units per unit spatial distance.  Thus the `beta1` value is -400 ft/deg longitude.  For every one deg longitude east, the piezometric head height decreases by 400 ft. We saw a trend in the SW-NE direction (exploratory plot). The `beta1` (`beta2`) value quantifies this slope in the east-west (north-south) direction.

The output also includes the parameter values for the nugget (`tausq`), the partial sill (`sigmasq`) and the range (`phi`).

Overlay the maximum likelihood solution to the spherical model by typing
```{r}
plot(variog(wca.gdf, 
            trend = "1st", 
            max.dist = 2.3))
lines.variomodel(cov.model = "sph", 
                 cov.pars = c(33000, .8),
                 nug = 10000, max.dist = 2.3,
                 col = "red")
lines(wca.vm, col = "blue")
```

We see the maximum likelihood solution is better at fitting the points at lag distances between .5 and 1. At these lags, the variogram estimates are most reliable as n (number of observation pairs) is largest.
```{r}
variog(wca.gdf)$n
```

The `likfit()` function is iterative and should find the same solution using somewhat different starting values. Try it using a partial sill of 30000 a nugget of 5000 and a range of 1. 
```{r}
likfit(wca.gdf, 
                 trend = "1st", 
                 ini = c(30000, 1),
                 nug = 5000, 
                 cov.model = "spherical")
```

A summary function on the `likfit` output provides more information about the fitted variogram model.
```{r}
summary(wca.vm)
```

The summary provides parameters of the trend model and values of the spatial model parameters. Importantly the summary gives the log likelihood along with the AIC and BIC values. It also gives those values for a non-spatial model. A non-spatial model in this context is the trend plus spatially uncorrelated random variation.

Since the log likelihood value is larger for the spatial model and the AIC and BIC values are smaller, it is clear that a spatial model is better than the non-spatial model.

#### Profile likelihood

The `proflik()` function gives a matrix of log likelihood values for a range of model parameters. The matrix can then be plotted to get a synoptic view of the relationship between the parameters and the likelihood. This can take a few seconds to compute.
```{r}
prof <- proflik(wca.vm, geodata = wca.gdf, 
                sill.val = seq(30000, 60000, length = 6),
                range.val = seq(.3, 2.3, length = 6), 
                nug.val = 10000, uni.only = FALSE)
plot(prof, nlevels = 16)
```

The horizontal axis is the partial sill ($\sigma^2$) and the vertical axis is the range ($\phi$). The nugget is set at 10000. The best fit parameters are indicated by the circle. At this location the log-likelihood is maximized. The likelihood is near the maximum for a broad set of range and partial sill values.

The marginal profiles are also available and can be plotted alongside the contour plot using
```{r}
par(mfrow = c(1, 3))
plot(prof, nlevels = 8)
```

### Another example: A variogram model for April temperatures in the Midwest

Let's finish with another example.

The data in `MidwestTemps.txt` are average temperatures in and around the state of Iowa for the month of April.
```{r}
L <- "http://myweb.fsu.edu/jelsner/temp/data/MidwestTemps.txt"
t.df <- read.table(L, header = TRUE)
summary(t.df)
```

Map the data values. Create a `geodata` object. Plot the empirical variogram using the residuals from a 2nd-order trend model. Fit an exponential model to the variogram using the `likfit()` function and starting values of 3 for the sill and 4 for the range.

Map the values.
```{r}
library(tmap)

t.sf <- st_as_sf(x = t.df, 
                 coords = c("lon", "lat"),
                 crs = "+proj=longlat +datum=WGS84")

library(USAboundaries)

sts <- us_states()

tm_shape(t.sf) +
  tm_text(text = "temp", size = .6) +
tm_shape(sts) +
  tm_borders() 
```

Create a `geodata` object from the data frame.
```{r}
t.gdf <- as.geodata(t.df)
summary(t.gdf)
plot(t.gdf)
```

Remove the trends and examine the residuals.
```{r}
plot(t.gdf, trend = "1st")
plot(t.gdf, trend = "2nd")
```

Plot the empirical variogram.
```{r}
plot(variog(t.gdf, 
            trend = "2nd", 
            max.dist = 5.5))
```

Fit a variogram model.
```{r}
iv <- c(3, 4)  

( t.vm <- likfit(t.gdf, 
                 trend = "2nd",
                 ini = iv, 
                 cov.model = "exp") )
```

The beta values refer to the second order trend. The nugget is `tausq` the partial sill is `sigmasq` and the range is `phi`.

```{r}
plot(variog(t.gdf, 
            trend = "2nd", 
            max.dist = 5.5))
lines(t.vm, col = "blue")
```

## Example 1: Wolfcamp Aquifer head heights

We've been examining piezometric head heights from the Wolfcamp Aquifer. Head heights measurements indicate the potential energy of the water in units of height.

The data are in `wolfcamp.csv` on my website.
```{r}
L <- "http://myweb.fsu.edu/jelsner/temp/data/wolfcamp.csv"
wca.df <- read.csv(L, header = TRUE)
```

### Step 1: Examine the data for trends and isotropy

Convert the data frame to a `geodata` object. The duplicate measurement is removed with the subset operator when converting from a `data.frame` to a `geodata` object.
```{r}
wca.gdf <- as.geodata(wca.df[-30, ], 
                      coords.col = 1:2, 
                      data.col = 3)
```

Plot the residuals after removing the 1st-order trend.
```{r}
plot(wca.gdf, trend = "1st")
```

### Step 2: Compute empirical variograms

To compute and plot the variogram on the residuals from a 1st-order trend type
```{r}
wca.v <- variog(wca.gdf, 
                trend = "1st",
                max.dist = 2.3)
plot(wca.v)
```

### Step 3: Fit a variogram model

Eyeball the initial values for the nugget, partial sill and range. Then use the `likfit()` function together with the initial estimates to get a variogram model. Here we assign the model to the object `wca.vm`.
```{r}
wca.vm <- likfit(wca.gdf, 
                 trend = "1st", 
                 ini = c(33000, .8),
                 nug = 10000, 
                 cov.model = "spherical")
wca.vm
```

### Step 4: Create the kriged surface

Kriging uses the variogram model and the observed data to estimate data values at any location of interest. The kriged estimates are a weighted average of the neighborhood values where the weights are taken from the variogram model. 

Estimates are often made at locations defined on a regular grid.

(a) First create a grid of locations across the domain. Here we use the `expand.grid()` function. The coordinates names of the grid are those defined in the `geodata` object (here `lon` and `lat`). Grid spacing is defined by the `l =` argument in the sequence function.
```{r}
pgrid.df <- expand.grid(lon = seq(-105, -100, l = 61),
                        lat = seq(33, 37, l = 61))
head(pgrid.df)
```

The resulting data frame is a series of locations specified as in a raster starting in the southwest corner. Plot the grid. First convert it to a simple feature data frame. Do the same for the observations.
```{r}
library(sf)
pgrid.sf <- st_as_sf(x = pgrid.df,
                     coords = c("lon", "lat"),
                     crs = "+proj=longlat +datum=WGS84")
wca.sf <- st_as_sf(x = wca.df, 
                   coords = c("lon", "lat"),
                   crs = "+proj=longlat +datum=WGS84")

library(USAboundaries)
sts <- us_states()

library(tmap)
tm_shape(wca.sf) +
  tm_bubbles(size = .25) +
tm_shape(pgrid.sf) +
  tm_dots(col = "red")
```

(b) Next predict the head heights at the grid locations. 

The `krige.conv()` function performs the kriging. Predictions are made at the grid locations using the data and the variogram model (`wca.vm`). We specify a 1st-order trend in the data (`trend.d`) and we want the predictions to include the trend (`trend.l`) so these are included as parameters in the `krige.control()` function.
```{r}
wca.ks <- krige.conv(wca.gdf, 
                     loc = pgrid.df, 
                     krige = krige.control(trend.d = "1st", 
                                           trend.l = "1st", 
                                           obj.m = wca.vm))  
str(wca.ks)
```

The fitted values (`predict`) and the uncertainty (`krige.var`) are output as a vector of length  61 x 61 = 3721. The uncertainty is the standard deviation squared of the predicted value. Kriging performed using global neighborhood.

(c) Plot the results.

First add the fitted and uncertainty values as columns to the `pgrid.df` data frame.
```{r}
pgrid.df$height <- wca.ks$predict
pgrid.df$var <- wca.ks$krige.var
```

Map the predicted values and the uncertainty values using `ggplot()`.

Start with the predicted values. Here we use the `geom_raster()` function.
```{r}
library(ggplot2)

ggplot(data = pgrid.df, aes(x = lon, y = lat)) + 
  geom_raster(aes(fill = height)) +
  scale_fill_viridis_c()
```

The map shows the predicted values as a combination of the spatial gradient and spatial autocorrelation.

Or use {tmap}. First convert the grid data frame to a spatial pixels data frame (S4 spatial object). Then convert the spatial pixels data frame to a raster.
```{r}
library(sp)
spdf <- pgrid.df
coordinates(spdf) <- c("lon", "lat")
spdf <- as(spdf, "SpatialPixelsDataFrame")
proj4string(spdf) <- st_crs(wca.sf)$proj4string

library(raster)
r <- raster(spdf)

tm_shape(r) +
  tm_raster(n = 9) +
tm_shape(wca.sf) +
  tm_text("head", size = .5)
```

Plot the uncertainty.
```{r}
ggplot(data = pgrid.df, aes(lon, lat)) + 
  geom_raster(aes(fill = var)) +
  scale_fill_gradient() +
  geom_point(data = wca.df, aes(x = lon, y = lat))
```

The map shows that the prediction variances are smallest in regions surrounding the observations. This makes sense since what we know about the field comes from the observations.

### Step 5: Evaluate the prediction

How do we evaluate how good the interpolated surface is? If we use the variogram model to predict at the observation locations, we will get the observed values back when the nugget is fixed at zero. So this is not helpful. Instead we use cross validation.

Cross validation is a procedure for assessing how well a model will do at predicting values when observations specific to the prediction are removed. The procedure first partitions the data into disjoint subsets. The model is then fit to one subset of the data (training set) and the model is validated on a different subset (testing set). 

Leave-one-out cross validation uses all but one observation for fitting and the left-out observation for testing. The procedure is repeated with every observation taking turns in being left out. 

K-fold cross validation uses K observations for fitting and N-K for testing. With large K there are many ways to slice the sample so the procedure is not exhaustive like hold-one-out. 

With kriging, the data is used in two ways (1) to fit the variogram model, and (2) to interpolate the values. Thus cross validation has two cases: weak and strong. Weak cross validation uses the entire dataset to estimate the variogram model. Then kriging is performed N times using a leave-one-out strategy with the predicted value saved only for the observation left out.

The `xvalid()` function from the {geoR} package computes the cross-validated prediction error of the Wolfcamp aquifer interpolation in this weak sense.
```{r}
xv.wk <- xvalid(wca.gdf, model = wca.vm)
df <- data.frame(observed = xv.wk$data, 
                 predicted = xv.wk$predicted)

ggplot(df, aes(x = observed, y = predicted)) + 
  geom_point() +
  geom_abline(intercept = 0, slope = 1) +
  geom_smooth(method = lm, color = "red") +
  ylab("Predicted head heights (ft)") +
  xlab("Observed head heights (ft)") +
  ggtitle("Weak Cross Validation") +
  theme_minimal()
mean(xv.wk$error^2)
mean(abs(xv.wk$error))
```

The mean squared cross-validated prediction error is 30636 ft^2 and the mean absolute cross-validated prediction error is 137 ft.

Strong cross validation requires that the variogram be re-estimated each time an observation is removed. The model must be fit using the `variofit()` function and the call must include the empirical variogram object. This is done with the argument `reestimate = TRUE`.
```{r, warning=FALSE}
xv.st <- xvalid(wca.gdf, model = wca.vm,
               variog.obj = wca.v,
               reestimate = TRUE)
df <- data.frame(observed = xv.st$data, 
                 predicted = xv.st$predicted)

mean(xv.st$error^2)
mean(abs(xv.st$error))
```

Strong cross validation will result in an error estimate that is larger than the error estimate from a weak cross validation.

## Example 2: April temperatures in the Midwest

The data in `MidwestTemps.txt` are average temperatures in and around the state of Iowa for the month of April. The goal is a spatial interpolation of these values onto a 56 by 35 grid.

### Step 1: Examine the data for spatial trends and normality
```{r}
L <- "http://myweb.fsu.edu/jelsner/temp/data/MidwestTemps.txt"
t.df <- read.table(L, header = TRUE)
summary(t.df)
```

Map the values.
```{r}
t.sf <- st_as_sf(x = t.df, 
                 coords = c("lon", "lat"),
                 crs = "+proj=longlat +datum=WGS84")

tm_shape(t.sf) +
  tm_text(text = "temp", size = .6) +
tm_shape(sts) +
  tm_borders() 
```

Create a `geodata` object from the data frame.
```{r}
t.gdf <- as.geodata(t.df)
summary(t.gdf)
plot(t.gdf)
```

The maximum pairwise distance is 11.5 degrees. There is a pronounced 1st order trend in the north/south direction as we might expect with air temperatures.

Remove the trend and examine the residuals.
```{r}
plot(t.gdf, trend = "1st")
```

There is some evidence of a 2nd-order trend in the west-east direction. 
```{r}
plot(t.gdf, trend = "2nd")
```

By specifying a higher order trend, the lower order trends are taken care of. The distribution of residuals is approximately normal as we might expect. The data are monthly means.

### Step 2: Compute empirical variograms

Check for anisotropy by plotting directional variograms.
```{r}
par(mfrow = c(1, 1))
plot(variog4(t.gdf, trend = "2nd", 
             max.dist = 5.5), 
     omni = TRUE, legend = FALSE)
```

There is no strong evidence to reject isotropy.

### Step 3: Fit a variogram model to the data

Here we consider several likelihood fits to an exponential model and examine the AIC for final parameter selection. 

The AIC is used as a selection criterion and is a function of the maximized likelihood function but includes a penalty for model complexity that favors simpler models. Recall the best fit has the largest log-likelihood and smallest AIC.  

Set initial values for the sill and range. From the variograms start with 3 for the sill and 4 for the range.
```{r}
iv <- c(3, 4)  
summary(likfit(t.gdf, 
               ini = iv, 
               cov.model = "exp", 
               trend = "2nd",
               fix.nug = TRUE, 
               message = FALSE))$likelihood$AIC
summary(likfit(t.gdf, 
               ini = iv, 
               cov.model = "exp", 
               trend = "2nd",
               fix.nug = FALSE, 
               message = FALSE))$likelihood$AIC
summary(likfit(t.gdf, ini = iv, 
               cov.model = "sph", 
               trend = "2nd",
               fix.nug = TRUE, 
               message = FALSE))$likelihood$AIC
summary(likfit(t.gdf, ini = iv, 
               cov.model = "sph", 
               trend = "2nd",
               fix.nug = FALSE, 
               message = FALSE))$likelihood$AIC
```

It appears that a good variogram model would be on the residuals of a 2nd order trend using an exponential function with fixed nugget equal to zero. A spherical function with a nugget is also a reasonable model.

To obtain the model parameters, type
```{r}
likfit(t.gdf, ini = iv, cov.model = "exp", 
       trend = "2nd", fix.nug = TRUE)
likfit(t.gdf, ini = iv, cov.model = "sph", 
       trend = "2nd", fix.nug = FALSE)
```

Plot the competing models on the empirical variogram.
```{r}
plot(variog(t.gdf, trend = "2nd", 
            uvec = seq(0, 5.5, l = 29)))
lines.variomodel(cov.model = "exp", 
                 cov.pars = c(2.114, .4139), 
                 nug = 0, max.dist = 5.5, col = "red")
lines.variomodel(cov.model = "sph", 
                 cov.pars = c(1.638, 1.307),
                 nug =.5, max.dist = 5.5, col = "green")
```

Save the models.
```{r}
modelE <- likfit(t.gdf, ini = iv, cov.model = "exp", 
                  trend = "2nd", fix.nug = TRUE)
modelS <- likfit(t.gdf, ini = iv, cov.model = "sph", 
                  trend = "2nd", fix.nug = FALSE)
```

### Step 4: Create an interpolated surface

We create a grid of locations at which we want the temperatures to be interpolated. Here we use the `expand.grid` function where the arguments are the sequence of longitudes and latitudes, respectively. We then use the `krige.conv()` function to interpolate the values to the grid. We save the interpolation in `kcE` when we use the exponential variogram model to weight the observations and save the interpolation in `kcS` when we use the spherical model to weight the observations.
```{r}
pgrid.df<- expand.grid(lon = seq(-99, -88, l = 224), 
                       lat = seq(38.4, 45.4, l = 136))
kcE <- krige.conv(t.gdf, 
                  loc = pgrid.df, 
                  krige = krige.control(trend.d = "2nd", 
                                       trend.l = "2nd",
                                       obj.m = modelE))
kcS <- krige.conv(t.gdf, 
                  loc = pgrid.df,
                  krige = krige.control(trend.d = "2nd", 
                                       trend.l = "2nd",
                                       obj.m = modelS))
```

Plot the interpolated surface generated using the exponential variogram. function.
```{r}
pgrid.df$temp <- kcE$predict
pgrid.df$var <- kcE$krige.var

spdf <- pgrid.df
coordinates(spdf) <- c("lon", "lat")
spdf <- as(spdf, "SpatialPixelsDataFrame")
proj4string(spdf) <- st_crs(t.sf)$proj4string

library(raster)
r <- raster(spdf)

tm_shape(r) +
  tm_raster(n = 9, palette = "OrRd") +
tm_shape(sts) +
  tm_borders() +
tm_shape(t.sf) +
  tm_text("temp", size = .5)
```

Plot the interpolated surface generated using the spherical variogram.
```{r}
pgrid.df$temp <- kcS$predict
pgrid.df$var <- kcS$krige.var

spdf <- pgrid.df
coordinates(spdf) <- c("lon", "lat")
spdf <- as(spdf, "SpatialPixelsDataFrame")
proj4string(spdf) <- st_crs(t.sf)$proj4string

library(raster)
r <- raster(spdf)

tm_shape(r) +
  tm_raster(n = 9, palette = "OrRd") +
tm_shape(sts) +
  tm_borders() +
tm_shape(t.sf) +
  tm_text("temp", size = .5)
```

The model with a non-zero nugget is smoother. The greater the nugget relative to the sill (relative nugget effect), the smoother the interpolation.

## Combine prediction and uncertainty in a single map

See `Pixelate.Rmd`.

## Synthetic data

Synthetic data are useful as input to a deterministic model. An example is rainfall as input to spatially-distributed rainfall-runoff model. Interpolated values of precipitation and their variances might be of little value, but running the rainfall-runoff model with a large number of simulated rainfall fields can give a realistic assessment of the uncertainty in runoff arising from the variability in the rainfall.

The `grf()` function in the {geoR} package generates synthetic data from Gaussian random fields on regular or irregular sets of locations. For example, to generate 100 randomly spaced points with values at the points being a sample from a Gaussian random field with an exponential variogram (default with zero nugget) having a sill of 1 and a range of .25, type
```{r}
set.seed(3042)
sim1 <- grf(100, cov.pars = c(1, .25))
```

Plot the points and the variograms.
```{r}
layout(matrix(c(1, 2), byrow = TRUE, ncol = 2), 
       respect = TRUE)
points.geodata(sim1, 
               main = "simulated locations and values")
plot(sim1, max.dist = .5, 
     main = "true and empirical variograms")
```

The random fields can be put on a regular grid.
```{r}
sim2 <- grf(441, grid = "reg", cov.pars = c(1, .25))
image(sim2, col = gray(seq(1, .1, l = 30)))
sim3 <- grf(4441, grid = "reg", cov.pars = c(1, .25))
image(sim3, col = gray(seq(1, .1, l = 30)))
par(mfrow = c(1, 1))
```

For an even higher resolution simulation it is necessary to have the {RandomFields} package.
```{r, eval=FALSE}
library(RandomFields)
sim4 <- grf(40401, grid = "reg", cov.pars = c(10, .2))
image(sim4, main = "simulation on a fine grid", 
      col = gray(seq(1, .1, l = 30)))
```

## Spatial statistical interpolation using functions from the {gstat} package

**"Commenting your code is like cleaning your bathroom - you never want to do it, but it really does create a more pleasant experience for you and your guests."** --- Ryan Cambell

We start with the Meuse river data from Burrough and McDonnell (1998). The data are part of {gstat}.
```{r}
library(gstat)
library(sp)

data(meuse)
head(meuse)
```

The data are topsoil heavy metal concentrations along with a number of soil and landscape variables at locations in a flood plain of the river Meuse near the town of Stein (NL).

Heavy metal concentrations are composite samples of an area of approximately 15 m x 15 m. The dataset also gives local elevation (m) above local river bed and distance to river normalized to a value between 0 and 1.

Create a spatial points data frame (S4 class) and map the concentration of zinc in the soil on a log scale as a bubble plot.
```{r}
coordinates(meuse) <- ~ x + y
proj4string(meuse) <- CRS("+init=epsg:28992")
class(meuse)
bubble(meuse, "zinc", 
       do.log = TRUE, 
       key.space = "bottom")
```

Zinc in the soil is concentrated along the east bank of the Meuse river.

### Inverse distance weighted interpolation

We begin by creating an interpolated surface with the method of inverse distance weighting (IDW). Here the average of the observed values for any location is weighted inversely based on how far the observation points are from the location of interest. An exponent on the weights matrix determines the degree to which nearer observations are weighted more than more distant ones. 

A regular grid where the interpolated values of zinc are made is given in the data frame `meuse.grid`. 
```{r}
data(meuse.grid)
head(meuse.grid)
```

Convert the grid as a data frame to a spatial pixels data frame. Note this is done first by converting the data frame to spatial points then to spatial pixels.
```{r}
coordinates(meuse.grid) <- ~ x + y
proj4string(meuse.grid) <- CRS("+init=epsg:28992")
meuse.grid2 <- as(meuse.grid, "SpatialPixelsDataFrame") 
head(meuse.grid2)
```

As mentioned last time, the above conversion works since the locations have a spatial ordering. It will not work for a data frame with an arbitrary ordering of locations. 

Here we plot the pixel locations using the attribute `dist` indicating distance to the river.
```{r}
library(tmap)

tm_shape(meuse.grid2) +
  tm_raster("dist")
```

Interpolate the zinc values to the pixel locations using IDW. The `idp = ` argument is the exponent on the weights. The larger the exponent, the more variation there is in the interpolated values.
```{r}
zinc.idw <- idw(zinc ~ 1, 
                meuse, 
                newdata = meuse.grid2, 
                idp = 2.5)
str(zinc.idw, max.level = 3)
```

Note the formula. Here `zinc ~ 1`, where `zinc` is the field variable that is to be interpolated and 1 indicates a IDW interpolation (no autocorrelation and no trend or covariates).

The saved object (`zinc.idw`) inherits the class of the interpolation grid (here `SpatialPixelDataFrame`). The interpolated values are saved in the data frame slot with variable name `var1.pred`. The other variable is `var1.var` which is an estimate of the interpolation errors. With IDW they are `NA`.

IDW results in a surface that is often similar to a kriged surface. But interpolations may show undesired effects if the observation locations have a non-uniform distribution.

Map the results. Start with an image plot, then add a bubble plot of the observed concentrations and a legend. Here the size of the point character (circle) is set by the `cex = argument`.
```{r}
tm_shape(zinc.idw) +
  tm_raster("var1.pred") +
tm_shape(meuse) +
  tm_bubbles("zinc", alpha = .3)
```

IDW tends to generate 'bulls-eye' patterns.

The spatial class makes it easier to create maps and export them to different platforms.

### Create a geoTIF file that can be read into ENVI or ArcMap.

Here you repeat the analysis but this time use a planar projection for the interpolation grid, interpolate the logarithm of the zinc, and create a `geoTIF` file for output.

Again make `meuse.grid` a spatial pixels data frame.
```{r}
data(meuse.grid)
coordinates(meuse.grid) <- ~ x + y
gridded(meuse.grid) <- TRUE
proj4string(meuse.grid) <- CRS(paste("+init=epsg:28992", 
                                     "+towgs84=565.237,50.0087,465.658,-0.406857,0.350733,-1.87035,4.0812"))
```

Make `meuse` a spatial points data frame with the same projection.
```{r}
data(meuse)
coordinates(meuse) <- ~ x + y
proj4string(meuse) <- CRS(proj4string(meuse.grid))
```

Use the `krige` function in the {gstat} package and perform inverse-distance predictions. Since no variogram is specified in the function, the method defaults to inverse-distance weighting.
```{r}
logZinc <- krige(log(zinc) ~ 1, 
                meuse, 
                meuse.grid)["var1.pred"]
summary(logZinc)
```

### Create a KML file that can be read by Google Earth

We export a colored raster of interpolated log zinc ppm values to a PNG file with an alpha channel for viewing it in Google Earth. Since GE requires geographical coordinates, a number of steps are needed.

First we make a polygon to bound the study area and project it to geographical coordinates.
```{r}
library(maptools)
grd <- as(meuse.grid, "SpatialPolygons")
proj4string(grd) <- CRS(proj4string(meuse.grid))
grd.union <- unionSpatialPolygons(grd, rep("x", length(slot(grd, "polygons"))))  
ll <- CRS("+proj=longlat +datum=WGS84")
grd.union.ll <- spTransform(grd.union, ll)
plot(grd.union.ll, axes = TRUE) 
```

The result is a single polygon as the union of all polygons in grd in geographic coordinates. Next, we construct a grid in geographical coordinates as our target object for export using the `GE_SpatialGrid` function. This grid is the container for the output PNG graphics file. `GE_SpatialGrid` returns values that will be used in setting up the PNG graphics device.

We use the `over()` method to set grid cells outside the river bank area to NA and then discard them by coercion.
```{r}
llGRD <- GE_SpatialGrid(grd.union.ll)
llGRD_in <- over(llGRD$SG, grd.union.ll)
llSGDF <- SpatialGridDataFrame(grid = slot(llGRD$SG, "grid"), 
                              proj4string = CRS(proj4string(llGRD$SG)), 
                              data = data.frame(in0 = llGRD_in))
llSPix <- as(llSGDF, "SpatialPixelsDataFrame")
```

Next we interpolate. Here again we use the `idw` function to make an inverse-distance weighted interpolation of zinc values in ppm.
```{r}
meuse_ll <- spTransform(meuse, 
                       CRS("+proj=longlat +datum=WGS84"))
llSPix$pred <- idw(log(zinc) ~ 1, 
                   meuse_ll, 
                   llSPix)$var1.pred
spplot(llSPix["pred"])
```

Since `GE_SpatialGrid` was set to the size of an PNG graphics device, we can use it with the `image()` function. Other base graphics methods and functions can be used to create an image overlay. First open the graphics device, then set the plotting margins, then use `image`, and finally turn off the device.
```{r}
png(file <- "zincIDW.png", width = llGRD$width, 
    height = llGRD$height, bg = "transparent")
par(mar = c(0, 0, 0, 0), xaxs = "i", yaxs = "i")
image(llSPix, "pred", col = bpy.colors(20))
dev.off()
```

After closing the graphics device, we use the `kmlOverlay` to write the interpolated grid to a KML file giving the location of the overlay. This will load the image at the position when opened in Google Earth.
```{r}
kmlOverlay(llGRD, "zincIDW.kml", "zincIDW.png")
```

Open the `zincIDW.kml` file in Google Earth.

### Distance to river as a covariate

Here you use distance to river as a covariate. It is a trend model for the spatial variability of zinc using a covariate defined by the square root of the distance to river. This is not kriging since no variogram model is used. 
```{r}
zn.lm <- krige(log(zinc) ~ sqrt(dist), 
               locations = meuse, 
               newdata = meuse.grid)

tm_shape(zn.lm) +
  tm_raster("var1.pred")
```

Note that the spacing in colors is not linear.

### Empirical variogram analysis

The `variogram()` function from {gstat} computes the empirical variogram. Here the trend is not in the cardinal directions but rather in distance to the river. The variogram is on the residuals from this trend.
```{r}
meuse.v <- variogram(log(zinc) ~ sqrt(dist), 
                     data = meuse)
plot(meuse.v, xlab = "Lag Distance (m)")
```

The default cutoff lag (`cutoff`) and lag tolerance (`width`) can be changed. For example, type
```{r}
plot(variogram(log(zinc) ~ sqrt(dist), 
               data = meuse, 
               cutoff = 1000, 
               width = 50), 
      xlab = "Lag Distance (m)")
```

For comparison, plot a variogram of residuals after removing a 1st-order trend in direction.
```{r}
plot(variogram(log(zinc) ~ x + y, 
               data = meuse, 
               cutoff = 1000, 
               width = 50), 
     xlab = "Lag Distance (m)")
```

Directional variograms are estimated by adding the argument alpha.
```{r}
plot(variogram(log(zinc) ~ x + y, 
               data = meuse, 
               cutoff = 1000, 
               width = 50, 
               alpha = c(0, 45, 90, 135)), 
     xlab = "Lag Distance (m)")
```

The variogram in the sw-ne direction appears to be different. And there appears to be very little spatial autocorrelation in the n-s direction. This is partly due to the anistropy of the domain.

### Variogram maps

Another way of looking at directional dependence in semivariograms is with variogram maps. Instead of classifying point pairs $Z(s)$ and $Z(s + h)$ by direction and distance separately, they are classified jointly. 

If $h = (x, y)$ is the two-dimensional coordinates of the separation vector, in the variogram map the semivariance contribution of each point pair $(Z(s) - Z(s + h))^2$ is attributed to the grid cell in which $h$ lies. The map is centered at (0, 0), as $h$ is geographical distance rather than geographical location. 

Cutoff and width correspond to map extent and cell size; the semivariance map is point symmetric around (0, 0), as $\gamma(h) = \gamma(-h)$. 

The variogram map is obtained with the `map = TRUE` argument. The threshold assures that only semivariogram map values based on at least 5 point pairs are shown, removing too noisy estimation. 
```{r}
meuse.vmap <- variogram(log(zinc) ~ x + y, 
                        data = meuse, 
                        cutoff = 1500, 
                        width = 100,
                        map = TRUE)
plot(meuse.vmap, threshold = 5)
```

The symmetry about the dx, dy = (0, 0) is apparent. The variances range from 0 to 2.5 and appear to elongate along the long axis of the domain.

Compare with a variogram map when the square root of the distance to river is used as a covariate.
```{r}
meuse.vmap <- variogram(log(zinc) ~ sqrt(dist), 
                        data = meuse, 
                        cutoff = 1500, 
                        width = 100,
                        map = TRUE)
plot(meuse.vmap, threshold = 5)
```

The variances range from 0 to .8 and the directional dependence is much less obvious in this case.

### Variogram modeling

A plot of the model families is available with the `show.vgms()` function.
```{r}
show.vgms() 
```

We've seen the exponential (`Exp`), spherical (`Sph`), and Gaussian (`Gau`) families when working with the functions in the {geoR} package.

After choosing a family starting values are needed in the `fit.variogram()` function. The function calls the `vgm()` function that includes initial guesses for the partial sill (`psill`), the range (`range`), and the nugget (`nugget`) and uses a weighted least squares method.
```{r}
meuse.vm <- fit.variogram(meuse.v, 
                          vgm(model = "Sph", 
                              psill = .15,
                              range = 800, 
                              nugget = .1))
meuse.vm
```

The output includes a 2 x 2 table with the first row giving information about the nugget and the second giving information about the rest of the model. Here the nugget is .08. By definition the range is zero. The partial sill is .15 with a range of 872.7 m. Plot the sample variogram and the variogram model.
```{r}
plot(meuse.v, meuse.vm, 
     xlab = "Lag distance (m)")
```

The `fit.variogram` function refines the initial estimates given in the `vgm` function. If our initial choice is poor the fit may not work.  For example try
```{r}
fit.variogram(meuse.v, 
              vgm(model = "Sph",
                  psill = 1, 
                  range = 10, 
                  nugget = 1))
```

An argument for visual fitting over numerical fitting is when you have knowledge that goes beyond the information in the data. Information may come from other studies or derived from measurement error characteristics for a specific measuring device (nugget effect).

### Kriging

Simple kriging is done when there is no trend and the mean height of the surface is provided. Because you know the mean exactly you also know the residuals exactly at the data locations. If you know the residuals exactly then you can do a better job of estimating the autocorrelation function (variogram). For ordinary kriging you estimate the mean so the residuals are also estimates.

Universal kriging is done when there is a trend. With universal kriging both the trend and spatial variability are estimated in tandem.

Here we apply kriging to the logarithm of the zinc concentration using the fitted variogram model.
```{r}
lz.sk <- krige(formula = log(zinc) ~ 1, 
               locations = meuse, 
               newdata = meuse.grid, 
               model = meuse.vm, 
               beta = 5.9)
lz.ok <- krige(formula = log(zinc) ~ 1, 
               locations = meuse, 
               newdata = meuse.grid, 
               model = meuse.vm)
lz.uk <- krige(formula = log(zinc) ~ sqrt(dist),
               locations = meuse, 
               newdata = meuse.grid, 
               model = meuse.vm)

m1 <- tm_shape(lz.sk) +
        tm_raster("var1.pred", title = "Simple")

m2 <- tm_shape(lz.ok) +
        tm_raster("var1.pred", title = "Ordinary")

m3 <- tm_shape(lz.uk) +
        tm_raster("var1.pred", title = "Universal")

tmap_arrange(m1, m2, m3, ncol = 3)
```

Simple and ordinary kriging treat all spatial variation as autocorrelation. Universal kriging lets some of the variation be due to trend and covariates.

Predictive skill.

## Example 1: Tropical cyclone rainfall

In 2008, Tropical Cyclone Fay formed from a tropical wave near the Dominican Republic, passed over the island of Hispaniola, Cuba, and the Florida Keys, then crossed the Florida peninsula and moved westward across portions of the panhandle producing heavy rains in parts of the state.

Storm total rainfall amounts from stations in and around the state are in `FayRain.txt`. They are a compilation of reports from the U.S. NOAA/NWS official weather sites and coop sites.

The coop sites are the Community Collaborative Rain, Hail and Snow Network (CoCoRaHS), a community-based, high density precipitation network made up of volunteers who take measurements of precipitation in their backyards. The data were obtained from NOAA/NCEP/HPC and from the Florida Climate Center.

Load the data.
```{r}
L <- "http://myweb.fsu.edu/jelsner/temp/data/FayRain.txt"
FR.df <- read.table(L, header = TRUE)
names(FR.df)
```

The data frame contains 803 rainfall sites. Longitude and latitude coordinates of the sites are given in the first two columns and total rainfall in inches and millimeters are given in the second two columns. Create a spatial points data frame by specifying columns that contain the spatial coordinates. Then assign a geographic coordinate system and convert the rainfall from millimeters to centimeters.
```{r}
library(sf)

FR.sf <- st_as_sf(x = FR.df,
                 coords = c("lon", "lat"),
                 crs = 4326)

FR.sf$tpm <- FR.sf$tpm/10
summary(FR.sf$tpm)
```

The median value is 15.8 cm and the maximum is 60.2 cm. 

Next get the Florida county boundaries from the {USAboundaries} package.
```{r}
FL.sf <- USAboundaries::us_counties(states = "Florida")
```

Next we create a character string specifying the tags for a planar projection and transform the geographic coordinates of the site locations and map polygons to the projected coordinates. Here we use Florida GDL Albers (EPSG:3087) with meter as the distance unit.

```{r}
FR.sf <- st_transform(FR.sf, crs = 3087)
FL.sf <- st_transform(FL.sf, crs = 3087)
```

A map of the rainfall sites and storm totals with the state boundaries is made by typing
```{r}
library(tmap)

tm_shape(FR.sf) +
  tm_dots(col = "tpm", size = 1) +
tm_shape(FL.sf) +
  tm_borders()
```

Two areas of heavy rainfall are noted.  One running north-south along the east coast and another across the north. Rainfall collection sites are clustered in and around cities. This will make it difficult to use a spline interpolation.

### Empirical variogram

Rainfall is an example of geostatistical data. In principle it can be measured anywhere, but typically we have values at a sample of sites. The pattern of observation sites is not of much interest as it is a consequence of constraints (convenience, opportunity, economics, etc) unrelated to the phenomenon. Interest centers on inference about how much rain fell across the region.

The empirical variogram is computed using the `variogram()` function. The first argument is the model formula specifying the rainfall column from the data frame and the second argument is the data frame name.  Here `~ 1` in the model formula indicates no covariates or trends in the data. Trends can be included by specifying coordinate names through the `st_coordinates()` function.

We compute the empirical variogram for Fay's rainfall and save it by typing
```{r}
FR.v <- variogram(tpm ~ 1, 
                  data = FR.sf)
```

We plot the variogram values as a function of lag distance and add text indicating the number of point pairs for each lag distance.
```{r}
library(ggplot2)

v.df <- data.frame(dist = FR.v$dist/1000,
                   gamma = FR.v$gamma,
                   np = FR.v$np)

( pv <- ggplot(v.df, aes(x = dist, y = gamma)) +
  geom_point() +
  geom_text(aes(label = np), nudge_y = -5) +
  scale_y_continuous(limits = c(0, 220)) +
  scale_x_continuous(limits = c(0, 400)) +
  xlab("Lagged distance (h) [km]") +
  ylab(expression(paste("Semivariance (", gamma, ") [", cm^2, "]"))) +
  theme_minimal() )
```

Values start low (~50 cm$^2$) at short lag distance, then increase to over 200 cm$^2$ at lag distance of about 200 km.

The zero-lag semivariance is called the 'nugget' and the semivariance at a level where the variogram values no longer increase is called the 'sill.'  The lag distance to the sill is called the 'range.'  These three parameters (nugget, sill, and range) are used to fit a model to the variogram.

### Variogram model

Next we fit a model to the empirical variogram. The variogram model is a mathematical relationship defining the semivariance as a function of lag distance. We first save the family and the initial parameter guesses in a variogram model (`FR.vmi`) object by typing
```{r}
FR.vmi <- vgm(model = "Gau", 
              psill = 150, 
              range = 200 * 1000, 
              nugget = 50)
FR.vmi
```

The `psill` argument is the partial sill as the difference between the sill and the nugget. We get estimates of the parameter values from looking at the empirical variogram. 

Next we use the `fit.variogram()` function to improve the fit.  Given a set of initial parameter values, the method of weighted least squares improves the parameter estimates. Ordinary least squares is not appropriate as the semivariances are correlated across the lag distances and the precision on the estimates varies depending on the number of site pairs for a given lag.
```{r}
FR.vm <- fit.variogram(object = FR.v, 
                       model = FR.vmi)
FR.vm
```

The result is a variogram model with a nugget of 46.8 cm$^2$, a partial sill of 157 cm$^2$, and a range on the sill of 129 km. 

Let $r$ be the range, $c$ the partial sill and $c_o$ the nugget, then the equation defining the curve over the set of lag distances $h$ is
$$
\gamma(h)=c\left(1-\exp\left(-\frac{h^2}{r^2}\right)\right)+c_o
$$

We create a data frame with values of h and gamma using this equation.

```{r}
nug <- FR.vm$psill[1]
ps <- FR.vm$psill[2]
r <- FR.vm$range[2] / 1000
h <- seq(0, 400, .2)
gamma <- ps * (1 - exp(-h^2 / (r^2))) + nug

vm.df <- data.frame(dist = h,
                    gamma = gamma)

pv + geom_line(aes(x = dist, y = gamma), data = vm.df)
```

Check for anisotropy.
```{r}
plot(variogram(tpm ~ 1, 
               data = FR.sf, 
               alpha = c(0, 45, 90, 135)), 
     xlab = "Lag Distance (m)")
```

We see the range of correlations is longer (about 300 km) in the north-south direction (0 degrees). We refit the variogram defining an anistropy ellipse with the `anis =` argument. The first parameter is the direction of longest range and the second parameter is the ratio of the longest to shortest. Here about (200/300 = .67).
```{r}
FR.vmi <- vgm(model = "Gau", 
              psill = 150, 
              range = 300 * 1000, 
              nugget = 50,
              anis = c(0, .67))
FR.vm <- fit.variogram(FR.v, FR.vmi)
```

### Kriging

The final step is to use the variogram model together with the rainfall values at the observation sites to create an interpolated surface. The process is called kriging. As Edzer Pebesma notes, 'krige' is to 'kriging' as 'predict' is to 'predicting.' Here we use ordinary kriging as there are no spatial trends in the rainfall.

Interpolation is done using the `krige()` function. The first argument is the model specification and the second is the data. Two other arguments are needed. One is the variogram model using the argument name model and the other is a set of locations identifying where the interpolations are to be made. This is specified with the argument name `newdata =`.

#### Interpolate to point loctions

Here we interpolate to locations on a regular grid. We create a grid of locations within the boundary of Florida using the `st_sample()` function.
```{r}
grid.sf <- st_sample(FL.sf,
                     size = 5000,
                     type = "regular")
```

We specify the number of locations using the argument `size =`. Note that the actual number of locations will be somewhat different because of the irregular boundary. 

First we use the `krige()` function to interpolate the observed rainfall to the grid locations. Recall that for a given location, the interpolation is a weighted average of the rainfall across the entire region where the weights are determined by the variogram model.
```{r}
ipl <- krige(tpm ~ 1, 
             locations = FR.sf, 
             newdata = grid.sf,
             model = FR.vm)
```

If the variogram model is not included then inverse distance weighted interpolation is performed. The function will not work if different values share the same location. 

The saved object (`ipl`) inherits the spatial object specified in the `newdata` argument, but extends it to a spatial data frame. The data frame with two variables.  The first `var1.pred` is the interpolated rainfall and the second `var1.var` is the prediction variance.

We plot the interpolated field.
```{r}
tm_shape(ipl) +
  tm_dots("var1.pred",
          size = .1,
          palette = "Greens",
          title = "Rainfall (cm)") +
  tm_shape(FL.sf) +
  tm_borders() +
  tm_layout(legend.position = c("left", "bottom"),
            title = "Tropical Cyclone Fay (2008)",
            title.position = c("left", "bottom"))
```

Note that a portion of the data values where outside of the state but our interest is only to have values on a grid within the state. 

The spatial interpolation shows that parts of east central and north Florida were deluged by Fay.

#### Interpolate to the counties (block kriging)

We use block kriging to estimate average rainfall within each county. The county-wide rainfall average is relevant for water resource managers. Block kriging produces a smoothed estimate of this area average, which will differ from a simple average over all sites within the county because of spatial autocorrelation.

We use the same function to interpolate but specify the spatial polygons rather than the spatial grid as the new data.
```{r}
ipl2 <- krige(tpm ~ 1, 
              locations = FR.sf, 
              newdata = FL.sf, 
              model = FR.vm)
```

Again we plot the interpolations.
```{r}
tm_shape(ipl2) +
  tm_polygons(col = "var1.pred",
            palette = "Greens",
            title = "Rainfall (cm)") +
  tm_layout(legend.position = c("left", "bottom"),
            title = "Tropical Cyclone Fay (2008)",
            title.position = c("left", "bottom"))
```

The overall pattern of rainfall from Fay featuring the largest amounts along the central east coast and over the eastern panhandle are similar in both maps.

We compare the kriged average with the simple average at the county level with `aggregate()` from the {sf} package.
```{r}
ipl3 <- aggregate(FR.sf, by = FL.sf, FUN = mean)
```

The function returns a simple feature data frame of the average rainfall in each county.

The state-wide mean of the kriged estimates at the county level is 
```{r}
round(mean(ipl2$var1.pred), 2)
```

This compares with a state-wide mean from the simple averages.
```{r}
round(mean(ipl3$tpm), 2)
```

The correlation between the two estimates across the 67 counties is 
```{r}
round(cor(ipl3$tpm, ipl2$var1.pred), 2)
```

The variogram model reduces the standard deviation of the kriged estimate relative to the standard deviation of the simple averages because of the local smoothing.
```{r}
round(sd(ipl2$var1.pred), 2)
round(sd(ipl3$tpm), 2)
```

This can be seen with a scatter plot of simple averages versus kriged averages at the county level.
```{r}
compare.df <- data.frame(simpleAvg = ipl3$tpm,
                         krigeAvg = ipl2$var1.pred)
ggplot(compare.df, aes(x = simpleAvg,
                       y = krigeAvg)) +
  geom_point() +
  geom_abline(slope = 1) +
  geom_smooth(method = lm, se = FALSE)
```

### Uncertainty

Another advantage of kriging as a method of spatial interpolation is the accompanying uncertainty estimates.  

The prediction variances are listed in a column in the spatial data frame saved from apply the `krige()` function. Variances are smaller in regions with more rainfall observations.  

Prediction variances are also smaller with block kriging as much of the variability within the county averages out. To compare the distribution characteristics of the prediction variances for the point and block kriging of the rainfall observations, type
```{r}
round(summary(ipl$var1.var), 1)
round(summary(ipl2$var1.var), 1)
```

The median prediction variance (in cm$^2$) for our point kriging is close to the value of the nugget.
```{r}
round(fivenum(ipl$var1.var)[3], 1)
```

In contrast the median prediction variance for our block kriging is a much smaller 
```{r} 
round(fivenum(ipl2$var1.var)[3], 1)
```

Simulations exploit this uncertainty and provide synthetic data for use in deterministic models. 

Conditional simulation, where the simulated field (realization) is generated given the data and the variogram model, is done using the same `krige()` function by adding the argument `nsim` to specify the number of simulations.  

For a large number it may be necessary to limit the number neighbors in the kriging. This is done using the `nmax` argument. For a given location, the weights assigned to observations far away are very small, so it is efficient to limit how many are used in the simulation.

As an example, here we generate four realizations of the county-level storm total rainfall for Fay and limit the neighborhood to 50 of the closest observation sites. Note that it may take a few seconds.
```{r}
ipl.sim <- krige(tpm ~ 1, 
                 locations = FR.sf, 
                 newdata = FL.sf, 
                 model = FR.vm, 
                 nsim = 4, 
                 nmax = 50)
```

Simulations are conditional on the observed rainfall and the variogram model using block kriging on the counties.

```{r}
library(tmap)

tm_shape(ipl.sim) +
    tm_polygons(col = c("sim1", "sim2", "sim3", "sim4"),
                palette = "Greens",
                title = "Simulated Rainfall [cm]") +
    tm_facets(free.scales = FALSE) 
```

The overall pattern of rainfall remains the same, but there are differences especially in counties with fewer observations and in counties where the rainfall gradients are sharp.


## Example 2: Beauregard Tornado

BeauregardTornado.Rmd

## Cokriging

Statistical interpolation can be extended to obtain surfaces of multiple field variables. The idea is that if two field variables are correlated then information about the spatial correlation in one field variable can help provide information about values in the other field variable. The spatial variability of one variable is correlated with the spatial variability of the other variable. And this idea is not limited to only two variables. 

Here we return to the Meuse River data. Concentrations of heavy metals in the flood plain of the Meuse River in Holland.
```{r}
library(gstat)
library(sp)

data(meuse)
names(meuse)
coordinates(meuse) <- ~ x + y
prj <- paste("+init=epsg:28992", 
             "+towgs84=565.237,50.0087,465.658,-0.406857,0.350733,-1.87035,4.0812")
proj4string(meuse) <- CRS(prj)
```

Suppose we are interested in the spatial distribution of all the heavy metals in the soils along the river (Not just zinc like we looked in Lesson 22). 

First we need to organize the data as a `gstat` object. This is done with the `gstat()` function which orders (and copies) the field variables into a single object. It is done successively with the `gstat()` function.

Here we specify the trend using the square root of the distance to river as we did previously.
```{r}
g <- gstat(NULL, "logCd", log(cadmium) ~ sqrt(dist), meuse)
g <- gstat(g, "logCu", log(copper) ~ sqrt(dist), meuse)
g <- gstat(g, "logPb", log(lead) ~ sqrt(dist), meuse)
g <- gstat(g, "logZn", log(zinc) ~ sqrt(dist), meuse)
g
```

Next we use the `variogram()` function to compute empirical variograms. The function, when operating on a `gstat` object, computes all direct and cross variograms.
```{r}
v <- variogram(g)
plot(v)
```

The plot method displays the set of direct and cross variograms. The direct variograms are shown in the four panels along the diagonal of the triangle of plots.

The cross variograms are shown in the six panels below the diagonal. For example, the cross variogram between the values of cadmium and copper is given in the second row of the first column. 

Q: The cross variogram is similar to what function from the `spatstat` package?

Next we use `fit.lmc()` to fit separate models to each of the empirical variograms. We use an initial partial sill and nugget equal to one and a range of 800.
```{r}
vm <- fit.lmc(v, g, 
              vgm(psill = 1, model = "Sph", 
              range = 800, nugget = 1))
plot(v, vm)
```

As the variograms indicate, the variables have a strong cross correlations. Because these variables are co-located, we can also compute direct correlations.
```{r}
cor(as.data.frame(meuse)[c("cadmium", "copper", "lead", "zinc")])
```

The correlation matrix confirms strong cross correlation among the four variables at zero lag. The cross variogram generalizes these correlations across lag distance.

Given the variogram models, cokriged maps are produced using the `predict()` method after setting the grid locations for interpolation. The CRS for the grid locations matches the CRS of the data.
```{r}
data(meuse.grid)
coordinates(meuse.grid) <- ~ x + y
proj4string(meuse.grid) <- CRS(proj4string(meuse))
cok <- predict(vm, meuse.grid)
names(cok)
```

The predictions for logarithm of zinc concentration are plotted.
```{r}
spplot(cok, "logZn.pred")
```

Compare with predictions using only zinc.
```{r}
v2 <- variogram(log(zinc) ~ sqrt(dist), 
               data = meuse)
vm2 <- fit.variogram(v2, vgm(psill = .15, model = "Sph", 
                             range = 800, nugget = .1))
uk <- krige(log(zinc) ~ sqrt(dist), meuse, newdata = meuse.grid, 
              model = vm2)
spplot(uk, "var1.pred")
```

The predicted covariances between zinc and cadmium are plotted.
```{r}
spplot(cok, "cov.logCd.logZn")
```

The map shows areas of the flood plain with high (and low) correlations between cadmium and zinc.

Obtaining a quality statistical interpolation is a nuanced process but with practice kriging can be an important tool in a spatial analyst's toolbox.

## Compare kriging surfaces with surface from a random forest algorithm

https://github.com/thengl/GeoMLA

## Geostatistical modeling using INLA

https://www.paulamoraga.com/book-geospatial/sec-geostatisticaldataexamplespatial.html

## Problem Set #5

Consider the data set `parana` from the {geoR} package containing rainfall values from the state of Paran\`a. The data are average rainfall (mm) over the period May-June.

1. Plot the values. Plot the residuals after a 1st order trend is removed.
2. Compute and plot an empirical variogram on the residuals after removing the 1st-order trend. Check for anisotropy.
3. Fit a variogram model from the gaussian family to the empirical variogram. Assume isotropy. Set the initial values of the nugget to 500, the partial sill to 800 and the range to 100. Plot the modeled variogram on top of the empirical variogram.
4. Create a kriged surface by interpolating the rainfall values to locations on a regular grid. Use the `pred_grid(parana$borders, by = 2)` function to generate the grid.
5. Plot the interpolated surface. Note this can be done with the `image()` function. Use `col = rev(terrain.colors(21))`. 
6. Compute the cross-validated prediction errors assuming a constant variogram model. Plot the observed versus predicted values as a scatter plot. Include the best fit line and the y = x line.
