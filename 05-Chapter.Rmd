# Analyzing and modeling spatial data aggregated to areal units

**"An awful lot of time I spend "coding" is actually spent copying and pasting (And much of the rest is spent googling)."**--- Meghan Duffy

## Quantifying spatial autocorrelation

In this chapter we will use functions from the {spdep} package (Roger Bivand et al.). Functions in this package provide tools analyzing and modeling data that are aggregated to polygons. This type of data is widely used in the social sciences including demography and epidemiology. Sometimes called lattice data.

The concept of spatial autocorrelation plays a central role in spatial statistics. How it gets estimated and exploited depends on the spatial geometry of the data. For example, Ripley's K function estimates spatial autocorrelation of events as point geometries. 
In the case of data aggregated to polygons (or raster cells) spatial autocorrelation measures the degree to which attribute values tend to cluster. Recall attributes are variables in a spatial data frame (either as a column in a simple feature object or as a layer in a raster object). 

Temporal versus spatial autocorrelation

Spatial clustering arises from:

* Association: whatever is causing an attribute to have a certain value in one area causes the attribute to have a similar value in areas nearby. Crime rates in nearby neighborhoods might tend to cluster due to similar factors such as economic status and the amount of policing. Non-infectious diseases (e.g., lung cancer) might have similar rates in neighborhoods close to an oil refinery.

* Causality: something within a given area directly influences outcomes within nearby areas. The broken window theory of crime suggests that poverty, lack of maintenance, and petty crime tends to breed more crime due to a perceived breakdown in civil order.

* Interaction: the movement of people, goods or information creates relationships between areas. Infectious diseases might spread from a source region thus increasing the disease rates in surrounding areas as the direct result of contact or movement of people between regions.

Spatial statistics can quantify, and condition on, spatial autocorrelation but are silent about physical causes. Understanding the reason for spatial autocorrelation is important for causal inference because the causal mechanism might be confounded (statistically) by it. Divorce rates are high in states in the South but so is the number of Waffle Houses.

Understanding causation requires domain specific knowledge.

### Quantifying spatial autocorrelation

When variables are spatially aggregated to regions, autocorrelation is quantified by calculating how similar a value in region $i$ is to the value in region $j$ and weighting this similarity by how 'close' region $i$ is to region $j$. Closer regions have greater weight.

High similarities with high weight (similar values close together) yield high values of spatial autocorrelation. Low similarities with high weight (dissimilar values close together) yield low values of spatial autocorrelation. 

Let $\hbox{sim}_{ij}$ denote the similarity between values $Y_i$ and $Y_j$, and let $w_{ij}$ denote a weight describing the 'distance' between regions $i$ and $j$, for $i$, $j$ = 1, ..., $N$. 

Then a spatial autocorrelation index (SAI) is given by
$$
\hbox{SAI} = \frac{\sum_{i,j=1}^N w_{ij}\hbox{sim}_{ij}}{\sum_{i,j=1}^N w_{ij}}
$$
which represents the weighted similarity between regions. The collection of weights is called a spatial weights matrix. The spatial weights matrix defines the neighbors for each region and their strength of association.

For cells in a raster under the rook-contiguity criterion, $w_{ij}$ = 1 if cell $i$ and $j$ share a boundary, and 0 otherwise. In this case $w_{ij}$ = $w_{ji}$. Also a cell is not a neighbor to itself so $w_{ii}$ = 0.

As an alternative we can define center locations from a set of polygon regions and let $w_{ij}$ = 1 if the center of region $i$ is near the center of region $j$ and 0 otherwise. In this case we need to decide on the number of nearest neighbors.

We can also define neighbors by distance. For example, if $d_{ij}$ is the distance between centers $i$ and $j$, we can let $w_{ij}$ = 1 if $d_{ij}$ < $\delta$ and 0 otherwise.

### An example: Columbus, Ohio crime

Consider crime data at the tract level in the city of Columbus, Ohio (Anselin, 1988: Spatial Econometrics. Boston, Kluwer Academic). The tract polygons are projected with arbitrary spatial coordinates.
```{r chapter5}
library(sf)

download.file("http://myweb.fsu.edu/jelsner/temp/data/columbus.zip",
              "columbus.zip")
unzip("columbus.zip")

CC.sf <- read_sf(dsn = "columbus", 
                 layer = "columbus")
```

The simple feature data frame contains the following names and geometry.
```{r}
names(CC.sf)
st_geometry(CC.sf)
```

Create a choropleth map of the crime rates (`"CRIME"`). Residential burglaries and vehicle thefts per 1000 households.
```{r}
library(tmap)

tm_shape(CC.sf) +
  tm_fill("CRIME",
          title = "") +
  tm_borders(col = "gray70") +
  tm_layout(title = "Burglary & Vehicle Thefts\n/1000 Households",
            legend.outside = TRUE)
```

Alternatively
```{r, eval=FALSE}
library(ggplot2)

ggplot(data = CC.sf) + 
  geom_sf(aes(fill = CRIME), col = "gray70") +
  ggtitle("Burglary & Vehicle Thefts/1000 Households")
```

High crime areas tend to cluster. Spatial autocorrelation quantifies this clustering. To quantify we first need to define a list of neighbors for each polygon. 

### List of neighbors

We create a list of neighbors using the `poly2nb()` function from the {spdep} package. The function now works on S4 spatial classes and simple features.
```{r}
library(spdep)

( nbs <- poly2nb(CC.sf) )
```

The `nb` in the `poly2nb()` function names stands for neighbor list object. The function builds the list from regions based on contiguity. Neighbors must share one or more geometry points. By default the contiguity is defined as having at least one point in common. This is changed by using the argument `queen = FALSE`.

There are 49 tracts (polygons). Each tract is bordered by at least one other tract (each tract is bordered by at least one tract). The average number of neighbors is 4.8. The total number of neighbors over all tracts is 236. This represents 9.8% of all possible connections (if every tract is a neighbor of itself and every other tract 49 * 49).

A graph of the neighbor links is obtained with the `plot()` method. The arguments include the neighbor list object (`nbs`) and the location of the polygon centers, which are extracted from the simple feature data frame using the `st_centroid()`.
```{r}
plot(nbs, 
     st_centroid(CC.sf$geometry))
```

The graph is a network showing the contiguity pattern (topology). Tracts close to the center of the city have more neighbors than those on the outskirts.

The number of links per node (tract)--link distribution--is obtained with the `summary()` method.
```{r}
summary(nbs)
``` 

The list of neighboring tracts for the first two tracts.
```{r}
nbs[[1]]
nbs[[2]]
```

The first tract has two neighbors that include tracts 2 and 3. The neighbor numbers are stored as an integer vector. Tract 2 has three neighbors that include tracts 1, 3, and 4. Tract 5 has 8 neighbors and so on. The function `card()` tallies the number of neighbors by tract.
```{r}
card(nbs)
```

Tract 5 has 8 neighbors.

### Weights matrix

Next weights are added to the neighbor list object to create a weights matrix. The weights specifying how 'close' each neighbor is. This is done with the `nb2listw()` function. The function turns the neighbor list object into a spatial weights object. By default the weighting scheme gives each link the same weight equal to the multiplicative inverse of the number of neighbors.
```{r}
wts <- nb2listw(nbs)

class(wts)
```

This new `wts` object is a list with two elements. The first element (`listw`) is the weights matrix and the second element (`nb`) is the neighbor list object.
```{r}
summary(wts)
```

The network statistics are given along with information about the weights. By default all neighboring tracts are assigned a weight equal to the inverse of the number of neighbors  (`style = "W"`). For a tract with 5 neighbors each neighbor gets a weight of 1/5. The sum over all weights (`S0`) is the number of tracts.

To see the weights for the first two tracts type
```{r}
wts$weights[1:2]
```

The `$weights` list a sparse representation of the spatial weights matrix. The full matrix has dimensions 49 x 49.

To see the neighbors of the first two tracts type
```{r}
wts$neighbours[1:2]
```

Tract 1 has two neighbors (tract 2 & 3) so each are given a weight of 1/2. Tract 2 has three neighbors (tract 1, 3, & 4) so each are given a weight of 1/3.

With the weights matrix specified and saved as an object we are ready to compute a metric of spatial autocorrelation.

Caution: Contiguity can result in areas having no neighbors; islands for example. By default the `nb2listw()` function assumes each area has at least one neighbor. If this is not the case we need to specify how areas without neighbors are handled using the argument `zero.policy = TRUE`. This permits the weights list to be formed with zero-length weights vectors.

For example, consider the districts in Scotland.
```{r}
download.file("http://myweb.fsu.edu/jelsner/temp/data/scotlip.zip",
              "scotlip.zip")
unzip("scotlip.zip")

SL.sf <- read_sf(dsn = "scotlip", 
                 layer = "scotlip")
plot(SL.sf$geometry)
```

Here there are three island districts.

Create a list of neighbors.
```{r}
( nbs2 <- poly2nb(SL.sf) )
```

Three regions with no links.

Use the `nb2listw()` function with the argument `zero.policy = TRUE`. Otherwise we get an error saying the empty neighbor sets are found.
```{r}
wts2 <- nb2listw(nbs2,
                 zero.policy = TRUE)
head(wts2$weights)
```

### Moran's I

A commonly used autocorrelation statistic is Moran's I. Moran's I follows the basic form of spatial autocorrelation indexes where the similarity between regions $i$ and $j$ is proportional to the product of the deviations from the mean such that
$$
\hbox{sim}_{ij} \propto (Y_i - \bar Y) (Y_j - \bar Y)
$$

where $i$ indexes the region and $j$ indexes the neighbors of $i$. The value of $\hbox{sim}_{ij}$ is large when the $Y$ values in the product are on the same side of their respective means and small when they are on opposite sides of their respective means.

The formula for I is
$$
\hbox{I} = \frac{N} {W} \frac {\sum_{i,j} w_{ij}(Y_i-\bar Y) (Y_j-\bar Y)} {\sum_{i} (Y_i-\bar Y)^2}
$$
where $N$ is the number regions, $w_{ij}$ is the matrix of spatial weights, and $W$ is the sum over all weights.

Consider the following spatial grid of cells containing random attribute values.
```{r}
library(spatstat)

set.seed(6750)
Y <- ppp(runif(200, 0, 1), 
         runif(200, 0, 1))
plot(quadratcount(Y), main = "")
```

The formula for I results in one value for the entire grid. 

Let's first consider a single cell on the grid ($N$ = 1). Here the middle cell (row 3, column 3). Let $i$ = 3 in the above formula and let $j$ index the cells touching the center cell in reading order starting with cell (2, 2), then cell (2, 3), etc.

Assume each neighbor is given a weight of 1/8 so $W$ = 1. Then the value of I for the single center cell is 
I_{3, 3} = (6 - mean(y)) * 
                     ((8 - mean(y)) + 
                     (3 - mean(y)) +
                     (9 - mean(y)) +
                     (12 - mean(y)) +
                     (10 - mean(y)) +
                     (10 - mean(y)) +
                     (9 - mean(y))) / 
            (6 - mean(y))^2)

```{r}
y <- c(3, 10, 7, 12, 5, 11, 8, 3, 9, 12, 
      6, 12, 6, 10, 3, 8, 10, 10, 9, 7, 
      5, 10, 8, 5, 11)
( yb <- mean(y) )
```

```{r}
Inum_i <- (6 - yb) * 
                 ((8 - yb) + (3 - yb) + (9 - yb) + 
                  (12 - yb) + (10 - yb) + (10 - yb) + 
                  (10 - yb) + (9 - yb))
Iden_i <- (6 - yb)^2
Inum_i/Iden_i
```

The I value of -3.5 indicates that the center cell which has a value below the average over all 25 cells is mostly surrounded by cells having values above the average.

Repeat this calculation for every cell and then take the sum.

This is what the function `moran()` from the {spdep} package does. The first argument is the vector containing the values for which we are interested in determining the magnitude of the spatial autocorrelation and the second argument is the `listw` object. 

Further, we need to specify the number of regions and the sum of the weights `S0`. The latter is obtained from the `Szero()` function applied to the `listw` object.

Returning to the Columbus crime data here we let `m` be the number of census tracts and `s` be the sum of the weights. We then apply the `moran()` function on the variable `CRIME` in
```{r}
m <- length(CC.sf$CRIME)
s <- Szero(wts)

moran(CC.sf$CRIME, 
      listw = wts, 
      n = m, 
      S0 = s)
```

The function returns the Moran's I statistic and the kurtosis (K) of the distribution of crime values. Moran's I ranges from -1 to +1. The value of .5 for the crime rates indicates fairly high spatial autocorrelation. This is expected based on the clustering of crime in the central city. Positive values of Moran's I indicate clustering and negative values indicate inhibition.

Kurtosis is a statistic measuring the peakedness of the distribution of the attribute values. A normal distribution has a kurtosis of 3. If the kurtosis is too large (or small) relative to a normal distribution then the inferences we make with Moran's I are suspect.

Q: Which of the two variables (`INC` or `HOVAL`) shows the most clustering?

Another index of spatial autocorrelation is the Geary's C statistic. The equation is 
$$
\hbox{C} = \frac{(N-1) \sum_{i,j} w_{ij} (Y_i-Y_j)^2}{2 W \sum_{i}(Y_i-\bar Y)^2} 
$$
where again $W$ is the sum over all weights ($w_{ij}$) and $N$ is the number of areas.

The syntax of the `geary()` function is similar to the syntax of the `moran()` function except we also specify `n1` to be one minus the number of polygons.
```{r}
geary(CC.sf$CRIME, 
      listw = wts,
      n = m, 
      S0 = s, 
      n1 = m - 1)
```

Values for Geary's C range from 0 to 2 with 1 indicating no spatial autocorrelation. Values less than 1 indicate positive autocorrelation. Both I and C are global measures of autocorrelation, but C is more sensitive to local variations in autocorrelation.

Rule of thumb: If the interpretation of Geary's C is much different than the interpretation of Moran's I then consider computing local measures of spatial autocorrelation.

### Spatial lag

Moran's I is the slope coefficient from a regression of the weighted average of the neighborhood values onto the observed values. The weighted average of neighborhood values is called the spatial lag.

Let `crime` be the set of crime values in each region. We create a spatial lag variable using the `lag.listw()` function. The first argument is the `listw` object and the second is the vector of crime values.
```{r}
crime <- CC.sf$CRIME
Wcrime <- lag.listw(wts, 
                    crime)
```

For each value in the vector `crime` there is a corresponding value in the vector `Wcrime` representing the average crime over the neighboring regions.

Recall tract 1 had tract 2 and 3 as its only neighbors. So the following should return a `TRUE`.
```{r}
Wcrime[1] == (crime[2] + crime[3])/2
```

A scatter plot of the neighborhood average crime rates versus the individual polygon crime rates in each shows there is a relationship.
```{r}
library(dplyr)
library(ggplot2)

data.frame(crime, Wcrime) %>%
ggplot(aes(x = crime, y = Wcrime)) + 
  geom_point() + 
  geom_smooth(method = lm) +
  scale_x_continuous(limits = c(0, 70)) +
  scale_y_continuous(limits = c(0, 70)) +
  xlab("Crime") + 
  ylab("Average Crime in the Neighborhood") +
  theme_minimal()
```

The vertical axis contains the neighborhood average crime rate. The range of neighborhood averages is smaller than the range of individual polygon crime rates.

Tracts with low values of crime tend to be surrounded by tracts with low values of crime on average and tracts with high values of crime tend be surrounded by tracts with high values of crime. The slope is upward (positive).

The magnitude of the slope is the Moran's I value. To check this type
```{r}
lm(Wcrime ~ crime)
```

The coefficient on the `crime` variable in the simple linear regression is .5. 

The scatter plot is a  Moran's scatter plot.

Let's consider another data set.
```{r}
download.file("http://myweb.fsu.edu/jelsner/temp/data/sids2.zip",
              "sids2.zip")
unzip("sids2.zip")

SIDS.sf <- read_sf(dsn = "sids2")
st_crs(SIDS.sf) <- 4326
head(SIDS.sf)
```

The column `SIDR79` contains the death rate per 1000 live births (1979-84) from sudden infant death syndrome. Create a choropleth map of the SIDS rates.
```{r}
tm_shape(SIDS.sf) +
  tm_fill("SIDR79", title = "") +
  tm_borders(col = "gray70") +
  tm_layout(title = "SIDS Rates 1979-84 [per 1000]",
            legend.outside = TRUE)
```

Again, alternatively we can use `geom_sf()`.
```{r, eval=FALSE}
ggplot(data = SIDS.sf) + 
  geom_sf(aes(fill = SIDR79), col = "gray70") +
  ggtitle("SIDS Rates 1979-84 [per 1000]")
```

Create a neighborhood list (`nb`) and a `listw` object (`wts`) then graph the neighborhood network.
```{r}
nb <- poly2nb(SIDS.sf)
wts <- nb2listw(nb)

plot(nb, 
     st_centroid(st_geometry(SIDS.sf)))
```

Next compute Moran's I on the SIDS rates over the period 1974--1979.
```{r}
m <- length(SIDS.sf$SIDR79)
s <- Szero(wts)

moran(SIDS.sf$SIDR79, 
      listw = wts, 
      n = m, 
      S0 = s)
```

I is .14 and K is 4.4. A normal distribution has a kurtosis of 3. Values less than about 2 or greater than about 4 indicate that inferences about autocorrelation based on the assumption of normality are suspect.

Weights are specified using the `style =` argument in the `nb2listw()` function. The default "W" is row standardized (sum of the weights over all links equals the number of polygons). "B" is binary (each neighbor gets a weight of one). "S" is a variance stabilizing scheme. 

Each style gives a somewhat different value for I.
```{r}
x <- SIDS.sf$SIDR79
moran.test(x, nb2listw(nb, style = "W"))$estimate[1]
moran.test(x, nb2listw(nb, style = "B"))$estimate[1]  # binary
moran.test(x, nb2listw(nb, style = "S"))$estimate[1]  # variance-stabilizing
```

Thus when we report a Moran's I we need to state what weighting scheme was used.

Let `sids` be a vector with elements containing the SIDS rate in each county. We create a spatial lag variable using the `lag.listw()` function. The first argument is the `listw` object and the second is the vector of rates.
```{r}
sids <- SIDS.sf$SIDR79
Wsids <- lag.listw(wts, 
                   sids)
```

For each value in the vector `sids` there is a corresponding value in the object `Wsids` representing the neighborhood average SIDS rate. 
```{r}
Wsids[1]
j <- wts$neighbours[[1]]
j
sum(SIDS.sf$SIDR79[j])/length(j)
```

The weight for county one is `Wsids[1]` = 2.659. The neighbor indices for this county are in the vector `wts$neighbours[[1]]` of length 3. Add the SIDS rates from those counties and divide by the number of counties (`length(j)`).

A scatter plot of the neighborhood average SIDS rate versus the actual SIDS rate in each region.
```{r}
data.frame(sids, Wsids) %>%
ggplot(aes(x = sids, y = Wsids)) + 
  geom_point() + 
  geom_smooth(method = lm) +
  scale_x_continuous(limits = c(0, 7)) +
  scale_y_continuous(limits = c(0, 7)) +
  xlab("SIDS") + ylab("Spatial Lag of SIDS") +
  theme_minimal()
```

The slope of the regression line is upward indicating positive spatial autocorrelation. The value of the slope is I. To check this type
```{r}
lm(Wsids ~ sids)
```

## Statistical significance of spatial autocorrelation

Even in a random pattern of attribute values across a spatial domain the value of $I$ will almost certainly be non-zero. If we want to use $I$ to establish clustering we need a way to guard against being fooled by this randomness.

Is the value of Moran's I significant with respect to the null hypothesis of no spatial autocorrelation? One way to answer this question is to draw an uncertainty band on the regression line and see if a horizontal line can be placed within the band. If not, then I is statistically different than what we would expect if the null hypothesis of no spatial autocorrelation were true.

More formally the question is answered by comparing the standard deviate ($z$ value) of the I statistic with a standard normal deviate. This is done using the `moran.test()` function, where the $z$ value is the difference between I and the expected value of I divided by the square root of the variance of I. 

The function takes a variable name or numeric vector and a spatial weights list object in that order. The argument `randomisation = FALSE` means the variance of I is computed under the assumption of normally distributed SIDS rates.
```{r}
( mt <- moran.test(sids, 
                   listw = wts,
                   randomisation = FALSE) )
```

I is .143 with a variance of .0043. The $z$ value for the I statistic is 2.3438 giving a $p$-value less than .01 under the null hypothesis of no spatial autocorrelation. Thus we reject the null hypothesis and conclude there is weak but significant spatial autocorrelation in SIDS rates across North Carolina at the county level.

To check on things first look at the structure of the output with the `str()` function.
```{r}
str(mt)
```

The list element called `estimate` is a vector of length three containing the Moran's I statistic, the expected value of Moran's I under the assumption of uncorrelated normally distributed SIDS rates, and the variance of Moran's I. 

The $z$ value is the difference between the statistic and it's expected value divided by the square root of the variance.
```{r}
( mt$estimate[1] - mt$estimate[2] ) / sqrt(mt$estimate[3])
```

The $p$-value arises as the area under a standard normal distribution curve to the right (`lower.tail = FALSE`) of 2.3438 (`mt$statistic`).
```{r}
pnorm(mt$statistic, 
      lower.tail = FALSE)

curve(dnorm(x), from = -4, to = 4, lwd = 2)
abline(v = mt$statistic, col = 'red')
```

So slightly less than 1% of the area lies to the right of the red line.

#### About statistical significance

Recall the $p$-value summarizes the evidence in support of the null hypothesis. The smaller the $p$-value, the less evidence there is in support of the null hypothesis. 

In this case it is the probability that the county SIDS rates could have been arranged at random across the state if the null hypothesis is true. The small $p$-value tells us that the spatial arrangement of our data is unusual with respect to the null hypothesis.

The interpretation of the $p$-value is stated as evidence AGAINST the null hypothesis. This is because our interest lies in the null hypothesis being untenable. 

$p$-value        | Statement of evidence against the null
---------------- | ---------------------
less than  .01   | convincing
.01 - .05        | moderate 
.05 - .15        | suggestive, but inconclusive
greater than .15 | no

Under the assumption of normal distributed and uncorrelated data, the expected value for Moran's I is -1/(n-1) where n is the number of counties. 

A check on the distribution of SIDS rates indicates that normality is somewhat suspect. Recall a good way to check the normality assumption is to use the `sm.density()` function from the {sm} package.
```{r}
sm::sm.density(sids, 
               model = "Normal",
               xlab = "SIDS Rates 1979-84 [per 1000]")
```

The SIDS rates are more "peaked" (higher kurtosis) than a normal distribution. In this case it is better to use the default `randomisation = TRUE` argument.

Further, the assumptions underlying Moran's test are sensitive to the form of the graph of neighbour relationships and other factors so results should be checked against a test that involves permutations.

### Monte Carlo approach to inference

A permutation test for Moran's I is performed with the `moran.mc()` function. MC stands for Monte Carlo which refers to the city of Monte Carlo in Monaco famous for its gambling casinos. The MC procedure refers to random sampling.

The name of the data vector and the weights list object (`listw`) are required arguments as is the number of permutations (`nsim`). Each permutation is a random rearrangement of the SIDS rates across the counties. This removes the spatial autocorrelation but keeps the non-spatial distribution of the SIDS rates. The neighbor topology and weights remain the same.

For each permutation (random shuffle), I is computed and saved. The $p$-value is obtained as the ratio of the number of permuted I values greater or exceeding the observed I over the number of permutation plus one. In the case where there are 5 permuted I values greater or equal to the observed value based on 99 simulations, the $p$-value is 5/(99 + 1) = .05.

For example, if you want inference on I using 99 permutations type
```{r}
set.seed(4102)

( mP <- moran.mc(sids, 
                 listw = wts,
                 nsim = 99) )
```

Two of the permutations yield a Moran's I greater than 0.1428, hence the $p$-value as evidence in support of the null hypothesis (the true value for Moran's I is zero) is .02.

Note: Here we initiate the random number generator to a specific seed value so that the set of random permutations of the values across the domain will be the same each time we knit this Rmd. This is important for reproducibility. The default random number generator seed value is determined from the current time (internal clock) and so no random permutations will be identical. To control the seed use the `set.seed()` function.

The values of I computed for each permutation are saved in the vector `mP$res`.
```{r}
head(mP$res)
tail(mP$res)
```

The last value in the vector is I computed using the data in the correct counties. The $p$-value as evidence in support of the null hypothesis that I is zero is given as
```{r}
sum(mP$res > mP$res[100])/99
```

A density graph displays the distribution of permuted I's. First, rerun using 999 simulations. Then plot a density curve and add a vertical line at the value of Moran's I computed from the data at the actual locations.
```{r}
mP <- moran.mc(sids, 
               listw = wts, 
               nsim = 999)

df <- data.frame(mp = mP$res[-1000])
ggplot(df, aes(mp)) + 
  geom_density() + 
  geom_rug() + 
  geom_vline(xintercept = mP$res[1000], 
             color = "red", size = 2) +
  theme_minimal()
```

The density curve is centered just to the left of zero consistent with the theoretical expectation (mean) of -.01. Also note that the right tail is fatter than the left tail. This is due to the skewness of the rates used in the calculation of I.

What do you do with the knowledge that the SIDS rates have significant spatial autocorrelation? By itself not much but it can provide notice that something might be going on in certain regions (hot spot analysis).

More typically the knowledge is useful after other known factors are considered. Or in the language of statistics, knowledge of significant spatial autocorrelation in the model residuals can help you build a better model.

### Spatial autocorrelation in model residuals

A spatial regression model may be needed whenever the residuals from a non-spatial regression model exhibit significant spatial autocorrelation. So a common way to proceed is to first regress the response variable onto the explanatory variables and check for spatial autocorrelation.

Even if the response variable indicates a high level of spatial autocorrelation it might not be necessary to use a spatial regression model if the explanatory variables remove this correlation.

Let's return to the Columbus crime data and fit a linear regression model with `CRIME` as the response variable and `INC` and `HOVAL` as the explanatory variables.
```{r}
model <- lm(CRIME ~ INC + HOVAL, 
            data = CC.sf)
summary(model)
```

The model statistically explains 55% of the variation in crime. As income and housing values increase crime goes down. 

We use the `residuals()` method to extract the vector of residuals from the model.
```{r}
res <- residuals(model)
```

We then check on the distribution of the residuals relative to a normal distribution.
```{r}
sm::sm.density(res, 
               model = "Normal")
```

The next step is to create a choropleth map of the model residuals. Are the residuals clustered?
```{r}
CC.sf$res <- res

library(tmap)
tm_shape(CC.sf) +
  tm_fill("res") +
  tm_borders(col = "gray70") +
  tm_layout(title = "Linear Model Residuals")
```

Yes. There are clustered regions where the model over predicts crime conditional on household income and housing values and where it under predicts crime.

The amount of clustering is less than before. That is, after accounting for regional factors related to crime the spatial autocorrelation is reduced.

To determine I on the residuals we use the `lm.morantest()` function and pass the regression model object and the weights object to it.
```{r}
nbs <- poly2nb(CC.sf)
wts <- nb2listw(nbs)
lm.morantest(model, wts)
```

Moran's I on the model residuals is .222.  This compares with the value of .5 on crime alone. Part of the spatial autocorrelation is absorbed by the explanatory factors.

Do we need a spatial regression model?  The output gives a $p$-value on I of .002, thus we reject the null hypothesis of no spatial autocorrelation in the residuals and conclude that a spatial regression model would improve the fit.  

The $z$-value takes into account the fact that these are residuals so the variance is adjusted accordingly.

The next step is to choose a spatial regression model.

### Challenge

Download the county-level police data for the state of Mississippi. Compute Moran's I and Geary C statistics for the percentage of whites (`WHITE`) and test for statistical significance against the null hypothesis of no spatial autocorrelation. Create a Moran's scatter plot.

Read the data as a spatial polygons data frame.
```{r}
download.file("http://myweb.fsu.edu/jelsner/temp/data/police.zip",
                    "police.zip")
unzip("police.zip")
PE.sf <- read_sf(dsn = "police", 
                 layer = "police")
st_crs(PE.sf) <- 4326
```

### Percent of whites in Mississippi counties

Download the county-level police data. Import the data as a simple feature data frame and assign the geometry a geographic CRS. The file *police.zip* contains shapefiles in a folder called *police* on my website. 
```{r}
download.file("http://myweb.fsu.edu/jelsner/temp/data/police.zip",
                    "police.zip")
unzip("police.zip")

library(sf)

PE.sf <- read_sf(dsn = "police", 
                 layer = "police")
st_crs(PE.sf) <- 4326
names(PE.sf)
```

Variables include police expenditures (`POLICE`), crime (`CRIME`), income (`INC`), unemployment (`UNEMP`) and other socio-economic characteristics across Mississippi at the county level. The police expenditures are per capita 1982 (dollars per person). The personal income per county resident, 1982 (dollars per person). The crime is the number of serious crimes per 100,000 residents, 1981. Unemployment is percent unemployed in 1980.

The geometries are polygons defining the county borders.
```{r}
plot(PE.sf$geometry)
```

Recall we first need to assign the neighborhoods and the associated weights between the each region and each neighbor of that region. One way to do this is based on contiguity with the weights based on row standardization (default `style = "W"`) using the `poly2nb()` and `nb2listw()` functions in that order from the {spdep} package.
```{r}
library(spdep)

nbs <- poly2nb(PE.sf)
wts <- nb2listw(nbs)
```

Another way is to specify the number of neighbors and define who the neighbors are based on distance. We do this with the `knearneigh()` function. We first extract the coordinates of the polygon centroids.
```{r}
library(dplyr)

coords <- PE.sf %>%
  st_geometry() %>%
  st_centroid() %>%
  st_coordinates()
head(coords)
```

We can specify that each county for example has six neighbors where the neighbors are based on proximity. Since the CRS is geographic we include the `longlat = TRUE` argument so the distances are based on great circles.
```{r}
knn <- knearneigh(coords, 
                  k = 6, 
                  longlat = TRUE)
head(knn$nn)
```

The output is a list of five elements with the first being a matrix of dimension number of counties by the number of neighbors. We see here that the neighboorhoods are non-symmetric. County 3 is a neighbor of county 2, but county 2 is not a neighbor of county 3. This is important since some spatial regression models require symmetric neighborhood definitions.

We turn this list into a neighborhood object (class `nb`) with the `knn2nb()` function. 
```{r}
nbs2 <- knn2nb(knn)
summary(nbs2)
```

The argument `sym = TRUE` will force the output neighbors list to be symmetric. Let's create another neighborhood object.
```{r}
nbs3 <- knn2nb(knn,
               sym = TRUE)
summary(nbs3)
```

Compare the neighborhood topologies.
```{r, eval=FALSE}
par(mfrow = c(1, 2))

plot(st_geometry(PE.sf), border = "grey")
plot(nbs, coords, add = TRUE)

plot(st_geometry(PE.sf), border = "grey")
plot(nbs2, coords, add = TRUE)
```

Create weight matrices for these alternative neighborhood definitions using the same `nb2listw()` function.
```{r}
wts2 <- nb2listw(nbs2)
wts3 <- nb2listw(nbs3)
```

We then compute Moran's I for the variable percentage white (`WHITE`) with the `moran()` function. And we do it separately for the three different weight matrices.
```{r}
moran(PE.sf$WHITE,
       listw = wts,
       n = length(nbs),
       S0 = Szero(wts))

moran(PE.sf$WHITE,
       listw = wts2,
       n = length(nbs2),
       S0 = Szero(wts2))

moran(PE.sf$WHITE,
       listw = wts3,
       n = length(nbs3),
       S0 = Szero(wts3))
```

Values of Moran's I are constrained between -1 and +1. In this case the neighborhood definition has little or no impact on inferences made about spatial autocorrelation. The kurtosis is between 2 and 4 consistent with a set of values from a normal distribution.

In a similar way we compute Geary's c statistic.
```{r}
geary(PE.sf$WHITE, 
      listw = wts,
      n = length(nbs), 
      S0 = Szero(wts), 
      n1 = length(nbs) - 1)
```

Values of Geary's c are between 0 and 2 with values less than one indicating positive autocorrelation. If the interpretation on the amount of spatial autocorrelation based on Geary's c is different than the interperation on the amount of autocorrelation based on Moran's I then it might be a good idea to examine local variations in autocorrelation.

Finally, recall that to create a Moran's scatter plot. We first assign to `white` the percentage of whites in each county (variable `WHITE`) and to `Wwhite` the spatial lagged value of percentage of whites. We then use `ggplot()` as follows.
```{r}
white <- PE.sf$WHITE
Wwhite <- lag.listw(wts, 
                    white)

library(ggplot2)
data.frame(white, Wwhite) %>%
ggplot(aes(x = white, y = Wwhite)) + 
  geom_point() + 
  geom_smooth(method = lm) +
  scale_x_continuous(limits = c(0, 100)) +
  scale_y_continuous(limits = c(0, 100)) +
  xlab("% White") + ylab("Avg of % White in the Neighborhood\n(Spatial Lag)") +
  theme_minimal()
```

The line is the best-fit linear regression of `Wwhite` onto `white` and it's slope is equal to the value of Moran's I. This is true regardless of the neighborhood definition used to compute to
```{r}
lm(Wwhite ~ white)
```

We test for significant spatial autocorrelation with the `moran.test()` function.
```{r}
moran.test(white, listw = wts)
```

We see that the value of .56 is much larger than the expected value under the null hypothesis of no autocorrelation (-.012 = -1/(n-1)).

We can also use the `moran.mc()` function as a non-parametric test for significance. It computes Moran's I for each resampling of the data allowing your to establish the rank of the observed statistic in relation to the statistic computed on the samples. The data are resampled by randomly shuffling them across the geometric features. The distribution of the Moran's I computed on the resampled data is a way to visualize the null hypothesis.


### Bivariate spatial autocorrelation

The idea of spatial autocorrelation can be extended to two variables. It is motivated by the fact that aspatial bivariate association measures, like Pearson's correlation coefficient, does not recognize spatial arrangement aspects of the data.

Consider for example the correlation between police expenditure (`POLICE`) and the amount of crime (`CRIME`) in the police expenditure data set.
```{r}
police <- PE.sf$POLICE
crime <- PE.sf$CRIME

cor.test(police, crime)
```

We find a significant (direct) correlation ($p$-value < .01) between these two variables under the null hypothesis that they are uncorrelated. But we also note some spatial autocorrelation in these variables.
```{r}
moran.test(police, listw = wts)
moran.test(crime, listw = wts)
```

The Lee statistic [Lee (2001). Developing a bivariate spatial association measure: An integration of Pearson's r and Moran's I. J Geograph Syst 3: 369-385.] integrates the Pearson's r as an aspatial bivariate association metric and Moran's I as a univariate spatial autocorrelation metric. The formula is
$$
L(x,y) = \frac{n}{\sum_{i=1}^{n}(\sum_{j=1}^{n}w_{ij})^2}
\frac{\sum_{i=1}^{n}(\sum_{j=1}^{n}w_{ij}(x_i-\bar{x})) ((\sum_{j=1}^{n}w_{ij}(y_j-\bar{y}))}{\sqrt{\sum_{i=1}^{n}(x_i - \bar{x})^2} \sqrt{\sum_{i=1}^{n}(y_i - \bar{y})^2}}
$$

The formula is implemented in the `lee()` function where the first two arguments are the variables of interest and we need to include the weights matrix and the number of regions. The output from this function is a list of two with the first being the value of Lee's statistic (`L`).
```{r}
lee(crime, police, 
    listw = wts, 
    n = length(nbs))$L
```

Values can range between -1 and +1 with the present value of .13 indicating relatively weak bivariate spatial autocorrelation between crime and police expenditures. Statistically we can infer that crime in neighboring counties has some influence on police expenditure in each county, but not much.

Neither values in `crime` nor values in `police` are adequately described by a normal distribution.
```{r, eval=FALSE}
sm::sm.density(crime, model = "normal")
sm::sm.density(police, model = "normal")
```

Thus we perform a non-parametric test on the bivariate spatial autocorrelation with the `lee.mc()` function. The crime and police expenditure values are randomly permuted and values of `L` are computed for each permutation.
```{r}
lee.mc(crime, police, listw = wts, nsim = 999)
```

We conclude that there is no significant bivariate spatial autocorrelation between crime and police expenditure.

## Local indicators of spatial autocorrelation (LISA)

**"When I'm explaining some of the tidy verse principles and philosophy in R statistics, I often break down a home baked chunk of code and illustrate that 'it says what it does and it does what it says.'** --- Diane Beldame

The Moran's I statistic was first used in the 1950s. Localization of this statistic was presented by Luc Anselin in 1995 (Anselin, L. 1995. Local indicators of spatial association, Geographical Analysis, 27, 93–115).

We saw the `raster::MoranLocal()` function will compute local Moran's I on rasters and return a raster.

Local I is a deconstruction of global I where geographic proximity is used in two ways. (1) to define and weight neighbors and (2) to determine the spatial scale over which I is computed. Illustrate on the board.

Using queen's contiguity we determined the neighborhood topology and the weights for the police expenditure data from Mississippi. Here we print them in the full matrix form with the `list2mat()` function.
```{r}
round(listw2mat(wts)[1:5, 1:10], 2)
```

The matrix shows that the first county has three neighbors 2, 3, and 9 and each get a weight of 1/3. The third county has four neighbors 1, 4, 9 and 10 and each gets a weight of 1/4.

Compute local Moran's I with the `localmoran()` function. Two arguments are needed (1) the attribute variable for which we want to compute local correlation and (2) the weights matrix as a list object.
```{r}
Ii_stats <- localmoran(PE.sf$WHITE, 
                       listw = wts)
str(Ii_stats)
```

The local I is given in the first column of a matrix where the rows are the counties. The other columns are the expected value for I, the variance of I, the $z$ value and the $p$-value. For example, the local I statistics from the first county are given by typing
```{r}
head(Ii_stats)
```

Because these local values must average to the global value (when using row standardized weights), they can take on values outside the range between -1 and 1. A `summary()` method on the first column of the `Li`  object gives statistics from the non-spatial distribution of I's.
```{r}
summary(Ii_stats[, 1])
```

To map the values we first attach the matrix columns of interest to the simple feature data frame. Here we attach `Ii`, `Var`, and `Pi`.
```{r}
PE.sf$Ii <- Ii_stats[, 1]
PE.sf$Vi <- Ii_stats[, 3]
PE.sf$Pi <- Ii_stats[, 5]
```

Map the local spatial autocorrelation.
```{r}
( g1 <- ggplot(data = PE.sf) +
  geom_sf(aes(fill = Ii)) +
  scale_fill_gradient2(low = "green",
                       high = "blue") )
```

Map the variances.
```{r}
ggplot(data = PE.sf) +
  geom_sf(aes(fill = Vi)) +
  scale_fill_gradient()
```

Variances are larger for counties near the boundaries as the sample sizes are smaller.

Compare the map of local autocorrelation with a map of percent white. 
```{r}
( g2 <- ggplot(data = PE.sf) +
  geom_sf(aes(fill = WHITE)) +
  scale_fill_gradient(low = "black",
                      high = "white") )
```

Plot them together.
```{r, eval=FALSE}
library(gridExtra)

grid.arrange(g1, g2, nrow = 1)
```

Areas where percent white is high over the northeast are areas with the largest spatial correlation. Other areas of high spatial correlation include the Mississippi Valley and in the south. Note the county with the most negative spatial correlation is the county in the northwest with a fairly high percentage of whites neighbored by counties with much lower percentages of whites.

Local values of Lee's bivariate spatial autocorrelation are available from the `lee()` function.
```{r}
lee_stat <- lee(crime, police, 
                listw = wts, 
                n = length(nbs))

PE.sf$localL <- lee_stat$localL

library(tmap)

tm_shape(PE.sf) +
  tm_fill("localL",
          title = "") +
  tm_borders(col = "gray70") +
  tm_layout(title = "Local Bivariate Spatial Autocorrelation",
            legend.outside = TRUE)
```

Areas in dark green indicate where the correlation between crime and policing is most influenced by neighboring crime and policing.

#### Population and tornadoes in Iowa

We are interested in quantifying the bivariate spatial autocorrelation between tornado occurrences and population.

Get U.S. Census data with functions from the {tidycensus} package. When working outside this class you first need to obtain a API key from http://api.census.gov/data/key_signup.html. Then set and save the key.
```{r, eval=FALSE}
library(tidycensus)
census_api_key("YOUR API KEY GOES HERE INSIDE THE QUOTES", install = TRUE)
```

The `get_decennial()` function grants access to the 1990, 2000, and 2010 decennial US Census data and the `get_acs()` function grants access to the 5-year American Community Survey data. For example, here is how you would get county-level population  for Iowa.
```{r, eval=FALSE}
IA_counties.sf <- get_acs(geography = "county", 
                          variables = "B02001_001E", 
                          state = "IA", 
                          geometry = TRUE)
```

This returns a simple feature data frame with county borders as multipolygons. The variable `B02001_001E` is the 2015 (mid year of the 5-year period 2013-2017) population in each county.

I then write it out to my working directory as an ESRI Shapefile and zip it.
```{r, eval=FALSE}
st_write(IA_counties.sf, 
         dsn = "IA_Counties", 
         driver = "ESRI Shapefile")

zip(zipfile = "IA_Counties.zip", 
    files = "IA_Counties")
```

I then upload it to the RStudio Cloud Lesson 10 project so that we can import it.
```{r}
IA_counties.sf <- st_read(dsn = "IA_Counties") %>%
  st_transform(crs = 3857)
```

Now get the tornado locations and compute the annual tornado occurrence rate for each county. We did this at the state level in Example 2 of Lesson 8.

Start by first determining the intersections of the county polygons and the tornado points.
```{r, readTors}
Alltors.sfdf <- st_read(dsn = "1950-2018-torn-initpoint") %>%
                        filter(yr >= 1994) %>%
                        st_transform(crs = st_crs(IA_counties.sf))

mtrx <- st_contains(IA_counties.sf, 
                    Alltors.sfdf, 
                    sparse = FALSE)

nT <- rowSums(mtrx)
CountyArea <- st_area(IA_counties.sf)

( IA_counties.sf <- IA_counties.sf %>%
  mutate(nT,
         rate = nT/CountyArea/(2018 - 1994 + 1) * 10^10, 
         lpop = log(estimate)) )
```

Make a two-panel map displaying the log of the population and the tornado rates.
```{r, eval=FALSE}
map1 <- tm_shape(IA_counties.sf) +
  tm_borders(col = "gray70") +
  tm_fill(col = "lpop",
          title = "Log Population",
          palette = "Blues") +
  tm_layout(legend.outside = "TRUE")

map2 <- tm_shape(IA_counties.sf) +
  tm_borders(col = "gray70") +
  tm_fill(col = "rate",
          title = "Annual Rate\n[/10,000 sq. km]",
          palette = "Greens") +
  tm_layout(legend.outside = "TRUE")

tmap_arrange(map1, map2)
```

There appears some relationship. The non-spatial correlation between the two variables is obtained with the `cor.test()` function.
```{r}
lpop <- IA_counties.sf$lpop
rate <- as.numeric(IA_counties.sf$rate)

cor.test(lpop, rate)
```

The bivariate spatial autocorrelation is assessed using the Lee statistic. A formal non-parametric test under the null hypothesis of no bivariate spatial autocorrelation is done using a Monte Carlo simulation.
```{r}
nbs <- poly2nb(IA_counties.sf)
wts <- nb2listw(nbs)

lee_stat <- lee(lpop, rate, 
                listw = wts, 
                n = length(nbs))
lee_stat$L

lee.mc(lpop, rate, listw = wts, nsim = 999)
```

Finally we map out the local variation in the bivariate spatial autocorrelation.
```{r}
IA_counties.sf$localL <- lee_stat$localL

tm_shape(IA_counties.sf) +
  tm_fill("localL",
          title = "Local Bivariate\nSpatial Autocorrelation") +
  tm_borders(col = "gray70") +
  tm_layout(legend.outside = TRUE)
```

What might cause this?

### Spatial autocorrelation in model residuals

The knowledge of significant spatial autocorrelation by itself is not typically useful. But knowledge of significant spatial autocorrelation in the residuals from some statistical model can help us build a more precise model.

A spatial regression model may be needed whenever the residuals resulting from a aspatial regression model exhibit significant spatial autocorrelation. So a common way to proceed is to first regress the response variable onto the explanatory variables and check for spatial autocorrelation in the residuals.

If the explanatory variables remove the spatial autocorrelation then a spatial regression model is not needed.

Let's return to the Columbus crime data and fit a linear regression model with `CRIME` as the response variable and `INC` and `HOVAL` as the explanatory variables.

Import the data.
```{r}
download.file("http://myweb.fsu.edu/jelsner/temp/data/columbus.zip",
              "columbus.zip")
unzip("columbus.zip")

CC.sf <- read_sf(dsn = "columbus", 
                 layer = "columbus")
```

First make a choropleth map displaying the crime rates.
```{r}
tm_shape(CC.sf) +
  tm_fill("CRIME",
          title = "") +
  tm_borders(col = "gray70") +
  tm_layout(title = "Burglary & Vehicle Thefts\n/1000 Households",
            legend.outside = TRUE)
```

Next compute Moran's I on the `CRIME` variable using row-standardized weights.
```{r}
nbs <- poly2nb(CC.sf)
wts <- nb2listw(nbs)

moran(CC.sf$CRIME,
       listw = wts,
       n = length(nbs),
       S0 = Szero(wts))
```

Is this spatial autocorrelation the result of a clustering of income and housing values?

Fit a linear regression model to the crime rates using income and housing values as explanatory variables.
```{r}
model.lm <- lm(CRIME ~ INC + HOVAL, 
               data = CC.sf)
summary(model.lm)
```

Income and housing values vary indirectly with values of crime. The model statistically explains 55% of the variation in crime. 

We use the `residuals()` method to extract the vector of residuals from the model.
```{r}
res <- residuals(model.lm)
```

There are 49 residuals one for every census tract. A residual is defined as the difference between the observed value and the model predicted value. In this case a positive residual indicates that there is more crime than predicted by the model.

We then check on the distribution of the residuals relative to a normal distribution.
```{r}
sm::sm.density(res, 
               model = "Normal")
```

Based on this graph we find no reason to reject a normal distribution as a model for these residuals.

The next step is to create a choropleth map of the model residuals. Are there clusters of high and low residuals? First put the vector of residuals into simple feature data frame as a new column labeled `residuals`.
```{r}
CC.sf$residuals <- res

tm_shape(CC.sf) +
  tm_fill("residuals") +
  tm_borders(col = "gray70") +
  tm_layout(title = "Linear Model Residuals")
```

There appears to be clustered regions where the model over predicts crime (yellow to green tracts) conditional on household income and housing values and where it under predicts crime (yellow to red tracts).

The amount of clustering appears to be is less than before. That is, after accounting for regional factors related to crime the spatial autocorrelation is reduced.

To determine I on the residuals we use the `lm.morantest()` function and pass the regression model object and the weights object to it.
```{r}
lm.morantest(model.lm, wts)
```

Moran's I on the model residuals is .222.  This compares with the value of .5 on crime alone. Part of the spatial autocorrelation is absorbed by the explanatory factors.

Do we need a spatial regression model?  The output gives a $p$-value on I of .002, thus we reject the null hypothesis of no spatial autocorrelation in the residuals and conclude that a spatial regression model would improve the fit.  

The $z$-value takes into account the fact that these are residuals so the variance is adjusted accordingly.

The next step is to choose a spatial regression model.

## Geographic regression

If the residuals from a statistical model have significant autocorrelation then a spatial modeling approach is called for. One approach is to assume that the relationships between the response variable and the explanatory variables are modified by contextual factors. 

Like with local variants of spatial autocorrelation metrics, which use only neighbors for estimates, we can fit separate regression models for each polygon using only values in neighborhoods. This approach is useful for exploratory analysis (e.g., to show where the explanatory variables are most strongly related to the response variable). This approach is called geographically weighted regression (GWR) or geographic regression.

GWR fits a separate regression model for every location in the dataset. Thus it is not a single model but rather a procedure for fitting a set of models. It fits the models by using the response and explanatory variables only from locations that fall within some prescribed distance (bandwidth). The bandwidth is pre-specified or determined by a cross-validation procedure. GWR is used in epidemiology, particularly for research on infectious diseases and for evaluating health policies or health programs.

Let's see how GWR works with an example.

### Example: Southern homicides

The file `south.zip` contains shapefiles with homicide rates and explanatory variables for counties in the southern United States. Download the file from my website and unzip it in your working directory. 

Import the data using the `read_sf()` from the {sf} package. The data have latitude/longitude coordinates but there is no projection so we set the CRS to long-lat with the `st_crs()` function.
```{r}
download.file(url = "http://myweb.fsu.edu/jelsner/temp/data/south.zip",
              destfile = "south.zip")
unzip("south.zip")

library(sf)

SH.sf <- st_read(dsn = "south", 
                 layer = "south", 
                 stringsAsFactors = FALSE)
st_crs(SH.sf) <- 4326
names(SH.sf)
```

Each row is a separate U.S. county in the southeast. There are 1412 counties.

We are interested in predicting homicide rates (`HR`) given as the number of homicides per 100,000 people. And we consider five explanatory variables including `RD`: resource deprivation index, `PS`: population structure index, `MA`: marriage age, `DV`: divorce rate, and `UE`: unemployment rate. The two digit number appended to the column names is the census year from the 20th century.

First use the `plot()` method on the `geometry` column to see the extent of the data and the spatial geometries.
```{r}
plot(SH.sf$geometry, col = "gray70")
```

Next we reduce the number of variables in the data frame keeping only the ones of interest using the `select()` function from {dplyr}.
```{r}
library(dplyr)

SH.sf <- SH.sf %>%
  dplyr::select(HR90, RD90, PS90, MA90, DV90, UE90)
```

We create a thematic map of the homcide rates from the 1990 census (`HR90`).
```{r}
library(tmap)

tm_shape(SH.sf) +
  tm_fill("HR90", title = "1990\nHomicide Rates\n[/100,000]") +
  tm_layout(legend.outside = TRUE)
```

We start with a linear regression model where we regress homicide rate onto resource deprivation, population structure, marriage age, divorce rate, and unemployment rate.
```{r}
model.lm <- lm(HR90 ~ RD90 + PS90 + MA90 + DV90 + UE90, 
               data = SH.sf)
summary(model.lm)
```

We see that RD, PS, and DV have a positive relationship to HR while MA and UE have a negative relationship. 

Based on the $p$-values on the coefficients we suspect the model can be simplified by removing marriage age (MA). We check this supposition with the `drop1()` function.
```{r}
drop1(model.lm)
```

The single term delection table shows that when marriage age (`MA90`) is removed from the model the RSS (residual sum of squares) value increases by 35.2 units. This increase is not sufficient to justify the loss in the degrees of freedom by keeping it in the model. This is seen by an AIC value that is lower (4998.7) than the AIC when all terms are retained (4999.7) (see the row labeled `<none>`). 

Recall: The AIC is a way to balance the tradeoff between bias and variance. Choose a model that has the lowest AIC. A model may have too much bias (toward the particular dataset) if it has too many parameters and a model may have too much residual variance if there are too few parameters.

We therefore remove marriage age and refit the mode.
```{r}
model.lm2 <- lm(HR90 ~ RD90 + PS90 + DV90 + UE90, 
               data = SH.sf)
```

The new model (`model.lm2`) can't be simplified further using this criteria.
```{r}
drop1(model.lm2)
```

All the AIC values below the first row are above those in the first row.

Next we map the predicted values after adding them to the simple features data frame. The predicted values from the model object are extracted with the `predict()` method.
```{r}
SH.sf$predLM2 <- predict(model.lm2)
head(cbind(SH.sf$HR90, SH.sf$predLM))
```

The first column printed to the console is the actual homicide rates in the first six counties and the second column printed is the predicted homicide rate from the linear regression model. The predictions do not appear to be very good.

A scatterplot of the observed versus the predicted shows this clearly.
```{r}
library(ggplot2)

ggplot(SH.sf, aes(x = HR90, y = predLM2)) +
  geom_point() +
  geom_abline(slope = 1) +
  geom_smooth(method = lm, se = FALSE)
```

Since the homicide rates are non-negative, transforming them to logarithms is a good idea.

We create a new column in the `homicides.sf` data from called `logHR90`. Since there are some counties with no homicides we change that to the minimum observed value before taking logarithms. Here we first create a logical vector `x` corresponding to the rows with non-zero homicide rates. We then find the minimum non-zero rate and assign it to `e`. Next we subset on this value for all rates equal to zero and finally we create a new column as the logarithm of the non-zero rates.
```{r}
x <- SH.sf$HR90 != 0
e <- min(SH.sf$HR90[x])
SH.sf$HR90[!x] <- e
SH.sf$logHR90 <- log(SH.sf$HR90)
```

We then fit a model with `logHR90` as our response variable.
```{r}
model.lm3 <- lm(logHR90 ~ RD90 + PS90 + DV90 + UE90, 
                data = SH.sf)
summary(model.lm3)
```

We again compute the predicted values and include them in the data frame as `predLM3`. The predictions are on the natural logarithm scale so we use the exponential function `exp()`. We then create a scatter plot of the observed versus predicted as before.
```{r}
SH.sf$predLM3 <- exp(predict(model.lm3))

ggplot(SH.sf, aes(x = HR90, y = predLM3)) +
  geom_point() +
  geom_abline(slope = 1) +
  geom_smooth(method = lm, se = FALSE)
```

The range of predicted values is much better.

It is likely that homicide rates are similiar in neighboring counties. It also might be the case that the similarity is statistically explained by the  variables in the model.

So our next step it to test for significant autocorrelation in the model residuals. We create a weights matrix using the functions from the {spdep} package and then use the `lm.morantest()` function.
```{r}
library(spdep)

nbs <- poly2nb(SH.sf)
wts <- nb2listw(nbs)

lm.morantest(model.lm3, wts)
```

Moran's I is .11 but significant ($p$-value < .001).

Put the residuals on a map. First add the residuals as a column in the simple feature data frame.
```{r}
SH.sf$res3 <- residuals(model.lm3)

tm_shape(SH.sf) +
  tm_fill("res3", title = "Model\nResiduals") +
  tm_layout(legend.outside = TRUE)
```

There are small clusters of counties with positive residuals and other small clusters of negative residuals. Interestingly the pattern of these clusters appears to be different over western and northern areas compared to the deep South.

This suggests that the _relationships_ between homicide rates and the socioeconomic factors might vary across the domain. GWR is a procedure to fit local regression models.

Linear regression is a model for the conditional mean. The mean of the response variable depends on the explanatory variable(s). Geographic regression might show how this dependency varies by location. It is an exploratory technique intended to indicate where local regression coefficients are different from the global values.

A model is fit at each location. All observations contribute to the fit but they are weighted inversely by their distance to the location. At the shortest distances observations are given the largest weights based on a Gaussian function. The process results in a set of regression coefficients for each observation.

We do this with functions from the {spgwr} package. The geometry information in simple feature data frames is not accessible by functions in this package so we need to create a new S4 spatial data frame.
```{r}
SH.sp <- as(SH.sf, "Spatial")
```

The spatial information in the `SH.sp` is separated from the data frame (attribute table) but accessible by the functions `gwr.sel()` and `gwr()`. The variables remain the same.

We obtain the optimal bandwidth with the `gwr.sel()` function specifying the model and the data object. Since the CRS is geographic we use the argument `longlat = TRUE` to get the distances in kilometers.
```{r}
library(spgwr)

bw <- gwr.sel(formula = logHR90 ~ RD90 + PS90 + DV90 + UE90, 
              data = SH.sp,
              longlat = TRUE)
```

The automatic selection procedure makes an initial guess at the bandwidth distance then fits local regression models in each county using neighbors defined by that distance. A cross-validated (CV) skill score is computed as the root mean square prediction error. The cross-validation procedures successively removes one county from the modeling and that county's homicide rate is predicted. Each county takes turn getting removed.

The selection procedure continues by changing the initial guess at the bandwidth and computing the CV score. If the CV score is higher than wih the initial guess the bandwidth is changed in the other direction. If it is lower than the bandwidth is changed in the same direction. The entire procedure continues until no additional improvement is made to the CV score. This results in a minimum bandwith distance. In this case it is 165.5 km.

The bandwidth is assigned to the object `bw` as a single value.

Unfortunately we cannot use neighborhoods defined by contiquity. But to get a sense of what this bandwidth distance means in terms of the average number of neighbors per county we note that one-half the distance squared times pi is the area captured by the bandwidth.
```{r}
( bwA <- pi * (bw * 1000 /2)^2 ) 
```

In units of square meters. Or 21,519 square kilomenters.

County areas are computed using the `st_area()` function. The average size of the countys and the ratio of the bandwidth area to the average county area is also computed.
```{r}
areas <- st_area(SH.sf)
ctyA <- mean(areas)
bwA/ctyA
```

The ratio indicates that, on average, a neighborhood consists of 13 counties. For comparison, on a raster there are 8 first-order neighboring cells (queen contiguity) and 16 second-order neighboring cells (neighbors of neighbors) or a total of 24 neighbors.

We then use the `gwr()` function to includes the formula, data, and the `bandwith =` argument.
```{r}
model.gwr <- gwr(formula = logHR90 ~ RD90 + PS90 + DV90 + UE90, 
                 data = SH.sp, 
                 bandwidth = bw)
```

The model and observed data are assigned to list object with element names extracted with the `names()` function.
```{r}
names(model.gwr)
```

The first element of the list named `SDF` contains the model output as a S4 spatial class data frame. The geometry of the spatial data frame is inherited from the type of data frame specified in the `data = ` argument.

The structure of the S4 spatial class is obtained with the `str()` function and by setting the `max.level` argument to 2.
```{r}
str(model.gwr$SDF, max.level = 2)
```

Here we see there are 5 slots with the first slot being the attribute table labeled `@data`. The dimension of the attribute table is retrieved with the `dim()` function.
```{r}
dim(model.gwr$SDF)
```

There are 1412 rows and 9 columns. Each row corresponds to a county and information about the regression localized to the county is given in the columns. The attribute names are extracted with the `names()` function.
```{r}
names(model.gwr$SDF)
```

They include the sum of the weights `sum.w` (the larger the sum the more often the county was included in the local regressions--favoring smaller counties and ones farther from the borders of the spatial domain), the five regression coefficients (one for each of the 4 explanatory variables and an intercept term), the residual (`gwr.e`), the predicted value (`pred`) and the local goodness-of-fit (`localR2`).

We put the predictions into the `SH.sf` simple feature data frame with the column name `predGWR`.
```{r}
SH.sf$predGWR <- exp(model.gwr$SDF$pred)

tm_shape(SH.sf) +
  tm_fill("predGWR", title = "Predicted\nHomicide Rates\n[/100,000]") +
  tm_layout(legend.outside = TRUE)
```

The geographic regressions similarly capture the spatial pattern of homicides across the south. The spread of predicted values matches the observed spread better than the linear model. The pattern is also a smoother.

With many more model parameters metrics of predictive skill will favor the geographic regression. For example, the root mean-square-error is lower for GWR.
```{r}
sqrt(sum(residuals(model.lm3)^2))
sqrt(sum(model.gwr$SDF$gwr.e^2))
```

Geographic regression is valuable for generating hypothesis. From the linear model we saw that homicide rates increased with resource deprivation. How does this relationship vary across the South.
```{r}
coef(model.lm3)[2]
range(model.gwr$SDF$RD90)
```

The global regression coefficient is .51 but locally the coefficients range from 0.08 to .98.

Importantly we can map where resource deprevation has the most influence on the response variable.
```{r}
SH.sf$RDcoef <- model.gwr$SDF$RD90

tm_shape(SH.sf) +
  tm_fill("RDcoef", title = "Resource\nDeprivation\nCoefficient", palette = 'Blues') +
  tm_layout(legend.outside = TRUE)
```

All values are above zero, but areas in darker blue indicate where resource deprivation plays a stronger role in explaining homicide rates.

How about the unemployment rate?
```{r}
SH.sf$UEcoef <- model.gwr$SDF$UE90

tm_shape(SH.sf) +
  tm_fill("UEcoef", title = "Unemployment\nCoefficient", palette = 'PiYG') +
  tm_layout(legend.outside = TRUE)
```

While the global coefficient is negative indicating homicide rates tend to be lower in areas with more unemployment, the opposite is the case over much of Texas into Oklahoma.

Where does the model for homicide rates provide the best fit to the data? This is answered with a map of local R squared values (`localR2`).
```{r}
SH.sf$localR2 <- model.gwr$SDF$localR2

tm_shape(SH.sf) +
  tm_fill("localR2", title = "Local\nR Squared", palette = 'Purples') +
  tm_layout(legend.outside = TRUE)
```

When we use a regression model to fit data that vary spatially we are assuming an underlying stationary process. This means we believe the explanatory variables 'provoke' the same statistical response across the entire domain. If this is not the case then it shows up in a map of correlated residuals. One approach to investigate things further is to use geographic regression. Another approach is to use a single spatial regression model.

## Spatial regression

Ordinary regression models fit to spatially aggregated data can lead to improper inference because observations are not independent. Thus it's necessary to check the residuals from an aspatial model for spatial autocorrelation. If the residuals are strongly correlated the model is misspecified. 

In this case we can try to improve the model by adding variables. If that's not possible (no additional data, or no clue as to what variable to include), we can try a spatial regression model. Spatial regression models are widely used in econometrics and epidemiology.

### Spatial lag model and spatial error models

The equation for a regression model in vector notation is
$$
y = X \beta + \varepsilon
$$
where $y$ is a $n \times 1$ vector of response variable values, $X$ is a $n$ $\times$ $p+1$ matrix containing the explanatory variables and augmented by a column of ones for the intercept term, $\beta$ is a $p+1$ $\times$ 1 vector of model coefficients and $\varepsilon$ is a $n \times 1$ vector of residuals (iid).

Two options exist if the elements of the vector $\varepsilon$ are spatially correlated. The first is to rewrite the model adding a spatial lag term as
$$
y = \rho W y + X \beta + \varepsilon
$$
where $Wy$ is the weighted average of the neighborhood response values with $W$ the spatial weights matrix, and $\rho$ is the autoregression coefficient. This is called a spatial autoregressive (SAR) model.

Note: $Wy$ is the spatial lag variable we compute with the `lag.listw()` function and $\rho$ is Moran's I. Thus the model is also called a spatial lag model (SLM).

Justification for the spatial lag model is motivated by a diffusion process. The response variable $y_i$ is influenced by the explanatory variables at location $i$ and by explanatory variables at locations $j$.

$\rho Wy$ is called the spatial signal term and $\beta X$ is called the trend term.

The second option is to rewrite the model by adding a spatial error term as
$$
y = X\beta + \lambda W \epsilon + u
$$
where $\lambda$ is the autoregression coefficient, $W\epsilon$ is the spatial error term representing the weighted average of the neighborhood residuals, and $u$ are the overall residuals assumed to be iid. This is called a spatial error model (SEM).

Here the lag term is computed using the residuals rather the response variable.

Application of the spatial error model is motivated by the omitted variable bias. Suppose the $y$ is statistically described by two variables $x$ and $z$ each centered on zero and independent. Then
$$
y = \beta x + \theta z
$$
If $z$ is not observed, the vector $\theta z$ is nested in the error term $\epsilon$.
$$
y = \beta x + \epsilon
$$

Examples of an unobserved latent variable $z$ include local culture, social capital, neighborhood readiness. Importantly we would expect the latent variable to be spatially correlated (e.g., culture will be similar across neighborhoods), so let
$$
z = \lambda W z + r\\
z = (I - \lambda W)^{-1} r
$$
where $r$ is a vector of random independent residuals (e.g., culture is similar but not identical), $W$ is the spatial weights matrix and $\lambda$ is a scalar spatial correlation parameter. Substituting into the equation above
$$
y = \beta x + \theta z \\
y = \beta x +   \theta (I - \lambda W)^{-1} r\\
y = \beta x + (I - \lambda W)^{-1} \varepsilon
$$
where $\varepsilon = \theta r$.

Another reason for fitting a spatial error model is spatial heterogeneity. Suppose we have multiple observations for each unit. If we want our model to incorporate individual effects we can include a $n \times 1$ vector $a$ of individual intercepts for each unit.
$$
y = a + X\beta
$$
where now $X$ is a $n$ $\times$ $p$ matrix.

In a cross-sectional setting with one observation per unit (typically the case in observational studies), this approach is not possible since we will have more parameters than observations.

Instead we can treat $a$ as a vector of spatial random effects. We assume that the intercepts follows a spatially smoothed process
$$
a = \lambda W a + \epsilon \\
a = (I - \lambda W)^{-1} \epsilon
$$
which leads to the previous model
$$
y = X\beta + (I - \lambda W)^{-1} \epsilon
$$

### Lagrange multiplier test for the type of spatial autocorrelation

Let's see how to choose between these two options with an example. Returning to the Columbus crime data, we import the data, fit a linear regression model to statistically explain crime rates using income and housing values. Further we first need to check whether a spatial regression model is warranted by testing whether there is significant spatial autocorrelation in the residuals of the aspatial model.
```{r}
download.file("http://myweb.fsu.edu/jelsner/temp/data/columbus.zip",
              "columbus.zip")
unzip("columbus.zip")

CC.sf <- read_sf(dsn = "columbus", 
                 layer = "columbus")

model.lm <- lm(CRIME ~ INC + HOVAL, 
               data = CC.sf)
CC.sf$residuals <- residuals(model.lm)

nbs <- poly2nb(CC.sf)
wts <- nb2listw(nbs)

lm.morantest(model.lm, wts)
```

The answer is 'yes' a spatial regression model is warranted.

To help decide between the two spatial regression models described above we run a sequence of statistical tests on our linear model object. The tests are the Lagrange multiplier (LM) tests. LM refers to a technique to determine the coefficients on the explanatory variables while simultaneously determining the coefficient on the spatial regression term. It does this iteratively.

The tests are performed with the `lm.LMtests()` function. The test type is specified as a character string. The tests should be considered in order. Start with the standard version of both the spatial error model and spatial lag model LM tests.

For example, to perform a LM test for the spatial error and spatial lag model on the Columbus crime model we type
```{r}
lm.LMtests(model.lm, 
           listw = wts, 
           test = c("LMerr", "LMlag"))
```

The output shows that both the spatial error and spatial lag model terms are significant ($p$-value < .15). Ideally one term is significant and the other is not and we choose the model with the significant term.

Since both are significant we should test again. This time we use the robust forms of the statistics.
```{r}
lm.LMtests(model.lm, 
           listw = wts, 
           test = c("RLMerr", "RLMlag"))
```

Here the error model has a large $p$-value and the lag model has a $p$-value that is less than .15 so we choose the lag model for our spatial regression.

A decision tree (from Luc Anselin) shows the sequence of tests for making a choice about which type of spatial model to use [Decision Tree](http://myweb.fsu.edu/jelsner/temp/SpatialRegressionDecisionTree.png)

If both tests show significance for both models, then we can fit both models and check which one results in the lowest AIC.

### Model fitting

To fit a spatial lag model to the crime data we use the `lagsarlm()` function. The value for $\rho$ is found first using the `optimize()` function and then the $\beta$'s are obtained using generalized least squares. These functions are now in the {spatialreg} package.
```{r}
library(spatialreg)

model.lag <- lagsarlm(CRIME ~ INC + HOVAL, 
                      data = CC.sf, 
                      listw = wts)

summary(model.lag)
```

Let's break down the output starting with the coefficients.

#### Interpreting the model coefficients

The first batch of output concerns the model residuals and the coefficients on the explanatory variables. The model residuals are the observed crime rates minus the predicted crime rates.

The coefficients on income and housing have the same sign (negative) and they remain statistically significant.

The spatial lag model allows for 'spillover'. That is a change in an explantory variable anywhere in the study domain will affect the value of the response variable everywhere. Spillover occurs even when the neighborhood weights matrix represents simple (1st-order) local contiguity. The spillover makes interpreting the marginal effects more complicated.

The spatial lag model implies that for a change in the value of an explanatory variable there are direct and indirect effects on the response variable.

For example, the direct effect gives the impact a change in income has on crime averaged over all tracts. It takes into account the effects that occur from a change in the $i$th tract's income on crime across neighboring tracts.

The indirect effect gives the impact of a change in income has on crime averaged over all OTHER tracts. The indirect effect represent spillovers. The influences on the dependent variable $y$ in a region rendered by change in $x$ in some other region. For example, if all tracts $i \ne j$ (i not equal to j) increase their income, what will be the impact on crime in region $i$?

The total effect (TE) is the sum of the direct and indirect effects. It measures the total cumulative impact on crime arising from one tract $j$ increasing its income over all other tracts (on average). It is given by
$$
\hbox{TE} = \left(\frac{\beta_k}{1-\rho^2}\right)\left(1 + \rho\right)
$$
where $\beta_k$ is the marginal effect of variable $k$ and $\rho$ is the spatial autocorrelation. With $\rho = 0$ TE is $\beta_k$.

Here $\beta_{INC}$ is -1.0487 so the total effect is
```{r}
( TE_INC <- -1.0487 / (1 - .4233^2) * (1 + .4233) )
```
where .4233 is the value for $\rho$.

The direct, indirect, and total effects are shown using the `impacts()` function from the {spatialreg} package.
```{r}
spatialreg::impacts(model.lag, 
                    listw = wts)
```

The direct effects are the changes in the response variable of a particular region arising from a one unit increase in an explanatory variable in that region.

The indirect effects are the changes in the response variable of a particular region arising from a one unit increase in an explanatory variable in another region. For example, due to spatial autocorrelation, a one-unit change in the income variable in region 1 affects the crime rate in regions 2 and 3.

See https://youtu.be/b3HtV2Mhmvk
lmSLX() function

#### Interpreting the coefficient rho

The next set of output is about the estimate of spatial autocorrelation ($\rho$).  The value is .4233 and a likelihood ratio test gives a value of 9.41 which translates to a $p$-value of .002.  The null hypothesis is the autocorrelation is zero, so we confidently reject it. This is consistent with the significant Moran's I value that we found in the linear model residuals.

Two other tests are performed on the value of $\rho$ including a z-test (t-test) using the asymptotic standard error and a Wald test. Both tests confirm that the lag term should be included in the model.

#### Overall model fit

The next set of output concerns the overall model fit. It includes the log likelihood value and the AIC (Akaike Information Criterion). The AIC value for the linear model is included. Here it is clear that the spatial lag model is an improvement (smaller AIC) over the aspatial model.

The larger the likelihood, the better the model and two times the difference in log likelihoods from two competing models divided by the number of observations gives a scale for how much improvement.
```{r}
x <- 2 * (logLik(model.lag) - logLik(model.lm))/49
x[1]
```

   1 | huge,
  .1 | large,
 .01 | good,
.001 | okay

#### Residual spatial autocorrelation

The final bit of output is a Lagrange multiplier test for residual autocorrelation. The null hypothesis is there is no remaining autocorrelation since we have a lag term in the model. We find a high $p$-value so you are satisfied that the lag term takes care of the autocorrelation.

Compare with a spatial error model. Here we use the `errorsarlm()` function.
```{r}
model.error <- errorsarlm(CRIME ~ INC + HOVAL, 
                          data = CC.sf, 
                          listw = wts)
summary(model.error)
```

Here we find the spatial autoregression ($\lambda$) is significant, but the log likelihood value from the model is smaller (-183.7) and the AIC value is larger (377.5) compared with corresponding values from the lag model. This is consistent with the LM tests indicating the spatial lag model is more appropriate.

We compare the log likelihoods from the two spatial regression models and find that the lag model is a good improvement over the error model. This result is consistent with our above decision to use the lag model.
```{r}
x <- 2 * (logLik(model.lag) - logLik(model.error))/49
x[1]
```

### Predictions

The `predict()` method implements the `predict.sarlm()` function to calculate predictions from the spatial regression model. The prediction is decomposed into a "trend" term (explanatory variable effect) and a "signal" term (spatial smoother). The predicted fit is the sum of the trend and the signal terms when using the spatial lag model.

We make predictions with the `predict()` method under the assumption that the mean response is known. We examine the structure of the corresponding predict object.
```{r}
pre <- predict(model.lag)
str(pre)

CC.sf$CRIME[1:5]
```

The trend term is $X\beta$ and the signal term is $\rho W y$.

The predictions are added to the simple features data frame.
```{r}
CC.sf$fit <- as.numeric(pre)
CC.sf$trend <- attr(pre, "trend")
CC.sf$signal <- attr(pre, "signal")
```

The components of the predictions are mapped and placed on the same page.
```{r}
( g3 <- ggplot() +
    geom_sf(data = CC.sf, aes(fill = fit)) +
    scale_fill_gradient(low = "white", high = "green") +
    ggtitle("Predicted Crime") )

( g4 <- ggplot() +
    geom_sf(data = CC.sf, aes(fill = trend)) +
    scale_fill_gradient(low = "white", high = "orange") +
    ggtitle("Trend (Covariate)") )

( g5 <- ggplot() +
    geom_sf(data = CC.sf, aes(fill = signal)) +
    scale_fill_gradient(low = "white", high = "blue") +
    ggtitle("Signal") )

gridExtra::grid.arrange(g3, g4, g5, nrow = 3)
```

The values in the map at the top are the sum of the values in the two maps below. The spatial smoother (signal) is about the same size as the explanatory variable effect especially for the most centrally located tracts.

Compare model residuals. How many tracts have a smaller residual when using the lag model versus the aspatial model?
```{r}
CC.sf <- CC.sf %>%
  mutate(residualsL = CRIME - fit,
         lagWins = abs(residuals) > abs(residualsL))

sum(CC.sf$lagWins)
```

In 32 out of the 49 tracts the residuals from the spatial model are smaller than the residuals from the aspatial model.

#### References

* Haining, R. 1990 Spatial data analysis in the social and environmental sciences, Cambridge: Cambridge University Press, p. 258.
* Cressie, N. A. C. 1993 Statistics for spatial data, Wiley, New York.
* Michel Goulard, Thibault Laurent & Christine Thomas-Agnan, 2017 About predictions in spatial autoregressive models: optimal and almost optimal strategies, Spatial Economic Analysis Volume 12, Issue 2–3, 304–325 https://doi.org/10.1080/17421772.2017.1300679.
* Kelejian, H. H. and Prucha, I. R. 2007 The relative efficiencies of various predictors in spatial econometric models containing spatial lags, Regional Science and Urban Economics, Volume 37, Issue 3, 363–374.
* Bivand, R. 2002 Spatial econometrics functions in R: Classes and methods, Journal of Geographical Systems, Volume 4, No. 4, 405–421

Finally, Stata is the dominant software package for economists needing to fit spatial regression models. Outside of Economics, however, R is widely used and more versatile. This cheatsheet may help make the transition. https://www.r-bloggers.com/stata-to-r-cheatsheet-for-econometrics/

### Example: California house prices

The file `CHP.zip` contains information about housing values at the census tract level across the state of California.
```{r}
library(sf)

download.file("http://myweb.fsu.edu/jelsner/temp/data/CHP.zip",
              "temporary.zip")
unzip("temporary.zip")

CHP.sf <- read_sf(dsn = "CHP")
```

The object `CHP.sf` is a simple feature spatial data frame with 29 variables whose values are aggregated to census tracts. There are 7049 tracts. To see the spatial extent of the data and the tract boundaries we use `tm_borders()`.
```{r}
library(tmap)

tm_shape(CHP.sf) +
  tm_borders()
```

The warning message tells us that at least one of the 7049 geometries is invalid. 

Invalid geometries include null or empty geometries or geometries that are not simple (e.g., intersecting polygons). The `st_is_valid()` function returns a logical vector that lets us determine the number of invalid geometries.
```{r}
sum(!st_is_valid(CHP.sf))
```

To see the reason we include the argument `reason = TRUE`.
```{r}
x <- st_is_valid(CHP.sf, reason = TRUE)
x[!st_is_valid(CHP.sf)]
```

This tells us that two geometries containing self-intersecting polygons.

To fix this we can create a new set of geometries with the `st_buffer()` function while setting the buffer distance to zero.
```{r}
library(dplyr)

CHP.sf <- CHP.sf %>%
  st_buffer(dist = 0)

sum(!st_is_valid(CHP.sf))
```

#### Aggregate the attributes to the county level

We will analyze these data at the county level. In Lesson 6 we saw how to aggregate variables in a spatial data frame using functions in the **areal** package. This allows for greater flexibility since the aggregating units do not need to be in alignment.

Since the California tracts are completely contained within county boundaries (aligned) here we use the `aggregate()` function from the **raster** package.

We have county names (`County`) as a character variable so we can aggregate by name after converting the simple feature to an S4 class spatial data frame.
```{r}
CHP.sp <- as(CHP.sf, "Spatial")

CHPcounty.sp <- raster::aggregate(CHP.sp, 
                                  by = "County")
```

This produces a spatial polygons data frame where the boundaries are counties but the attribute table contains only the county name. 

To see this let's convert the S4 spatial data frames to simple features and create a map.
```{r}
CHPcounty.sf <- as(CHPcounty.sp, "sf")

library(tmap)

tmap_mode("view")
tm_shape(CHPcounty.sf) +
  tm_borders()

tmap_mode("plot")
```

Zoom and note that San Francisco County is close to Marin County (to the north) but not by contiguity.

We need to aggregate the variables to the county level. Although it is possible to do everything in one step with `aggregate()` it is best to be careful because how we want a variable aggregated depends on its type (extensive or intensive).

The simplest case is where we can sum the values (extensive variables like the number of houses). First create a simple feature data frame and select a subset of the columns.
```{r}
CHP1.sf <- CHP.sf %>%
    dplyr::select(nhousingUn, recHouses, nMobileHom, Population,
                  Males, Females, Under5, White, Black,
                  AmericanIn, Asian, Hispanic, PopInHouse,
                  nHousehold, Families)
```

Create a list of the county names by tract.
```{r}
CountyList <- list(County = CHP.sf$County)
CountyList[[1]][1:50]
```

Then aggregate by county name (`County`) with the `aggregate()` method applied to the simple feature data frame. We employ the base R `sum()` function through the argument `FUN = sum` and specify what to do with missing values (encountered by in the summation) with the  `na.rm = TRUE` argument.
```{r}
CHP1.sf <- aggregate(CHP1.sf, 
                     by = CountyList,
                     FUN = sum, 
                     na.rm = TRUE)
head(CHP1.sf)

tm_shape(CHP1.sf) +
  tm_fill(col = "nMobileHom") +
  tm_borders()
```

The result is a spatial data frame containing county-level totals aggregated from the original tract-level data.

In other cases like with home values we need to use a weighted average to take into account the fact that housing values may be high in a small tract in a county but they may be much lower in a larger tract in the same county (intensive variable). So here we create average values per household in three steps. 

First select the variables of interest.
```{r}
CHP2.sf <- CHP.sf %>%
  dplyr::select(houseValue, yearBuilt, nRooms,
                nBedrooms, medHHinc, MedianAge,
                householdS, familySize, nHousehold)
```

Second multiply these variables by the number of households (`nHousehold`) in each tract.
```{r}
CHP2.sf <- CHP2.sf %>%
  mutate(houseValue = houseValue * nHousehold,
         yearBuilt = yearBuilt * nHousehold,
         nRooms = nRooms * nHousehold,
         nBedrooms = nBedrooms * nHousehold,
         MedianAge = MedianAge * nHousehold)
```

Third aggregate to get county totals before dividing by the number of households.
```{r}
CHP2.sf <- aggregate(CHP2.sf, 
                     by = CountyList, 
                     FUN = sum, 
                     na.rm = TRUE)
CHP2.sf <- CHP2.sf %>%
  mutate(houseValue = houseValue / nHousehold,
         yearBuilt = yearBuilt / nHousehold,
         nRooms = nRooms / nHousehold,
         nBedrooms = nBedrooms / nHousehold,
         medHHinc = medHHinc / nHousehold,
         MedianAge = MedianAge / nHousehold,
         householdS = householdS / nHousehold,
         familySize = familySize / nHousehold)

head(CHP2.sf)
```

Map the county level house values together with the number of bedrooms.
```{r}
tm_shape(CHP2.sf) +
  tm_fill(col = c("houseValue", "nBedrooms")) +
  tm_layout(legend.position = c("right", "top")) +
  tm_borders()
```

#### Fit a nonspatial regression model to predict house values

We examine the residuals from an aspatial regression of house price on house age and the number of bedrooms. Here we first assign the model formula to the object `mf` and then use the `lm()` function.
```{r}
mf <- houseValue ~ yearBuilt + nBedrooms
model.lm <- lm(mf, data = CHP2.sf)
summary(model.lm)
```

According to the model, `yearBuilt` is highly significant. Older houses are more valuable. House values increase by 13K dollars for every additional year of existence. The number of bedrooms is also marginally significant. Every bedroom adds about 192K dollars to the value of a house.

What is the expected (predicted from the model) value of a house built in 1999 with four bedrooms?
```{r}
predict(model.lm, 
        newdata = data.frame(yearBuilt = 1999, 
                             nBedrooms = 4))
```

What is the expected value of a house in San Francisco? First, what row contains data from San Francisco County?
```{r}
which(CHP2.sf$County == "San Francisco")

predict(model.lm)[38]
```

How does this compare with the county average?
```{r}
CHP2.sf$houseValue[38]
```

The model under predicts house values in San Francisco by over $100K.

Next, examine the model residuals for spatial autocorrelation. We first compute the neighborhood topology based on contiguity. But then add two links: between San Francisco and Marin County and vice versa (to consider the fact that the Golden Gate bridge connects these two counties in a way that would likely influence housing prices). 

This is done by knowing the row numbers of these two counties and adding these numbers to the neighborhood list object. Note that since county 21 is a neighbor of county 38, county 38 must be a neighbor of county 21 so we need to adjust both.
```{r}
library(spdep)

nbs <- poly2nb(CHP2.sf)

CHP2.sf$County[21]

nbs[[21]] <- sort(as.integer(c(nbs[[21]], 38)))
nbs[[38]] <- sort(as.integer(c(21, nbs[[38]])))
```

Or we can use nearest neighbors. Start by extracting the coordinates of each county using the `st_coordinates()` function after applying the `st_centroid()` function. We use the `knearneigh()` function to compute the set of k nearest neighbors for each center. Since the CRS is geographic we include the `longlat = TRUE` argument and then  convert the k nearest neighbor object to a neighborhood object with the `knn2nb()` function.
```{r}
coords <- st_coordinates(st_centroid(CHP2.sf))

nearestNeighbors <- knearneigh(coords, 
                               k = 5,
                               longlat = TRUE)
knb <- knn2nb(nearestNeighbors)
summary(knb)
```

Create the spatial weights matrix and check the residuals from the linear model for spatial autocorrelation.
```{r}
wts <- nb2listw(nbs)
kwt <- nb2listw(knb)
lm.morantest(model.lm, wts)
lm.morantest(model.lm, kwt)
```

The large amount of spatial autocorrelation indicates that a spatial regression model is warranted. The model coefficients are not reliably precise.

#### Fit a spatial regression model using the same explanatory variables

As we saw in Lesson 11, we first need to choose between a lag and an error model. We do this sequentially first using the Lagrange multiplier test and then, if needed, the robust version of the LM test.
```{r}
lm.LMtests(model.lm, 
           listw = wts, 
           test = c("LMerr", "LMlag"))
lm.LMtests(model.lm,
           listw = wts, 
           test = c("RLMerr", "RLMlag"))
```

The results from the LM tests do not help us discern between and a lag and an error model. However, the results from the RLM tests favor of a lag model. Recall the lag model includes a spatial lag term on the response variable.

This same decision is reached when using the nearest neighbor weights (`kwt`).

Fit the model. Here we need to specify a lower tolerance for numerically solving the inverse of the covariance matrix. This is needed in this case because the explanatory variables have very different scales. Actually it might be better to scale the variables first.
```{r}
library(spatialreg)

model.lag <- lagsarlm(mf, 
                      data = CHP2.sf, 
                      listw = wts,
                      tol.solve = 1e-30)
summary(model.lag)
```

The coefficients on year the house was built and on the number of bedrooms have the same sign and remain statistically significant but the values are considerably different.

The value of $\rho$ (coefficient on the spatial lag term) is .774, which is large and significant as indicated by the likelihood ratio test value and its corresponding $p$-value. The z-value and Wald statistic similarly indicate a significant coefficient on the spatial lag term.

The difference in log likelihoods between the spatial and aspatial regression models indicates a large to huge improvement of the spatial model over the aspatial model.
```{r}
2 * (logLik(model.lag) - logLik(model.lm))/58
```

   1 | huge,
  .1 | large,
 .01 | good,
.001 | okay

The AIC for the spatial lag model is lower than that for the linear model and there is no significant residual autocorrelation.

What is the expected value of a house in San Francisco predicted using the spatial lag model.
```{r}
predict(model.lag)[38]
```

The spatial lag model better captures the aggregated housing values in San Francisco than does the aspatial model.

The `trend` term is the collective influence of the two explanatory variables and the `signal` is the spatial smoothing.
```{r}
pre <- predict(model.lag)
str(pre)
CHP2.sf$houseValue[1:5]
```

Maps of the spatial signal and the trend terms are made separately.
```{r}
CHP2.sf$signal <- attr(pre, "signal")
CHP2.sf$trend <- attr(pre, "trend")

tm_shape(CHP2.sf) +
  tm_fill("signal")
```

The signal term 'explains' (statistically) the spatial influence of San Francisco on housing values across the state. Highest values are centered on San Francisco County and these high values decay northward and eastward quickly. The decay in the influence of San Francisco is more gradual to the south.

```{r}
tm_shape(CHP2.sf) +
  tm_fill("trend")

CHP2.sf$preLAG <- CHP2.sf$signal + CHP2.sf$trend

tm_shape(CHP2.sf) +
  tm_fill("preLAG", midpoint = NA)
```

The trend term shows that beyond the spatial smoothed term additional factors (year built and number of bedroom) make house values in San Francisco high.

## Spatial filtering

https://cran.r-project.org/web/packages/spatialreg/vignettes/SpatialFiltering.html

## Bayesian spatial regression using INLA

See for updates to notes below: https://github.com/becarioprecario/INLAMSM

Interest in spatial statistical models has increased in the recent years stemming in part from the availability of methods to fit them. One such method is called Bayesian inference.

Classical statistics relies on the likelihood function ($p(y|\theta)$) which specifies the probability distribution of our data ($y$) under a model defined by parameters ($\theta$). Recall that we compared our spatial models using values of the log likelihood.

Variability on $y$ comes about from sampling. That is we assume that the data are a random sample from a study population and the uncertainty arises because we only observe a single sample instead of the entire population.

But with Bayesian inference, the parameters $\theta$ are unknown quantities modeled through a suitable *prior* probability distribution $p(\theta)$ before we observing any sample $y$. The prior probability distribution reflects our prior belief about $\theta$. Then given the likelihood and the prior the inferential problem is solved using Bayes theorem. That is, we want to know the probability of our model being true given our sample of data $y$ (the posterior distribution).
$$
p(\theta|y) = \frac{p(y|\theta) \times p(\theta)}{p(y)}
$$

Determining the posterior distribution of $\theta$ given our data requires determining $p(y)$ (marginal distribution of our data). This can only be done analytically for a relatively small set of all models (those involving conjugate distributions), none of which are spatial. 

Bayesian models for spatial data started appearing around 2000 with the development of the Markov chain Monte Carlo (MCMC) procedure (Gilks et al. 1996). This allowed researchers to implement realistic models (involving fewer simplifying assumptions). A main contribution to spatial models at this time came with the development of the conditional autoregressive (CAR) representation for specifying spatial autocorrelation (Besag et al. 1991).

MCMC methods are now routinely used in Bayesian inference, but they are limited by the need for alot of processing power. To overcome this Rue et al. (2009) developed a technique called integrated nested Laplace approximation (INLA) that estimates the posterior marginals (not the joint posterior) for models that have an underlying (latent) Gaussian structure.

Latent Gaussian fields appear in a large number of spatial models. This means that INLA is useful for fitting more complex spatial regression models when only marginal inference on the model parameters is needed.

### The {INLA} package

See https://www.paulamoraga.com/book-geospatial/sec-inla.html

The {INLA} package (Rue et al. 2014) provides an interface to the free-standing INLA program so models can be fit using standard R commands. The package is available from http://www.r-inla.org/. The model interface is similar to the one used to fit generalized additive models (GAM) with the `gam()` function from the {mgcv} package. 

The `formula` argument can handle fixed effects, nonlinear terms, and random effects. The interface allows for the specification of various priors and model fitting options. Nonlinear and random effects are included in the formula as calls to the `f()` function. Spatial dependence is included as part of a vector of latent effects.

The package includes a model for uncorrelated random effects $u_i$ in aggregated data.
$$
u_i \sim N(0, \tau_u)
$$
where $\tau_u$ refers to the precision (inverse of the variance) of the Gaussian distribution. The {INLA} package assigns a prior to $\log(\tau_u)$ which, by default, is a log-gamma distribution.

As we've seen in order to model spatial correlation neighborhoods must be defined.

Spatial autocorrelation is modeled using a Gaussian distribution with zero mean and a precision matrix that specifies the correlation between neighbors. Given that these latent effects (unobserved) are Gaussian Markov random fields (GMRF), we can define the variance-covariance matrix of the random effects as:
$$
\Sigma = \frac{1}{\tau} Q^{-1}
$$
where $\tau$ is a precision parameter and $Q$ encodes the spatial structure as a matrix. Given the assumption of a latent GMRF, the matrix $Q$ is defined such that $Q_{ij}$ is zero if areas $i$ and $j$ are not neighbors. This means $Q$ is sparse.

One way to specify spatial dependence is through the conditional autoregressive (CAR) representation. This produces a $Q$ matrix in which element $Q_{ii}$ is $n_i$ (the number of neighbors of area $i$) and elements $Q_{ij}$ (with $i \ne j$) are one if areas $i$ and $j$ are neighbors and zero otherwise. 

This specification implies that the spatial random effects $v_i$ are distributed as normal with mean equal to the neighborhood average and variance scaled by the number of neighbors.
$$
v_i|v_j, \tau_v \sim N\left( \frac{1}{n_i} \sum_{i \sim j} v_j, \frac{1}{\tau_v n_i} \right) \hspace{.5cm} i \ne j
$$
where $\tau_v$ is the conditional precision of the random effects. Again, {INLA} uses a log-gamma prior on log($\tau_v$).

Download and install {INLA}. The package relies on a stand-alone C library (GMRF) and therefore is not on CRAN, but you can get all the info from http://www.r-inla.org.
```{r}
#install.packages("INLA", repos=c(getOption("repos"), INLA="https://inla.r-inla-download.org/R/stable"), dep = TRUE)
library(INLA)
```

Reference: Bivand et al. (2015) Spatial data analysis with R-INLA with some extensions. *Journal of Statistical Software*, v63.

### Example: California house prices

Let's use INLA to fit the spatial regression model to the California house prices. We have our weights matrix (`wts`) as a list w object. We convert this to a full matrix with the `listw2mat()` function. We print out the first ten rows and columns of the matrix.
```{r}
wtsF <- listw2mat(wts)
wtsF[1:10, 1:10]
```

To match the weights with the tracts in `CHP2.sf`, we need to add an index variable `idx` to the spatial data frame.
```{r}
CHP2.sf$idx <- 1:nrow(CHP2.sf)
head(CHP2.sf)
```

The model in matrix notation can be expressed as
$$
y = X \beta + v + u
$$
where $\beta$ is a vector of coefficients for the explanatory variables (and intercept), $v$ is the spatial random effect with a CAR specification, and $u$ is a random Gaussian error term. Note the similarity with the spatial lab model from Lesson 11.

The explanatory effects (including the intercept) are given Gaussian priors with mean of zero and precision of .01 by default. These can be changed through the `control.fixed()` function.

With Bayesian models it is advisable to center and scale the variables. Scaled variables make it is easier to determine prior distributions and, when using MCMC approaches, reasonable initial starting locations are easier to find. Here we use the `scale()` function that subtracts the mean and divides by the standard deviation.
```{r}
CHP2.sf <- CHP2.sf %>%
      mutate(houseValueS = scale(houseValue),
             yearBuiltS = scale(yearBuilt),
             nBedroomsS = scale(nBedrooms))
```

We already specified the explanatory effects (trend) part of the model. Here we repeat using the scaled variables. Then we add the intrinsic CAR specification by using the `f()` function with arguments `model = 'besag'` and `graph = wtsF`. The graph argument must contain the weights matrix in full matrix form.
```{r}
mf <- houseValueS ~ yearBuiltS + nBedroomsS + 
            f(idx, model = 'besag', graph = wtsF)
```

The `inla()` function fits the model. Here `control.predictor = list(compute = TRUE)` is used to compute summary statistics on the fitted values. Unfortunately RStudio Cloud does not have the proper fortran compiler so this will not work here. It should work on your own computer.

I ran it on my desktop and saved the model object as a serialized R data file and uploaded the file to RStudio Cloud.
```{r, eval=FALSE}
model.car <- inla(mf, 
                  data = CHP2.sf, 
                  control.predictor = list(compute = TRUE))

saveRDS(model.car, file = "modcar.rds")
```

We read it in 
```{r}
model.car <- readRDS("modcar.rds")
```

A summary of the model is obtained with the `summary` method.
```{r}
summary(model.car)
```

Compare with a non-Bayesian spatial lag model using the scaled (and centered) variables.
```{r}
summary(lagsarlm(houseValueS ~ yearBuiltS + nBedroomsS, 
                 data = CHP2.sf , 
                 listw = wts))
```

The estimated coefficients are similar but the uncertainty about the estimates is coded differently with Bayesian inference. There is no $p$-value. Rather there is an uncertainty interval about the estimated parameter value (called a credible interval).

Here we see that the 95% credible interval on the marginal posterior for the number of bedrooms coefficient includes the value of zero.

The posterior distributions for the coefficients are found in the `model.car$marginals.fixed`.
```{r}
names(model.car$marginals.fixed)
```

There is a marginal distribution for the intercept and marginal distributions for the year built term and the number of bedrooms term.

We use the `plot()` method to plot the distribution on the two explanatory variables.
```{r}
plot(model.car$marginals.fixed$"yearBuiltS", type = "l")
plot(model.car$marginals.fixed$"nBedroomsS", type = "l")
```

The posterior distributions for the spatial term are found in `model.car$marginals.random`. There is a posterior distribution for each county. Here we plot this distribution for the first county.
```{r}
plot(model.car$marginals.random$idx$index.1, type = "l")
```

Posterior marginals for the fitted values are computed. They include the mean, standard deviation, extreme quantiles, and the mode. Here we add the posterior mean of the fit to the spatial data frame and make a map.
```{r}
CHP2.sf$preCAR <- model.car$summary.fitted.values$mean

tm_shape(CHP2.sf) +
  tm_fill("preCAR", midpoint = NA)
```

The map shows the posterior mean housing prices (scaled) at the county level across the state.

### Bayesian spatial regression using Stan

Stan is a probabilistic programming language for statistical inference written in C++. The Stan language is used to specify a (Bayesian) statistical model with an imperative program calculating the log probability density function. Stan is named in honour of Stanislaw Ulam, pioneer of the Monte Carlo method.

The {brms} package using the R syntax for modeling data but calls Stan for fitting. https://rdrr.io/cran/brms/man/cor_car.html

Fit a CAR model. Create a symmetric weights matrix from the neighborhood list. Scale the variables. NOTE: I could not get this to work in RStudio Cloud.
```{r, eval=FALSE}
library(brms)

nbs <- poly2nb(CHP2.sp)
W <- nb2mat(nbs, style = "B")

CHP2.sf <- CHP2.sf %>%
              mutate(houseValueS = scale(houseValue),
              yearBuiltS = scale(yearBuilt))

mf <- houseValue ~ yearBuiltS + nBedrooms

fit <- brm(mf, 
           data = CHP2.sf, 
           family = lognormal(), 
           autocor = cor_car(W),
           control = list(adapt_delta = .95)) 
summary(fit)
```

Get priors.
```{r, eval=FALSE}
get_prior(mf,
          data = CHP2.sf,
          family = lognormal(),
          autocor = cor_car(W))
```

#### Binomial example on generated spatial data. First setup a 10 by 10 grid.
```{r, eval=FALSE}
east <- north <- 1:10
Grid <- expand.grid(east, north)
K <- nrow(Grid)
```

Next set up distance and neighborhood matrices based on rook contiguity.
```{r, eval=FALSE}
distance <- as.matrix(dist(Grid))
W <- array(0, c(K, K))
W[distance == 1] <- 1 
```

Next, generate the covariates and response data.
```{r, eval=FALSE}
x1 <- rnorm(K)
x2 <- rnorm(K)

theta <- rnorm(K, sd = 0.05)
phi <- rmulti_normal(1, mu = rep(0, K), Sigma = 0.4 * exp(-0.1 * distance))
eta <- x1 + x2 + phi
prob <- exp(eta) / (1 + exp(eta))
size <- rep(50, K)
y <- rbinom(n = K, size = size, prob = prob)
df <- data.frame(y, size, x1, x2)
```

Fit a CAR model
```{r, eval=FALSE}
fit <- brm(y | trials(size) ~ x1 + x2, 
           data = df, 
           family = binomial(), 
           autocor = cor_car(W)) 
summary(fit)
```

## Problem Set #3

Using the Mississippi police expenditure data (http://myweb.fsu.edu/jelsner/temp/data/police.zip):

(1) Import the data as a simple feature data frame.
(2) Create a new variable `pPOLICE` as the police expenditure per person by dividing `POLICE` by `POP` and a new variable `NONWHITE` as 100 minus `WHITE`.
(3) Fit a aspatial linear regression model regressing `pPOLICE` onto `CRIME`.
(4) Test the model residuals for spatial autocorrelation. Use queen contiguity to define the neighborhoods and `style = "W"` to define the weights. 
(5) Determine the type of spatial regression (lag or error model) using the LM diagnostics.
(6) Fit the appropriate spatial regression model.
(7) Add the variable `NONWHITE` to the aspatial regression. Is it a significant term in the model?  Test the residuals from the new model for spatial autocorrelation. What do you conclude?

## Challenge problem: Home foreclosures in Chicago

Another example: http://www.econ.uiuc.edu/~lab/workshop/Spatial_in_R.html

The data have been posted on my website http://myweb.fsu.edu/jelsner/temp/data/foreclosures.zip.

(1) Follow the code in this example but use simple feature data frames to import the data and map the results.
(2) Fit a CAR model to these data using INLA.

## Standarized incidence ratios

See `StandardizedIncidenceRatios.Rmd`