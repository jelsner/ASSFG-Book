# Getting started

First get R

* Go to http://www.r-project.org.
* Select the CRAN (Comprehensive R Archive Network). Scroll to a mirror site. 
* Choose the appropriate file for your hardware.
* Follow the instructions to install R.

Then get RStudio

* Go to on http://rstudio.org
* Download RStudio Desktop
* Install and open RStudio

Learn git

https://happygitwithr.com/install-git.html

The RStudio IDE

* Written in HTML
* Top menus
  + File > New File > R Markdown
  + Tools > Global Options > Appearance
  
* Upper left panel is the markdown file. This is where we put our text and code. 
  + Run code chunks from this panel 
  + Output from the operations can be placed in this panel or in the Console (see the gear icon above)
  + All the text, code, and output can be rendered to an HTML file or a PDF or Word document (see the Knit button above)
  
* Upper right panel shows what is in your current environment and the history of the commands you issued.
  + This is also where you can connect to github
  
* Lower left panel is the Console
  + I think of this as a sandbox where you try out small bits of code. If it works and is relevant move it to the markdown file.
  + This is also where output from running code will be placed.
  + Not a place for plain text
  
* Lower right panel shows your project files, the plots that get made, and all the packages associated with the project.
  + The File tab shows the files in the project. The most important one is the .Rmd.
  + The Plot tab currently shows a blank sheet
  + The Packages tab shows all the packages that have been downloaded from CRAN and are associated with this project.

## Your assignments

Your assignments will be done inside an Rmd file.

1. Download the assignment Rmd from Canvas and rename it to `yourLastName_yourFirstName.Rmd`
2. Open the Rmd file with RStudio
3. Replace 'Your Name' with your name in the YAML
4. Answer the questions by typing the code between code-chunk delimiters
5. Select the Knit button to generate an HTML file
6. Fix any errors
7. Email me your Rmd file

## Basic R

Applied statistics is the analysis and modeling of data. The `c()` function gets small lists of data items into R. The function combines (concatenates) items. Consider for example a set of hypothetical annual land falling hurricane counts over a ten-year period.

2  3  0  3  1  0  0  1  2  1

We put these count values into our working directory by typing them into the console as follows. The console is the lower left window.
```{r chapter1}
counts <- c(2, 3, 0, 3, 1, 0, 0, 1, 2, 1)
counts
```

We assign the values to an object called `counts`. The assignment operator is an equal sign (`<-` or `=`).  Values do not print. They are assigned to an object name. They are printed by typing the object name as we did on the second line. When printed the values are prefaced with a `[1]`. This indicates that the object is a vector and the first entry in the vector has a value of 2 (The number immediately to the right of `[1]`).

Note: We can assign and print by wrapping the entire line of code in parantheses.
```{r}
( counts <- c(2, 3, 0, 3, 1, 0, 0, 1, 2, 1) )
```

Note: we use the arrow keys to retrieve previous commands. Each command is stored in the history file. The up-arrow key moves backwards through the history file. The left and right arrow keys move the cursor along the line.

We apply functions to data that are stored in objects. For example:
```{r}
sum(counts)
length(counts)
sum(counts)/length(counts)
mean(counts)
```

The function `sum()` totals the hurricane counts over all years, `length()` returns the number of elements in the vector. Other functions include, `sort()`, `min()`, `max()`, `range()`, `diff()`, and `cumsum()`. Try these functions on the landfall counts. What does the function `range()` return?  What does the function `diff()` do?

The hurricane count data stored in the object `counts` is a vector. This means that R keeps track of the order that the data were entered. There is a first element, a second element, and so on. This is good for several reasons.

The vector of counts has a natural order; year 1, year 2, etc. We don't want to mix these. We would like to be able to make changes to the data item by item instead of entering the values again. Also, vectors are math objects so that math operations can be performed on them.

For example, suppose `counts` contain the annual landfall count from the first decade of a longer record.  We want to keep track of counts over other decades. This could be done by the following, example.
```{r}
d1 <- counts
d2 <- c(0, 5, 4, 2, 3, 0, 3, 3, 2, 1)
```

Most functions operate on each element of the data vector at the same time.
```{r}
d1 + d2
```

The first year of the first decade is added from the first year of the second decade and so on.

What happens if we apply the `c()` function to these two vectors?  Try it.
```{r}
c(d1, d2)
```

If we are interested in each year's count as a difference from the decade mean, we type:
```{r}
d1 - mean(d1)
```

In this case a single number (the mean of the first decade) is subtracted from a vector. The result is from subtracting the number from each entry in the data vector. This is an example of data recycling. R repeats values from one vector so that the vector lengths match. Here the mean is repeated 10 times.

Suppose we are interested in the variance of the set of landfall counts. The variance is computed as
$$
\hbox{var}(x) = \frac{(x_1 - \bar x)^2 + (x_2 - \bar x)^2 + \cdots + (x_n - \bar x)^2}{n-1} = \frac{1}{n-1}\sum_{i=1}^n (x_i - \bar x)^2
$$

Although the `var()` function computes this, here we see how to do this using simpler functions. The key is to find the squared differences and then sum.
```{r}
x <- d1
xbar <- mean(x)
x - xbar
(x - xbar)^2
sum((x - xbar)^2)
n <- length(x)
n
sum((x - xbar)^2)/(n - 1)
var(x)
```

Elements in a vector must all have the same type. This type can be numeric, as in counts, character strings, as in
```{r}
simpsons <- c('Homer', 'Marge', 'Bart', 'Lisa', 'Maggie')
simpsons
```

Character strings are made with matching quotes, either double, `"`, or single, `'`. If we mix types the values will be coerced into a common type, which is usually a character string. Arithmetic operations do not work on character strings.

Returning to the land falling hurricane counts. Now suppose the National Hurricane Center (NHC) reanalyzes a storm, and that the 6th year of the 2nd decade is a 1 rather than a 0 for the number of landfalls. In this case we type:
```{r}
d2[6] <- 1
```

This assigns to the 6th year of the decade a value of one. The assignment to the 6th entry in the vector `d2` is done by referencing the entry with square brackets `[]`. 

Keep this straight: Parentheses `()` are used for functions and square brackets `[]` are used to extract values from vectors (and arrays, lists, etc).
```{r}
d2
d2[2]
d2[-4]
d2[c(1, 3, 5, 7, 9)]
```

The first line prints all the values of the vector `df2`. The second prints only the 2nd value of the vector. The third prints all but the 4th value. The fourth prints the values with odd element numbers.

Sometimes we need to create structured data. For example, the integers 1 through 99. To enter these we use the `:` operator.
```{r, eval=FALSE}
1:99
rev(1:99)
99:1
```

The `seq()` function is more general than `:`. We specify the sequence interval with the `by =` or `length =` arguments.
```{r}
seq(from = 1, to = 9, by = 2)
seq(1, 10, by = 2)
seq(1, 9, length = 5)
```

The `rep()` function is to create repetitive sequences. The first argument is a value or vector that we want repeated and the second argument is the number of times we want it repeated.
```{r}
rep(1, times = 10)
rep(simpsons, times = 2)
```

In the second example the vector `simpsons` containing the Simpson characters is repeated twice.

To repeat each element of the vector use the `each =` argument.
```{r}
rep(simpsons, each = 2)
```

More complicated patterns can be repeated by specifying pairs of equal-sized vectors. In this case, each element of the first vector is repeated the corresponding number of times specified by the element in the second vector.
```{r}
rep(c("long", "short"), c(2, 3))
```

To find the maximum number of landfalls in the first decade we type.
```{r}
max(d1)
```

Which years had the maximum?
```{r}
d1 == 3
```

Notice the double equals signs (`==`).  This tests each value in `d1` to see if it is equal to 3. The 2nd and 4th values are equal to 3 so `TRUE`s are returned. Think of this as asking R a question. Is the value equal to 3?  R answers all at once with a vector of `TRUE`'s and `FALSE`'s.

Now the question is -- how do you get the vector element corresponding to the `TRUE` values?  That is, which years have 3 landfalls?
```{r}
which(d1 == 3)
```

The function `which.max()` can be used to get the first maximum.
```{r}
which.max(d1)
```

We might also want to know the total number of landfalls in each decade and the number of years in a decade without a landfall. Or how about the ratio of the mean number of landfalls over the two decades.
```{r}
sum(d1)
sum(d2)
sum(d1 == 0)
sum(d2 == 0)
mean(d2)/mean(d1)
```

So there is 85% more landfalls during the second decade. Is this difference statistically significant?

To remove an object from the environment use the `rm()` function.
```{r}
rm(d1, d2)
```

The package {swirl} contains functions to help you learn the basics of R. The `install.packages()` function gets the package from an CRAN mirror site. This needs to be done only once to your local computer. You can update packages using `update.packages()`. To make the functions work in your current session you must use the `library()` function. This needs to be done for every session, but only once per session.
```{r, eval=FALSE}
install.packages("swirl")
library(swirl)
```

Type:
```{r, eval=FALSE}
swirl()
```

Choose the lesson: R Programming. Work through lessons 1:8

Getting help: https://www.r-project.org/help.html

## Data frames

A data frame is used for storing data in tables like a spreadsheet. It is a list of vectors of equal length. For example, the following variable `df` is a data frame containing three vectors `n`, `s`, `b`.
```{r}
n <- c(2, 3, 5) 
s <- c("aa", "bb", "cc") 
b <- c(TRUE, FALSE, TRUE) 

df <- data.frame(n, s, b)
```

There are many built-in data frames. For example, here is a built-in data frame called `mtcars`.
```{r}
mtcars 
```

The top line of the table, called the header, contains the column names. Each horizontal line afterward denotes a data row, which begins with the name of the row, and then followed by the actual data. Each data member of a row is called a cell.

To retrieve data in a cell, we would enter its row and column coordinates in the single square bracket `[]` operator. The two coordinates are separated by a comma. In other words, the coordinates begins with row position, then followed by a comma, and ends with the column position. The order is important.

Here is the cell value from the first row, second column of mtcars.
```{r}
mtcars[1, 2]
```

We can use the row and column names instead of the numeric coordinates.
```{r}
mtcars["Mazda RX4", "cyl"] 
```

The number of data rows in the data frame is given by the `nrow()` function.
```{r}
nrow(mtcars)
```

The number of columns of a data frame is given by the `ncol()` function.
```{r}
ncol(mtcars)
```

Further details of the `mtcars` data set is available in the R documentation.
```{r, eval=FALSE}
help(mtcars)
```

Instead of printing out the entire data frame, it is often desirable to preview it with the `head()` function beforehand.
```{r}
head(mtcars)
```

Or with the `str()` function.
```{r}
str(mtcars)
```

Consider answers on questions given to all students in an introductory statistics class at Bowling Green State University. Some of the questions were: What is your height? Choose a number between 1 and 10. Give the time you went to bed last night. The data are available as a data frame called `studentdata` in the package {LearnBayes}.

First, install the package.
```{r}
# install.packages("LearnBayes")
library(LearnBayes)
```

Next, make a copy of the data frame and call it `df`. Then print the first six rows using the `head()` function and list only the 10th row.
```{r}
df <- LearnBayes::studentdata
head(df)
```

Recall data frames are like spreadsheets with rows and columns. The rows are the observations and the columns are the variables. All columns are of the same length like a matrix. We identify particular data elements of the matrix using the bracket notation [row, column] where row is the row number and column is the column number.

For example here we identify all the columns in the 10th row.
```{r}
df[10, ]
```

Drink preference was one of the questions. The responses are available in the column labeled `Drink` as a vector. We identify this vector using the `$` notation, where the name before the dollar sign identifies the data frame and the name after the dollar sign identifies the column name (`dataframeName$columnName`). 

So we list all the drink preferences using
```{r}
df$Drink
```

Note that some students left that response blank. That is coded as `<NA>`.

The names of the columns is available with the `names()` function.
```{r}
names(df)
```

We can table the responses with the `table()` function.
```{r}
table(df$Drink)

table(df$Drink,
      useNA = "ifany")
```

The numbers are the frequency of responses by `Drink` category.

Use the `plot()` method to make a plot of this table.
```{r}
plot(x = df$Drink)
```

Suppose we are interested in examining how long students slept. This was not asked directly. We compute it from the `ToSleep` and `WakeUp` times columns. We assign the result of the difference to a column we call `SleepHrs`.
```{r}
df$SleepHrs <- df$WakeUp - df$ToSleep
summary(df$SleepHrs)
```

To see the distribution of sleep times, we construct a histogram with the `hist()` function.
```{r}
hist(x = df$SleepHrs)
```

The histogram function divides the number of sleep hours into one-hour bins and counts the number of students whose computed number of sleep hours falls into each bin. For example based on when they said they went to sleep and when the said they woke up, about 100 students slept between five and six hours the night before the survey.

Since the gender of each student is recorded, we can make comparisons between those who identify as male and those who identify as female. For instance, do men sleep more than women? We can answer this question graphically with box plots.
```{r}
plot(x = df$Gender, 
     y = df$SleepHrs)
```

No apparent difference.

Repeat for hair cut prices.
```{r}
plot(x = df$Gender, 
     y = df$Haircut)
```

Big difference.

Finally, is the amount of sleep for a student related to bedtime?
```{r}
plot(x = df$ToSleep,
     y = df$SleepHrs)
```

The `ToSleep` variable is centered on midnight so that -2 means they went to sleep at 10p.

We describe the decreasing relationship by drawing a line through the points. The least-squares line is fit using the `lm()` function and the line is drawn on the existing plot with the `abline()` function applied to the linear regression object `model`.
```{r}
model <- lm(SleepHrs ~ ToSleep, 
            data = df)

plot(x = df$ToSleep,
     y = df$SleepHrs)
abline(model)
```

Records of past tornadoes in the United States.

We download the data from the Storm Prediction Center (SPC) http://www.spc.noaa.gov/gis/svrgis/zipped/. We give the zipped file a temporary name on our computer (here `temp.zip`) using the `destfile =` argument.
```{r}
download.file(url = "http://www.spc.noaa.gov/gis/svrgis/zipped/1950-2018-torn-initpoint.zip",
              destfile = "data/temp.zip")
```

Next we open the zipped file with the `unzip()` function. This creates a folder in our working directory called `1950-2018-torn-initpoint`.
```{r}
unzip("data/temp.zip")
```

Finally we load the shapefile into R. We use the `read_sf()` function from the {sf} package. We specify the data source name `dsn =`. The name of the file exists outside of R in your project directory so it needs to be inside quotes (either single or double).
```{r}
library(sf)

Torn.sf <- read_sf(dsn = "data/1950-2018-torn-initpoint")
```

We will have much more about working with shapefiles and spatial data frames throughout the semester.

We preview the resulting data frame
```{r}
head(Torn.sf)
```

Each row is a unique tornado report. Observations for each report include the date and time, the state (`st`), the maximum EF rating (`mag`), the number of injuries (`inj`), the number of fatalities (`fat`), estimated property losses (`loss`), estimated crop losses (`closs`), start and end locations in decimal degrees longitude and latitude, length of the damage path in yards (`len`), width of the damage path in miles (`wid`). 

There is also a column called `geometry` indicating the spatial information.

The total number of tornado reports in the database is returned from the `nrow()` function.
```{r}
nrow(Torn.sf)
```

Create a subset of the data frame keeping only tornadoes in years (`yr`) since 2001 and with EF ratings (`mag`) greater than 0. First we create a logical operation
```{r}
Torn.sf <- Torn.sf[Torn.sf$yr >= 2001 & Torn.sf$mag > 0, ]
```

Compute the correlation between EF rating (`mag`) and path length (`len`) and path width (`wid`). 
```{r}
cor(Torn.sf$mag, Torn.sf$len)
cor(Torn.sf$mag, Torn.sf$wid)
```

Path length is recorded in miles and path width in yards. To convert them to meters and add the converted values as new columns, type
```{r}
Torn.sf$Length <- Torn.sf$len * 1609.34
Torn.sf$Width <- Torn.sf$wid * .9144
```

Create side-by-side box plots of path length (in kilometers) by EF rating.
```{r}
plot(x = factor(Torn.sf$mag), 
     y = Torn.sf$Length/1000)
```

Create a scatter plot with the size of the point proportional to the EF rating.
```{r}
plot(x = log(Torn.sf$Width), 
     y = log(Torn.sf$Length), 
     cex = Torn.sf$mag)
```

Create a map of tornado genesis locations. First get a file containing the U.S. state borders. Then plot the geometry column and overlay the tornado location as points.
```{r}
library(USAboundaries)

sts <- state.name[!state.name %in% c("Alaska", "Hawaii")]
stateBorders <- us_states(states = sts)

plot(stateBorders$geometry,
     col = "grey")
plot(Torn.sf$geometry, 
     pch = ".", 
     col = "red",
     add = TRUE)
```

Hurricanes in the United States. We load data directly from the web by specifying the URL as a character string using the `file =` argument.
```{r}
df <- read.table(file = "http://myweb.fsu.edu/jelsner/temp/data/US.txt", 
                 header = TRUE)
```

The object `df` is a data frame. A data frame is like a spreadsheet. Values are arranged in rows and columns. Rows are the cases (observations) and columns are the variables. The `dim()` function returns the size of the data frame defined as the number of rows and the number of columns (in that order).
```{r}
dim(df)
```

There are 166 rows and 6 columns in the data frame object `df`.

To get a glimpse of the data values we list the first six lines of the data frame using the `head()` function.
```{r}
head(df)
```

The columns include `Year`, number of hurricanes (`All`), number of major hurricanes (`MUS`), number of Gulf coast hurricanes (`G`), number of Florida hurricanes (`FL`), and number of East coast hurricanes (`E`) in that order. The last six lines of your data frame are listed using the `tail()` function.
```{r}
tail(df)
```

The distribution of Florida hurricane counts by year is obtained using the `table()` function and specifying the `FL` column with `df$FL`.
```{r}
table(df$FL)
```

There are 93 years without a FL hurricane, 43 years with exactly one hurricane, 24 years with two hurricanes, and so on.

The columns in a data frame are referenced using the `df$name` syntax, where `name` refers to the column name.
```{r}
df$FL
sum(df$FL)
```

Each column is a vector of length equal to the number of rows in the data frame.

How many hurricanes hit Florida in 1906?
```{r}
df$FL[df$Year == 1906]
```

The operator `==` returns a logical vector of length equal to the number of rows in `df` with values of `TRUE` and `FALSE`. The subset operator `[` returns the value of the vector `df$FL` when the logical vector is true.

What years had the most East coast hurricanes?
```{r}
df$Year[which(df$E == max(df$E))]
```

As I mentioned last week, there are many functions for working with data frames using {base} R. These functions require understanding data frames as 'list' objects, which makes them harder to learn and to remember.

From the {readr} package as part of the {tidyverse} dialect we can use the `read_table()` function to create a tabled data frame.
```{r}
library(tidyverse)
df <- read_table(file = "http://myweb.fsu.edu/jelsner/temp/data/US.txt")
```

Functions in {base} R for working with data frames require understanding them as list objects. This makes the functions harder to learn and remember. Next time will explore a more formalized grammar for data wrangling.

## Your turn: Precipitation in Florida

Source: Monthly climate series. [Source:](http://www.esrl.noaa.gov/psd/data/timeseries/)

Get monthly statewide average rainfall (in inches) back to the year 1895. Copy/paste into a text editor (I use the app TextWrangler) then import into R using the `read.table()` function.

I did this and posted the file on my website. Missing values are coded as -9.900 so I included the argument `na.string = "-9.900"` in the function call.
```{r}
df <- read.table("http://myweb.fsu.edu/jelsner/temp/data/FLprecip.txt", 
                 na.string = "-9.900", 
                 header = TRUE)
head(df)
```

* What was the statewide average rainfall during the 10th month of the 65th year?
* What was the statewide average rainfall during June of 1900?
* What year had the wettest March?
* What month during 1965 was the wettest?

Use the tornado dataset from the Storm Prediction Center and write code to do or answer the following.

* List the names of the columns in the data frame.
* What is the last year in the data?
* How many tornadoes had at least one fatality?
* Create a table of the number of tornadoes by EF rating.
* Create a table of the number of tornadoes by state.
* Add a new variable to the data frame called `area` as the product of the path length and path width in units of square meters.
* Map the locations of all the tornadoes with an EF rating of at least 4.

So far we've worked with data stored as vectors. But we often import data as data frames so we need to know how to manipulate data frames in a logical and friendly way.

## A grammar for working with data frames

The functions in the {dplyr} package (Wickham et al. 2020) as part of the {tidyverse} set of packages simplify data munging tasks. But they work only on data frames. 

The function names are _verbs_ so they are easy to remember. Verbs help us to translate our thoughts into code. Recall, functions from packages are made available to the current session with the `library()` function.
```{r}
library(tidyverse)
```

We will look at the verbs one at a time using the `airquality` data frame. The data frame contains air quality measurements taken in New York City between May and September 1973. (`?airquality`). 
```{r chapter2}
dim(airquality)
head(airquality)
```

The columns include `Ozone` (ozone concentration in ppb), `Solar.R` (solar radiation in langleys), `Wind` (wind speed in mph), `Temp` (air temperature in degrees F), `Month`, and `Day`.

We get summary statistics on the values in each column with the `summary()` method.
```{r}
summary(airquality)
```

Note that columns that have missing values are tabulated. For example, there are 37 missing ozone measurements and 7 missing radiation meaurements.

Before we get started we need to talk about pipes and tibbles.

Importantly for literate programming we can apply the `summary()` function using the pipe operator (`%>%`). The pipe operator is a function in the {dplyr} package.
```{r}
airquality %>% summary()
```

We read the pipe as THEN. "airquality data frame THEN summarize".

The pipe operator allows us to string together a bunch of functions that when read makes it easy to understand what was done.

For example, suppose the object of my interest is called `me`. I could apply a function called `wake_up()` in two ways.
```{r, eval=FALSE}
wake_up(me)  # way number one

me %>% wake_up()  # way number two
```

The second way involves a bit more typing but it is easier to read in a literal sense and thus easier to understand.

This becomes clear when stringing together functions. For example, what happens to the result of `me` after the function `wake_up()` has been applied? How about `get_out_of_bed()` and the `get_dressed()`? 

Again, I can apply these functions in two ways.
```{r, eval=FALSE}
get_dressed(get_out_of_bed(wake_up(me)))

me %>%
  wake_up() %>%
  get_out_of_bed() %>%
  get_dressed()
```

The order of the functions usually matters to the outcome. 

Note that I create format that makes it easy to read. Each line is gets only one verb and each line ends with the pipe.

Continuing
```{r, eval=FALSE}
me %>%
  wake_up() %>%
  get_out_of_bed() %>%
  get_dressed() %>%
  make_coffee() %>%
  drink_coffee() %>%
  leave_house()
```

Which is much better in terms of 'readability' then `leave_house(drink_coffee(make_coffee(get_dressed(get_out_of_bed(wake_up(me))))))`.

Tibbles are data frames that make life a little easier. R is an old language, and some things that were useful 10 or 20 years ago now get in your way. To make a data frame a tibble (tabular data frame) use the `as_tibble()` function.
```{r}
class(airquality)
airquality <- as_tibble(airquality)
class(airquality)
```

Click on `airquality` in the environment. It is a data frame. We will use the terms 'tibble' and 'data frame' interchangeably (mostly).

Now we are ready to look at some of the commonly used verbs and how to apply them to the data frame `airquality`.

The function `select()` chooses variables by name. For example, choose the month, day, and temperature columns.
```{r}
airquality %>%
  select(Month, Day, Temp)
```

Suppose we want a new data frame with only the temperature and ozone concentrations.
```{r}
df <- airquality %>%
        select(Temp, Ozone)
df
```

We include an assignment operator (`<-`, left pointing arrow) and an object name (here `df`).

Note: The result of applying a {dplyr} verb is a data frame. From a data frame object to a data frame object.

The function `filter()` chooses observations based on specific values. Suppose we want only the observations where the temperature is at or above 80F.
```{r}
airquality %>%
  filter(Temp >= 80)
```

The result is a data frame with the same 6 columns but now only 73 observations. Each of the observations has a temperature of at least 80F.

Suppose we want a new data frame keeping only observations where temperature is at least 80F AND winds less than 5 mph.
```{r}
df <- airquality %>% 
  filter(Temp >= 80 & Wind < 5)
df
```

The function `arrange()` orders the rows by values given in a particular column.
```{r}
airquality %>%
  arrange(Solar.R)
```

The ordering is from lowest value of radiation to highest value. Here we see the first 10 rows. Note `Month` and `Day` are no longer chronological.

Repeat but order by the value of air temperature.
```{r}
airquality %>%
  arrange(Temp)
```

Importantly we can string the functions together. For example select the variables radiation, wind, and temperature then filter by temperatures above 90F and arrange by temperature.
```{r}
airquality %>%
  select(Solar.R, Wind, Temp) %>%
  filter(Temp > 90) %>%
  arrange(Temp)
```

The result is a data frame with three columns and 14 rows arranged by increasing temperatures above 90F. 

The `mutate()` function adds new columns to the data frame. For example, create a new column called `TempC` as the temperature in degrees Celcius. Also create a column called `WindMS` as the wind speed in meters per second.
```{r}
airquality %>%
  mutate(TempC = (Temp - 32) * 5/9,
         WindMS = Wind * .44704) 
```

The resulting data frame has 8 columns (two new ones) labeled `TempC` and `WindMS`.

On days when the temperature is below 60 F add a column giving the apparent temperature based on the cooling effect of the wind (wind chill) and then arrange from coldest to warmest apparent temperature.
```{r}
airquality %>%
  filter(Temp < 60) %>%
  mutate(TempAp = 35.74 + .6215 * Temp - 35.75 * Wind^.16 + .4275 * Temp * Wind^.16) %>%
  arrange(TempAp)
```

The `summarize()` function reduces (flattens) the data frame based on a function that computes a statistic. For example, to compute the average wind speed during July type
```{r}
airquality %>%
  filter(Month == 7) %>%
  summarize(Wavg = mean(Wind))

airquality %>%
  filter(Month == 6) %>%
  summarize(Tavg = mean(Temp))
```

We've seen functions that compute statistics including `sum()`, `sd()`, `min()`, `max()`, `var()`, `range()`, `median()`. Others include

Summary function  | Description
-----------------:|:-----------
`n()`             | Length of the column
`first()`         | First value of the column
`last()`          | Last value of the column
`n_distinct()`    | Number of distinct values

Find the maximum and median wind speed and maximum ozone concentration values during the month of May. Also determine the number of observations during May.
```{r}
airquality %>%
  filter(Month == 5) %>%
  summarize(Wmax = max(Wind),
            Wmed = median(Wind),
            OzoneMax = max(Ozone),
            NumDays = n())
```

Why do we get an `NA` for `OzoneMax`? How would you fix this?

```{r}
airquality %>%
  filter(Month == 5) %>%
  summarize(Wmax = max(Wind),
            Wmed = median(Wind),
            OzoneMax = max(Ozone, na.rm = TRUE),
            NumDays = n())
```

If we want to summarize separately for each month we use the `group_by()` function. We split the data frame by some variable (e.g., `Month`), apply a function to the individual data frames, and then combine the output.

Find the highest ozone concentration by month. Include the number of observations (days) in the month.
```{r}
airquality %>%
  group_by(Month) %>%
  summarize(OzoneMax =  max(Ozone, na.rm = TRUE),
            NumDays = n())
```

Find the average ozone concentration when temperatures are above and below 70 F. Include the number of observations (days) in the two groups.
```{r}
airquality %>%
  group_by(Temp >= 70) %>%
  summarize(OzoneAvg =  mean(Ozone, na.rm = TRUE),
            NumDays = n())
```

On average ozone concentration is higher on warm days (Temp >= 70 F) days. Said another way; mean ozone concentration statistically depends on temperature.

The mean is a model for the data. The statistical dependency of the mean implies that a model for ozone concentration will likely be improved by including temperature as an explanatory variable.

The important {dplyr} verbs are

Verb          | Description
-------------:|:-----------
`select()`    | selects columns; pick variables by their names
`filter()`    | filters rows; pick observations by their values
`arrange()`   | re-orders the rows
`mutate()`    | creates new columns; create new variables with functions of existing variables
`summarize()` | summarizes values; collapse many values down to a single summary
`group_by()`  | allows operations to be grouped

The syntax of the verb functions in the {dplyr} package are all the same:

Properties
* The first argument is a data frame. This argument is implicit when using the `%>%` operator.
* The subsequent arguments describe what to do with the data frame. We refer to columns in the data frame directly (without using `$`).
* The result is a new data frame

These properties make it easy to chain together many simple lines of code to do something complex.

The five functions form the basis of a grammar for data. At the most basic level, we can only alter a data frame in five useful ways: we can reorder the rows (`arrange()`), pick observations and variables of interest (`filter()` and `select()`), add new variables that are functions of existing variables (`mutate()`), or collapse many values to a summary (`summarise()`).

Example 1: Florida precipitation

Suppose we are interested in whether it is getting wetter or drier in Florida during spring? One way to examine this question is to divide the years into two groups early and late and compute averages.

Import the data, select the month of April (`Apr`) and year (`Year`), group by years > 1960, summarize the two groups of April rainfall with the mean and variance. Make the code work by filling in the blanks ('___').
```{r, eval=FALSE}
read_table(file = "http://myweb.fsu.edu/jelsner/temp/data/FLprecip.txt") %>%
  select(Apr, Year) %>%
  group_by(Year > 1960) %>%
  summarize(Avg = mean(Apr),
            Var = var(Apr))
```

Example 2: US tornadoes

Let's review by considering another data frame. The file `Torn.sf` is a data frame. The last column contains the geometry (genesis as a POINT) of the tornadoes in the format of well-known text [WKT](https://en.wikipedia.org/wiki/Well-known_text) making it a 'simple feature' data frame.

Note the column named `date` is a character vector in the format year-month-day.
```{r}
library(sf)

Torn.sf <- read_sf(dsn = "data/1950-2018-torn-initpoint")
head(Torn.sf$date)
```

Create new columns with the verb function `mutate()`

New columns are created with the `mutate()` function. Here we assign to the object `sfdf` the original data frame but with three new columns. 

1. `Date` as an actual calendar date with the `as.Date()` function, 
2. `Length` as the damage path length in meters and
3. `Width` as the damage path width in meters.

```{r}
sfdf <- Torn.sf %>%
  mutate(Date = as.Date(date),
         Length = len * 1609.34,
         Width = wid * .9144) %>%
  glimpse()
```

The simple feature data frame `sfdf` has the same columns as the original data frame but it now includes the columns `Date`, `Length`, and `Width`. The three new columns are placed in the data frame.

Note here the function `glimpse()` has no arguments. It inherits the data frame `sfdf` through the piping operator.

Rename a column with `rename()`.

To give a column a new name use the `rename()` function (new name = old name). For example to change the name of the column `yr` to `Year` and `mag` to `EF` type
```{r}
sfdf <- sfdf %>%
  rename(Year = yr,
         EF = mag) %>%
  glimpse()
```

The original names of `yr` and `mag` are replaced with `Year` and `EF`.

Select columns with `select()`

The `select()` function chooses specified columns by name to create a new data frame. Here we recycle the `sfdf` name.
```{r}
sfdf <- sfdf %>%
  select(Year, 
         Month = mo, 
         ST = st, 
         EF, 
         Date, 
         Length,
         Width, 
         Fatalities = fat,
         Injuries = inj)
glimpse(sfdf)
```

Note that we also change the name of the column when we use the `=` sign. For example `Month = mo`. `mo` is the original name of the column but it gets changed to `Month`.  

The `select()` function is useful in focusing on a relatively few variables when the data set has many variables. 

We can select columns having common character string names. For example, consider the `us_states` data frame from the {spData} package.
```{r}
library(spData)
head(us_states)
```

We note two columns contain population information `total_pop_10` (population from 2010) and `total_pop_15` (population from 2015).

Here we select columns containing only the population information. Since both start with the character string `total` we use the `starts_with()` function.
```{r}
us_states %>% 
  select(starts_with("total"))
```

Note that the `geometry` column remains.

Filter rows with `filter()`

The `filter()` function selects a subset of the rows of a data frame. The arguments are filtering (subsetting) expressions evaluated using column names of the data frame. For example, we can select all tornadoes recorded during October of 1980.
```{r}
sfdf %>%
  filter(Month == 10, 
         Year == 1980)
```

`Month` and `Year` are column names in `df` that were created with the `rename()` and `select()` functions above.

Q: How would you create a new data frame containing only tornadoes originating in Wisconsin?

```{r}
sfdf %>%
  filter(ST == "WI")
```

Arrange rows with `arrange()`

The function `arrange()` works like `filter()` except that instead of subsetting rows, it reorders them. It takes a data frame, and a set of column names (or more complicated expressions) to order by.

Here we use `desc()` together with `arrange()` to order a column by descending order of fatalities.
```{r}
sfdf %>%
  arrange(desc(Fatalities)) %>%
  glimpse()
```

The deadliest tornado in the record occurred in 2011 killing 158 people many in the city of Joplin, MO. 

Again, note here the `glimpse()` function has no arguments. It inherits the _arranged_ data frame through the piping operator.

If we provide more than one column name, each additional column is used to break ties in the values of the preceding column.
```{r}
sfdf %>%
  arrange(desc(Fatalities), desc(Injuries)) %>%
  glimpse()
```

Pull out a single variable with `pull()`

The function `pull()` pulls out a single variable from the data frame.
```{r}
Fatals <- sfdf %>%
  pull(Fatalities)
head(Fatals)
```

The result is a vector. This is equivalent to `Fatals <- sfdf$Fatalities`.

Summarize values with `summarise()`

The `summarize()` function collapses a data frame to a single row. Here we first create a regular data frame from the simple feature data frame by using the `as.data.frame()` function.
```{r}
df <- as.data.frame(sfdf)

df %>% 
  summarize(mL = median(Length),
            mW = median(Width))
```

The above functions are similar: The first argument is a data frame. This is implicit when using `%>%`. The subsequent arguments describe what to do with it, and you refer to columns in the data frame directly without using `$`. The result is a new data frame (except when using `pull()`).

Together these properties make it easy to chain together multiple simple steps to achieve a complex result. They functions provide the grammar for a data manipulation language. 

The remainder of the language comes from applying the five functions in various order and on various groups.

Grouped operations

The verb functions are powerful when we combine them with the idea of 'group by', repeating the operation individually on groups of observations within the data frame. 

We use the `group_by()` function to describe how to break a data frame down into groups of rows. We can then use the resulting object in the exactly the same functions as above; they'll automatically work 'by group' when the input is a grouped.

Of the five verbs `summarize()` is easy to understand and quite useful.

For example, here we filter the data frame for years starting with 2007 then group by EF rating before summarizing the path length and path width using the `median()` function.
```{r}
df %>%
  filter(Year >= 2007) %>%
  group_by(EF) %>%
  summarize(Count = n(),
            mL = median(Length),
            mW = median(Width))
```

The output is a table perhaps as part of exploratory analysis.

We use `summarize()` with aggregate functions, which take a vector of values, and return a single number. Functions in the {base} package like `min()`, `max()`, `mean()`, `sum()`, `sd()`, `median()`, and `IQR()` can be used. The {dplyr} packages has others:

* `n()`: number of observations in the current group.
* `n_distinct()`: count the number of unique values.
* `first()`, `last()` and `nth()` - these work similarly to `x[1]`, `x[length(x)]`, and `x[n]` but give you more control of the result if the value isn't present.

For example, we use these to find the number of tornadoes by state and the number of months in which there was at least one tornado.
```{r}
df %>%
  group_by(ST) %>%
  summarize(months = n_distinct(Month),
            nT = n())
```

When we group by multiple variables, each summary peels off one level of the grouping. That makes it easy to progressively roll-up a dataset. As an example: how would we determine the number of tornadoes by day of year?

We first use the function `day()` from the {lubridate} package to extract the day of the month from a `Date` object and add it to our data frame. We then use `group_by()` on the month and day. Finally we summarize by counting the number of cases.
```{r}
library(lubridate)
df %>%
  mutate(Day = day(Date)) %>%
  group_by(Month, Day) %>%
  summarize(nT = n())
```

The result is a data frame with the number of tornadoes by day of the year.

There are functions that combine some of the primitives. For example, we can use `tally()` instead of `summarize(nT = n())` or `count()` instead of both `group_by()` and `summarize()`. For example, the following code does the same thing.
```{r}
df %>%
  mutate(Day = day(Date)) %>%
  count(Month, Day)
```

Q: What state had the most tornado fatalities?
```{r}
df %>%
  group_by(ST) %>%
  summarize(nF = sum(Fatalities)) %>%
  arrange(desc(nF))
```

## A grammar for making graphs

The last lesson introduced the functions from the {dplyr} package for manipulating data frames in a grammatically consistent way. This makes data munging easier. The piping operator allows you to write code that is legible. This lesson introduces a grammar for making graphs.

Some of this material is taken from [Sharp Sight Labs](http://sharpsightlabs.com/blog/r-package-think-about-visualization/).

The package {ggplot2} and {dplyr} are part of {tidyverse}, which is a group of packages for data manipulation and visualization. A recent 2016 survey by O'Reilly media showed that {ggplot2} is the most frequently used data visualization tool among employed data scientists. It's popular because it teaches you how to think about visualizing your data. There are a few principles underlying the syntax.

1. Mapping data to aesthetics
2. Layering
3. Building plots iteratively

We make the functions available to our currenct working directory by typing
```{r}
library(ggplot2)
```

First principle: Map data to aesthetics

Consider the following vectors of data. Create a data frame `df2` using the `data.frame` function.
```{r}
foo <- c(-122.419416,-121.886329,-71.05888,-74.005941,-118.243685,-117.161084,-0.127758,-77.036871,
         116.407395,-122.332071,-87.629798,-79.383184,-97.743061,121.473701,72.877656,2.352222,
         77.594563,-75.165222,-112.074037,37.6173)
bar <- c(37.77493,37.338208,42.360083,40.712784,34.052234,32.715738,51.507351,38.907192,39.904211,
         47.60621,41.878114,43.653226,30.267153,31.230416,19.075984,48.856614,12.971599,39.952584,33.448377,55.755826)
zaz <- c(6471,4175,3144,2106,1450,1410,842,835,758,727,688,628,626,510,497,449,419,413,325,318)
df2 <- data.frame(foo, bar, zaz)
glimpse(df2)
head(df2)
```

To make a scatter plot specify the data frame as the first argument in the `ggplot()` function and the `aes()` function as the second argument. The arguments of the `aes()` function are the x and y positions as `foo` and `bar`, respectively. The plot is rendered after adding the geometric object `geom_point()` as a layer.
```{r}
ggplot(data = df2, 
       mapping = aes(x = foo, y = bar)) +
  geom_point()
```

We are mapping data to aesthetic attributes. The _points_ in the scatter plot are geometric objects that we draw. In {ggplot2} lingo, the points are _geoms_. More specifically the points are _point geoms_ that we denote syntactically with the function `geom_point()`.

All geometric objects have aesthetic attributes. Things like:

* x-position
* y-position
* color
* size
* transparency

When we create a data visualization in {ggplot2}, we are creating a mapping between variables in our data and the aesthetic attributes of the geometric objects in our visualization. When we visualize data, we are mapping between variables in our data frame and the aesthetic attributes of the geometric objects in the plot.

In our scatter plot example, when we create this plot, we're mapping `foo` to the x-position aesthetic and we're mapping `bar` to the y-position aesthetic. This may seem trivial `foo` is the x-axis and `bar` is on the y-axis. We can do that in Excel.

But here there is a deeper structure. Theoretically, geometric objects (i.e., the things we draw in a plot, like points) don't just have attributes like position. They have a color, size, etc. 

For example here we map a new variable to the size aesthetic.
```{r}
ggplot(data = df2, 
       mapping = aes(x = foo, y = bar)) +
  geom_point(aes(size = zaz))
```

We changed a scatter plot to a bubble chart by mapping a new variable to the size aesthetic. Any visualization we see can be deconstructed into geom specifications and mapping from data to the aesthetic attributes of the geometric objects.

Second principle: Build plots in layers

The principle of layering is important because to create more advanced visualizations, we often need to:

* Plot multiple datasets, or
* Plot a dataset with additional contextual information contained in a second dataset, or
* Plot summaries or statistical transformations over the raw data

Let's modify the bubble chart by getting additional data and plotting it as a new layer below the bubbles. First get the data from the {maps} package and store it in a new data frame.
```{r}
library(maps)

df3 <- map_data("world") %>%
  glimpse()
```

Plot the new data as a new layer underneath the bubbles.
```{r}
ggplot(data = df2, aes(x = foo, y = bar)) +
  geom_polygon(data = df3, aes(x = long, y = lat, group = group)) +
  geom_point(aes(size = zaz), color = "red")
```

This is the bubble chart from earlier in the post with a new layer added. We transformed a bubble chart into a new visualization called a "dot distribution map," which is much more insightful and much more visually interesting.

The bubble chart is a modified scatter plot and the dot distribution map is a modified bubble chart.

We used two of the data visualization principles (mapping & layering) to build this visualization:

* To create the scatter plot, we mapped `foo` to the x-aesthetic and mapped `bar` to the y-aesthetic
* To create the bubble chart, we mapped a `zaz` to the size-aesthetic
* To create the dot distribution map, we added a layer of polygon data under the bubbles.

Third principle: Iteration

The third principle is about process. The process begins with mapping and layering but ends with iteration when we add layers that modify scales, legends, colors, etc. The syntax of `ggplot` _layerability_ enables and rewards iteration. 

Let's assign to `p1` the output of our plot.
```{r}
p1 <- ggplot(data = df2, 
             mapping = aes(x = foo, y = bar)) +
        geom_polygon(data = df3, 
                     mapping = aes(x = long, y = lat, group = group)) +
        geom_point(aes(size = zaz), color = "red")
```

```{r}
p2 <- p1 + xlab("Longitude") + ylab("Latitude")
p2 <- p2 + scale_size_continuous(name = "Venture Capital Investment\n(USD, Millions)\n")
p2
```

The `facet_wrap()` function is a layer to iterate (repeat) the entire plot conditional on another variable. It is like the `group_by()` function in the data grammar.

Example 1: Tornadoes

We plot the number of tornadoes by year for the state of Kansas. Recall, the data are in the data frame `df`.
```{r}
df %>%
  filter(ST == "KS") %>%
  group_by(Year) %>%
  summarize(nT = n()) %>%
ggplot(mapping = aes(x = Year, y = nT)) +
  geom_line()
```

We create a bar chart indicating the number of tornadoes by EF rating since 2007.
```{r}
df %>%
  filter(Year >= 2007, EF != -9) %>%
  group_by(EF) %>%
  summarize(Count = n()) %>%
  ggplot(aes(x = factor(EF), y = Count)) +
    geom_bar(stat = "identity")
```

Here the argument `stat = "identity"` indicates the data are already tabulated. That is, by default `geom_bar()` tables the data if it is integer, character, or factor.

Improve the bar chart. Make it presentable for publication.
```{r}
df %>%
  filter(Year >= 2007, EF != -9) %>%
  group_by(EF) %>%
  summarize(Count = n()) %>%
ggplot(aes(x = factor(EF), y = Count, fill = Count)) +
  geom_bar(stat = "identity") +
  xlab("EF Rating") + 
  ylab("Number of Tornadoes") +
  scale_fill_continuous(low = 'green', high = 'orange') +
  geom_text(aes(label = Count), vjust = -.5, size = 3) +
  theme_minimal() +
  theme(legend.position = 'none') 
```

Plot a series of bar charts showing the frequency of tornadoes by EF rating for each year since 2005.
```{r}
df %>%
  filter(Year >= 2005, EF != -9) %>%
ggplot(aes(x = factor(EF))) +
  geom_bar() +
  facet_wrap(~ Year)
```

Example 2: Hot days in the city

The data are from the National Climatic Data Center (NCDC). They are [daily data](http://www.ncdc.noaa.gov/cdo-web/datasets) from the National Weather Service Forecast Office in Tallahassee. The observing site is the Tallahassee International Airport (previously the Tallahassee Municipal Airport and Tallahassee Regional Airport). 

Import the data.
```{r}
( TLH.df <- read_csv(file = "http://myweb.fsu.edu/jelsner/temp/data/TLH_DailySummary.csv",
                     na = "-9999") )
```

The warning concerns the column labeled `TOBS`. By default the column type is logical but there are cases when the values are numbers. This can be ignored safely.

The variable of interest is the daily high temperature in the column labeled `TMAX`. The values are in tens of degrees C so the value of 128 is 12.8 C.

Mutate to add new columns giving the temperatures (daily maximum and daily minimum) in degrees F (original measuring unit) and the dates in calendar days. Select only the date and maximum and minimum temperature columns.
```{r}
TLH.df <- TLH.df %>%
  mutate(TmaxF = round(9/5 * TMAX/10 + 32),
         TminF = round(9/5 * TMIN/10 + 32),
         Date = as.Date(as.character(DATE), 
                        format = "%Y%m%d")) %>%
  select(Date, TmaxF, TminF) %>%
glimpse()
```

Note we again use the `as.Date()` function ({base} see `?as.Date`). The format in the data file is a concatenation of a four-digit year, a two-digit month, and a two-digit day. Thus the format argument is `format = "%Y%m%d"`).

Q: Is it getting hotter in Tallahassee? 

Let's compute the annual average high temperature and create a time series graph. 

We use the `year()` function from the {lubridate} package to get a column called `Year`, the `group_by()` function to group by `Year`, and  the `summarize()` function from the {dplyr} package to get the average daily maximum temperature for each year.
```{r}
library(lubridate)

df <- TLH.df %>%
  mutate(Year = year(Date)) %>%
  group_by(Year) %>%
  summarize(AvgT = mean(TmaxF)) %>%
glimpse()
```

We now have a data frame with two columns: `Year` and `AvgT` (annual average daily high temperature in degrees F).

We now use the grammar of graphs to make a plot. We specify the x aesthetic as `Year` and the y aesthetic as the `AvgT`. We include a point layer and a line layer.
```{r}
library(ggplot2)

ggplot(df, aes(x = Year, y = AvgT)) +
  geom_point(size = 3) +
  geom_line() +
  ylab("Average Annual Temperature in Tallahassee, FL (F)")
```

Q: What's wrong? 

Fix and add a trend line layer. Here we go directly to the graph without saving the resulting data frame. That is, we pipe `%>%` the resulting data frame after applying the {dplyr} verbs to the `ggplot()` function. The object in the first argument of the `ggplot()` function is the result (data frame) from the code above.
```{r}
TLH.df %>%
  mutate(Year = year(Date)) %>%
  filter(Year < 2014) %>%
  group_by(Year) %>%
  summarize(AvgT = mean(TmaxF)) %>%
ggplot(aes(x = Year, y = AvgT)) +
  geom_point(size = 3) +
  geom_line() +
  ylab("Average Annual Temperature in Tallahassee, FL (F)") +
  geom_smooth() +
  theme_minimal()
```

Q: Is the frequency of extremely hot days increasing over time? Let's consider a daily high temperature of 100 F and above as extremely hot.

Here we count the number of days at or above 100F using the `summarize()` function together with the `sum()` function on the logical operator `>=`. If a day is missing a high temperature, we remove it with the `na.rm = TRUE` argument in the `sum()` function.
```{r}
TLH.df %>%
  mutate(Year = year(Date)) %>%
  filter(Year < 2014) %>%
  group_by(Year) %>%
  summarize(N100 = sum(TmaxF >= 100, na.rm = TRUE)) %>%
ggplot(aes(x = Year, y = N100, fill = N100)) + 
  geom_bar(stat = 'identity') + 
  scale_fill_continuous(low = 'orange', high = 'red') +
  geom_text(aes(label = N100), vjust = 1.5, size = 3) +
  scale_x_continuous(breaks = seq(1950, 2013, 10)) +
  ylab(expression(paste("Number of days in Tallahassee, FL at or above 100", {}^o, " F"))) +
  theme_minimal() +
  theme(axis.text.x  = element_text(size = 11), legend.position = "none")
```

Histogram of daily high temperature.
```{r}
gTLH <- ggplot(TLH.df, aes(x = TmaxF)) + 
  geom_histogram(binwidth = 1, aes(fill = ..count..)) +
  scale_fill_continuous(low = 'green', high = 'blue') +
  scale_x_continuous(limits = c(30, 120)) +
  scale_y_continuous(limits = c(0, 1000)) +
  ylab("Number of Days") + 
  xlab(expression(paste("Daily High Temperature in Tallahassee, FL (", {}^o, " F)"))) +
  theme_minimal() +
  theme(legend.position = "none")
```

Q: The most common high temperatures are in the low 90s, but there are relatively few 100+ days. Why?

Compare with Las Vegas, Nevada.
```{r}
LVG.df <- read_csv(file = "http://myweb.fsu.edu/jelsner/temp/data/LV_DailySummary.csv",
                     na = "-9999")

LVG.df <- LVG.df %>%
  mutate(TmaxF = round(9/5 * TMAX/10 + 32),
         TminF = round(9/5 * TMIN/10 + 32),
         Date = as.Date(as.character(DATE), 
                        format = "%Y%m%d")) %>%
  select(Date, TmaxF, TminF)

gLVG <- ggplot(LVG.df, aes(x = TmaxF)) + 
  geom_histogram(binwidth = 1, aes(fill = ..count..)) +
  scale_fill_continuous(low = 'green', high = 'blue') +
  scale_x_continuous(limits = c(30, 120)) +
  scale_y_continuous(limits = c(0, 1000)) +
  ylab("Number of Days") + 
  xlab(expression(paste("Daily High Temperature in Las Vegas, NV (", {}^o, " F)"))) +
  theme_minimal() +
  theme(legend.position = "none")

library(patchwork)
gTLH / gLVG
```

Geofacet

Bar plot of the number of tornadoes by month and by state. The code uses the {geofacet} package [@Hafen2020].
```{r}
Torn.sf <- read_sf(dsn = "data/1950-2018-torn-initpoint")

library(geofacet)
library(scales)

Torn.sf %>% 
  as.data.frame() %>%
  group_by(mo, st) %>%
  summarise(nT = n()) %>%
  mutate(month = factor(month.name[mo], levels = month.name)) %>%
#  filter(st != "DC") %>%
ggplot(aes(as.factor(mo), nT)) +
  geom_col(fill = "gray70") +
  facet_geo(~ st, grid = "us_state_grid3", scales = "free_y") +
  scale_x_discrete(breaks = 1:12, labels = c("J", "F", "M", "A", "M", "J", "J", "A", "S", "O", "N", "D")) +
  scale_y_continuous(breaks = function(x) unique(floor(pretty(seq(0, (max(x) + 1) * 1.1))))) +
  labs(title = "Relative peakedness of the tornado season in the Plains",
       caption = "SPC data from 1950-2018") +
  xlab("") + ylab("") +
  theme_minimal() +
  theme(strip.text.x = element_text(size = 6),
        axis.text.x = element_text(size = 6),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank())
```

* https://exts.ggplot2.tidyverse.org/
* Cheat sheets: https://rstudio.com/resources/cheatsheets/
* More examples: https://geocompr.robinlovelace.net/ {spData} package.

## Assignment #1

The object `us_states` from the {spData} package is a simple feature data frame from the U.S. Census Bureau. The variables include the name, region, area, and population.

1. Create a new data frame from `us_states` containing only the population information. (10)
2. Create a new data frame from `us_states`  containing only states from the South region. (10)
3. Create a new data framefrom `us_states` containing only states from the West region having area less then 250,000 square km and a 2015 population more than 5,000,000 residents. Hint: you will need to use `as.numeric(AREA)` to remove the units. (10)
4. What was the total population of the Midwest region in 2010 and 2015? (20)
5. How many states are in each region? (20)
6. Make a bar chart showing the total area in millions of square kilometers by region. Hint: include `stat = "identity"` in the `geom_bar()` function. (25)
7. How much has population density changed between 2010 and 2015 in each state? Calculate the change (in percent relative to population in 2010) for each state. (5)

Extra problem: Make a side-by-side comparison using `facet_wrap()`. Convective vs non-convective atmospheres.