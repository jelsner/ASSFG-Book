---
title: "Machine Learning for Spatial Prediction""
output: html_document
editor_options: 
  chunk_output_type: console
---

Modified from https://geocompr.robinlovelace.net/spatial-cv.html

The lesson uses the following packages:
```{r}
library(sf)
library(raster)
library(mlr)
library(dplyr)
library(parallelMap)
```

Statistical learning is concerned with the use of models for identifying patterns in data and predicting from these patterns. Statistical learning combines methods from statistics and machine learning and its methods can be categorized into supervised and unsupervised techniques. Both are increasingly used in disciplines ranging from physics, biology and ecology to geography and economics.

This lesson focuses on supervised techniques in which there is a training dataset, as opposed to unsupervised techniques such as clustering. Response variables can be binary (such as landslide occurrence), categorical (land use), integer (species richness count) or numeric (soil acidity measured in pH). Supervised techniques model the relationship between such responses — which are known for a sample of observations — and one or more predictors.

The primary aim of machine learning is to make good predictions, as opposed to statistical/Bayesian inference, which is good at helping to understand underlying mechanisms and uncertainties in the data. Machine learning is used in predicting the future behavior of customers, in recommending services (music, movies, what to buy next), in recognizing faces, in autonomous driving, in classifying text classification and in predicting maintenance (infrastructure, industry).

### Landslide susceptibility

Here we show how machine learning is used to predict landslides. The case is based on a dataset of landslide locations in Southern Ecuador described in detail in Muenchow, Brenning, and Richter (2012). A subset of the dataset used in that paper is provided in the {RSAGA} package, which can be loaded as follows.
```{r}
library(RSAGA)

data("landslides", package = "RSAGA")
```

This should load three objects: a data frame named `landslides`, a list named `dem`, and an sf object named `study_area`. `landslides` contains a factor column `lslpts` where `TRUE` corresponds to an observed landslide ‘initiation point’, with the coordinates stored in columns `x` and `y`.

There are 175 landslide points and 1360 non-landslide, as shown by `summary(landslides)`. The 1360 non-landslide points were sampled randomly from the study area, with the restriction that they must fall outside a small buffer around the landslide polygons.

To make the number of landslide and non-landslide points balanced (not sure why they need to be balanced?), let us sample 175 from the 1360 non-landslide points.
```{r}
non_pts <- landslides %>%
  dplyr::filter(lslpts == FALSE)

lsl_pts <- landslides %>%
  dplyr::filter(lslpts == TRUE)

set.seed(11042018)
non_pts_sub <- non_pts %>%
  dplyr::sample_n(size = nrow(lsl_pts))
```

Create smaller landslide dataset (`lsl`).
```{r}
lsl <- non_pts_sub %>%
  dplyr::bind_rows(lsl_pts)
```

The object `dem` is a digital elevation model consisting of two elements: `dem$header`, a list which represents a raster ‘header’, and `dem$data`, a matrix with the altitude of each pixel. `dem` is converted into a raster object with:
```{r}
dem <- raster(dem$data, 
  crs = dem$header$proj4string,
  xmn = dem$header$xllcorner, 
  xmx = dem$header$xllcorner + dem$header$ncols * dem$header$cellsize,
  ymn = dem$header$yllcorner,
  ymx = dem$header$yllcorner + dem$header$nrows * dem$header$cellsize)
```

Make a map.
```{r}
library(tmap)

lsl.sf <- lsl
coordinates(lsl.sf) = ~ x + y
lsl.sf <- st_as_sf(lsl.sf)

tm_shape(dem) +
  tm_raster() +
tm_shape(lsl.sf) +
  tm_dots(col = "lslpts", size = 1)
```

Landslide initiation points (red) and points unaffected by landsliding (white) in Southern Ecuador.

To model landslide susceptibility, we need some predictors. Terrain attributes are frequently associated with landsliding, and these can be computed from the digital elevation model (dem). 

slope: slope angle (°).
cplan: plan curvature (rad m−1) expressing the convergence or divergence of a slope and thus water flow.
cprof: profile curvature (rad m-1) as a measure of flow acceleration, also known as downslope change in slope angle.
elev: elevation (m a.s.l.) as the representation of different altitudinal zones of vegetation and precipitation in the study area.
log10_carea: the decadic logarithm of the catchment area (log10 m2) representing the amount of water flowing towards a location.

Data containing the landslide points, with the corresponding terrain attributes, is provided in the **spDataLarge** package, along with the terrain attribute raster stack from which the values were extracted.

Attach landslide points with terrain attributes
```{r}
install.packages("remotes")

remotes::install_github("Nowosad/spDataLarge")
data("lsl", package = "spDataLarge")

head(lsl)
```

Attach terrain attribute raster stack
```{r}
data("ta", package = "spDataLarge")
```

### Conventional modeling

Before introducing the **mlr** package, an umbrella-package providing a unified interface to dozens of learning algorithms, it is worth taking a look at the conventional modeling interface in R. 

This introduction to supervised statistical learning provides the basis for doing spatial CV, and contributes to a better grasp on the mlr approach presented subsequently.

Supervised learning involves predicting a response variable as a function of predictors. The following command specifies and fits a generalized linear model:
```{r}
fit <- glm(lslpts ~ slope + cplan + cprof + elev + log10_carea,
           family = binomial(),
           data = lsl)

fit
```

The model object `fit`, of class glm, contains the coefficients defining the fitted relationship between response and predictors. 

It can also be used for prediction. This is done with the generic `predict()` method, which in this case calls the function `predict.glm()`. Setting type to response returns the predicted probabilities (of landslide occurrence) for each observation in `lsl`, as illustrated below (see ?predict.glm):
```{r}
pred_glm <- predict(object = fit, type = "response")
head(pred_glm)
```

Spatial predictions are made by applying the coefficients to the predictor rasters. This is done manually or with `raster::predict()`. In addition to a model object (`fit`), this function also expects a raster stack with the predictors named as in the model’s input data frame.

### Making the prediction
```{r}
pred <- raster::predict(ta, model = fit, type = "response")

tm_shape(pred) + 
  tm_raster()
```

Here, when making predictions we neglect spatial autocorrelation since we assume that on average the predictive accuracy remains the same with or without spatial autocorrelation structures.

Spatial prediction maps are one very important outcome of a model. Even more important is how good the underlying model is at making them since a prediction map is useless if the model’s predictive performance is bad. 

The most popular measure to assess the predictive performance of a binomial model is the Area Under the Receiver Operator Characteristic Curve (AUROC). This is a value between 0.5 and 1.0, with 0.5 indicating a model that is no better than random and 1.0 indicating perfect prediction of the two classes. Thus, the higher the AUROC, the better the model’s predictive power. 

The following code chunk computes the AUROC value of the model with `roc()`, which takes the response and the predicted values as inputs. `auc()` returns the area under the curve.
```{r}
library(pROC)

pROC::auc(pROC::roc(lsl$lslpts, fitted(fit)))
```

An AUROC value of 0.83 represents a good fit. However, this is an overoptimistic estimation since we have computed it on the complete dataset. To derive a biased-reduced assessment, we have to use cross-validation and in the case of spatial data should make use of spatial CV.

### Spatial cross-validation

Cross-validation belongs to the family of resampling methods (James et al. 2013). The basic idea is to split (repeatedly) a dataset into training and test sets whereby the training data is used to fit a model which then is applied to the test set. 

Comparing the predicted values with the known response values from the test set (using a performance measure such as the AUROC in the binomial case) gives a bias-reduced assessment of the model’s capability to generalize the learned relationship to independent data. For example, a 100-repeated 5-fold cross-validation means to randomly split the data into five partitions (folds) with each fold being used once as a test set. 

This guarantees that each observation is used once in one of the test sets, and requires the fitting of five models. Subsequently, this procedure is repeated 100 times. Of course, the data splitting will differ in each repetition. Overall, this sums up to 500 models, whereas the mean performance measure (AUROC) of all models is the model’s overall predictive power.

However, geographic data is special. Points close to each other are, generally, more similar than points farther away. This means these points are not statistically independent because training and test points in conventional CV are often too close to each other. 

‘Training’ observations near the ‘test’ observations can provide a kind of ‘sneak preview’: information that should be unavailable to the training dataset. To alleviate this problem ‘spatial partitioning’ is used to split the observations into spatially disjointed subsets (using the observations’ coordinates in a k-means clustering; A. Brenning (2012b); second row of Figure 11.3).

[Partioning](13_partitioning.png)

This partitioning strategy is the only difference between spatial and conventional CV. As a result, spatial CV leads to a bias-reduced assessment of a model’s predictive performance, and hence helps to avoid overfitting.

### Spatial CV with mlr

There are dozens of packages for statistical learning, as described for example in the CRAN machine learning task view. Getting acquainted with each of these packages, including how to undertake cross-validation and hyperparameter tuning, can be time-consuming. Comparing model results from different packages can be even more laborious. 

The {mlr} package was developed to address these issues. It acts as a ‘meta-package’, providing a unified interface to popular supervised and unsupervised statistical learning techniques including classification, regression, survival analysis and clustering. 

The {mlr} modeling process consists of three main stages. First, a task specifies the data (including response and predictor variables) and the model type (such as regression or classification). Second, a learner defines the specific learning algorithm that is applied to the created task. Third, the resampling approach assesses the predictive performance of the model, i.e., its ability to generalize to new data).

#### Generalized linear model

To implement a GLM using the functions from the {mlr} package, we create a task containing the landslide data. Since the response is binary (two-category variable), we create a classification task with the `makeClassifTask()` function (for regression tasks, use `makeRegrTask()`, see `?makeRegrTask` for other task types). 

The first argument of these `make*()` functions is data. The target argument expects the name of a response variable and positive determines which of the two factor levels of the response variable indicate the landslide initiation point (in our case this is TRUE). 

All other variables of the `lsl` dataset will serve as predictors except for the coordinates (see the result of getTaskFormula(task) for the model formula). For spatial CV, the coordinates parameter is used which expects the coordinates as a xy data frame.
```{r}
library(mlr)

coords <- lsl[, c("x", "y")] # coordinates needed for the spatial partitioning
data <- dplyr::select(lsl, -x, -y) # select response and predictors to use in the modeling
task <- makeClassifTask(data = data, 
                        target = "lslpts",
                        positive = "TRUE", 
                        coordinates = coords)
```

The function `makeLearner()` determines the statistical learning method to use. All classification learners start with `classif.` and all regression learners with `regr.` (see `?makeLearners` for details).

Sample of available learners for binomial tasks in the mlr package.

classif.binomial	Binomial Regression	binomial	stats
classif.featureless	Featureless classifier	featureless	mlr
classif.fnn	Fast k-Nearest Neighbour	fnn	FNN
classif.gausspr	Gaussian Processes	gausspr	kernlab
classif.knn	k-Nearest Neighbor	knn	class
classif.ksvm	Support Vector Machines	ksvm	kernlab

This yields all learners able to model two-class problems (landslide yes or no). We opt for the binomial classification method used and implemented as `classif.binomial` in mlr. 

Additionally, we must specify the link-function, logit in this case, which is also the default of the binomial() function. predict.type determines the type of the prediction with prob resulting in the predicted probability for landslide occurrence between 0 and 1 (this corresponds to `type = response` in predict.glm).
```{r}
lrn <- makeLearner(cl = "classif.binomial",
                  link = "logit",
                  predict.type = "prob",
                  fix.factors.prediction = TRUE)
```

To find out from which package the specified learner is taken and how to access the corresponding help pages, we can run:
```{r}
getLearnerPackages(lrn)
helpLearner(lrn)
```

The set-up steps for modeling with {mlr} may seem tedious. But this single interface provides access to the 150+ learners shown by `listLearners()`. It would be far more tedious to learn the interface for each learner!

Further advantages are simple parallelization of resampling techniques and the ability to tune machine learning hyperparameters. Also (spatial) resampling in {mlr} is straightforward, requiring only two more steps: specifying a resampling method and running it. 

We will use a 100-repeated 5-fold spatial CV: five partitions will be chosen based on the provided coordinates in our task and the partitioning will be repeated 100 times:65
```{r}
perf_level <- makeResampleDesc(method = "SpRepCV", 
                               folds = 5, 
                               reps = 100)
```

To execute the spatial resampling, we run `resample()` using the specified learner, task, resampling strategy and of course the performance measure, here the AUROC. This takes some time (around 10 seconds on a modern laptop) because it computes the AUROC for 500 models. Setting a seed ensures the reproducibility of the obtained result and will ensure the same spatial partitioning when re-running the code.
```{r}
set.seed(012348)
sp_cv <- mlr::resample(learner = lrn, task = task,
                      resampling = perf_level, 
                      measures = mlr::auc)
```

The output of the preceding code chunk is a bias-reduced assessment of the model’s predictive performance, as illustrated in the following code chunk.

Summary statistics of the 500 models
```{r}
summary(sp_cv$measures.test$auc)
mean(sp_cv$measures.test$auc)
```

To put these results in perspective, let us compare them with AUROC values from a 100-repeated 5-fold non-spatial cross-validation. As expected, the spatially cross-validated result yields lower AUROC values on average than the conventional cross-validation approach, underlining the over-optimistic predictive performance due to spatial autocorrelation of the latter.

Machine learning, more specifically the field of predictive modeling, is primarily concerned with minimizing the error of a model or making the most accurate predictions possible, at the expense of explainability. In applied machine learning we will borrow, reuse and steal algorithms from many different fields, including statistics and use them towards these ends.

#### Support vector machines (SVM)

This section introduces support vector machines (SVM) for the same purpose. Random forest models might be more popular than SVMs; however, the positive effect of tuning hyperparameters on model performance is much more pronounced in the case of SVMs. Since (spatial) hyperparameter tuning is the major aim of this section, we will use an SVM.

SVMs search for the best possible ‘hyperplanes’ to separate classes (in a classification case) and estimate ‘kernels’ with specific hyperparameters to allow for non-linear boundaries between classes (James et al. 2013). Hyperparameters should not be confused with coefficients of parametric models, which are sometimes also referred to as parameters. 

Coefficients can be estimated from the data, while hyperparameters are set before the learning begins. Optimal hyperparameters are usually determined within a defined range with the help of cross-validation methods. This is called hyperparameter tuning.

Some SVM implementations such as that provided by kernlab allow hyperparameters to be tuned automatically, usually based on random sampling. This works for non-spatial data but is of less use for spatial data where ‘spatial tuning’ should be undertaken.

Before defining spatial tuning, we will set up the {mlr} building blocks, introduced for the SVM. The classification task remains the same, hence we can simply reuse the task object created earlier. Learners implementing SVM can be found using `listLearners()` as follows:

```{r}
lrns <- listLearners(task, warn.missing.packages = FALSE)
filter(lrns, grepl("svm", class)) %>% 
  dplyr::select(class, name, short.name, package)
```

Of the options illustrated above, we will use `ksvm()` from the **kernlab** package (Karatzoglou et al. 2004). To allow for non-linear relationships, we use the popular radial basis function (or Gaussian) kernel which is also the default of `ksvm()`.
```{r}
lrn_ksvm = makeLearner("classif.ksvm",
                        predict.type = "prob",
                        kernel = "rbfdot")
```

The next stage is to specify a resampling strategy. Again we will use a 100-repeated 5-fold spatial CV.

```{r}
perf_level <- makeResampleDesc(method = "SpRepCV", 
                               folds = 5, 
                               reps = 100)
```

Note that this is the exact same code as used for the GLM. We have simply repeated it here as a reminder.

The next step is to tune the hyperparameters. Using the same data for the performance assessment and the tuning would potentially lead to overoptimistic results (Cawley and Talbot 2010). This can be avoided using nested spatial CV.

Schematic of hyperparameter tuning and performance estimation levels in CV (from Schratz et al. 2018).
[Hyperparameter tuning](13_cv.png)

This means that we split each fold again into five spatially disjoint subfolds which are used to determine the optimal hyperparameters. To find the optimal hyperparameter combination, we fit 50 models (ctrl object in the code chunk below) in each of these subfolds with randomly selected values for the hyperparameters C and Sigma. The random selection of values C and Sigma is additionally restricted to a predefined tuning space (ps object). The range of the tuning space was chosen with values recommended in the literature (Schratz et al. 2018).

```{r}
# five spatially disjoint partitions
tune_level <- makeResampleDesc("SpCV", iters = 5)
# use 50 randomly selected hyperparameters
ctrl <- makeTuneControlRandom(maxit = 50)
# define the outer limits of the randomly selected hyperparameters
ps <- makeParamSet(
  makeNumericParam("C", lower = -12, upper = 15, trafo = function(x) 2^x),
  makeNumericParam("sigma", lower = -15, upper = 6, trafo = function(x) 2^x)
  )
```

The next stage is to modify the learner `lrn_ksvm` in accordance with all the characteristics defining the hyperparameter tuning with the `makeTuneWrapper()` function.
```{r}
wrapped_lrn_ksvm <- makeTuneWrapper(learner = lrn_ksvm, 
                                    resampling = tune_level,
                                    par.set = ps,
                                    control = ctrl, 
                                    show.info = TRUE,
                                    measures = mlr::auc)
```

The mlr is now set-up to fit 250 models to determine optimal hyperparameters for one fold. Repeating this for each fold, we end up with 1250 (250 * 5) models for each repetition. Repeated 100 times means fitting a total of 125,000 models to identify optimal hyperparameters. 

These are used in the performance estimation, which requires the fitting of another 500 models (5 folds * 100 repetitions). To make the performance estimation processing chain even clearer, let us write down the commands we have given to the computer:

Performance level: split the dataset into five spatially disjoint (outer) subfolds.
Tuning level: use the first fold of the performance level and split it again spatially into five (inner) subfolds for the hyperparameter tuning. Use the 50 randomly selected hyperparameters in each of these inner subfolds, i.e., fit 250 models.
Performance estimation: Use the best hyperparameter combination from the previous step (tuning level) and apply it to the first outer fold in the performance level to estimate the performance (AUROC).
Repeat steps 2 and 3 for the remaining four outer folds.
Repeat steps 2 to 4, 100 times.

The process of hyperparameter tuning and performance estimation is computationally intensive. Model runtime can be reduced with parallelization, which can be done in a number of ways, depending on the operating system.
Before starting the parallelization, we ensure that the processing continues even if one of the models throws an error by setting `on.learner.error = warn`. This avoids the process stopping just because of one failed model, which is desirable on large model runs. To inspect the failed models once the processing is completed, we dump them:
```{r}
configureMlr(on.learner.error = "warn", on.error.dump = TRUE)
```

To start the parallelization, we set the mode to multicore which will use `mclapply()` in the background on a single machine in the case of a Unix-based operating system. Equivalenty, `parallelStartSocket()` enables parallelization under Windows. level defines the level at which to enable parallelization, with `mlr.tuneParams()` determining that the hyperparameter tuning level should be parallelized (see lower left part of Figure 11.6, ?parallelGetRegisteredLevels, and the mlr parallelization tutorial for details). We will use half of the available cores (set with the cpus parameter), a setting that allows possible other users to work on the same high performance computing cluster in case one is used (which was the case when we ran the code). Setting mc.set.seed to TRUE ensures that the randomly chosen hyperparameters during the tuning can be reproduced when running the code again. Unfortunately, mc.set.seed is only available under Unix-based systems.
```{r}
library(parallelMap)
if (Sys.info()["sysname"] %in% c("Linux", "Darwin")) {
parallelStart(mode = "multicore", 
              # parallelize the hyperparameter tuning level
              level = "mlr.tuneParams", 
              # just use half of the available cores
              cpus = round(parallel::detectCores() / 2),
              mc.set.seed = TRUE)
}

if (Sys.info()["sysname"] == "Windows") {
  parallelStartSocket(level = "mlr.tuneParams",
                      cpus =  round(parallel::detectCores() / 2))
}
```

Now we are set up for computing the nested spatial CV. Using a seed allows us to recreate the exact same spatial partitions when re-running the code. Specifying the resample() parameters follows the exact same procedure as presented when using a GLM, the only difference being the extract argument. This allows the extraction of the hyperparameter tuning results which is important if we plan follow-up analyses on the tuning. After the processing, it is good practice to explicitly stop the parallelization with parallelStop(). Finally, we save the output object (result) to disk in case we would like to use it another R session. Before running the subsequent code, be aware that it is time-consuming: the 125,500 models took ~1/2hr on a server using 24 cores (see below).

```{r}
set.seed(12345)
result <- mlr::resample(learner = wrapped_lrn_ksvm, 
                       task = task,
                       resampling = perf_level,
                       extract = getTuneResult,
                       measures = mlr::auc)
```
```{r}
# stop parallelization
parallelStop()
```
```{r}
# save your result, e.g.:
saveRDS(result, "svm_sp_sp_rbf_50it.rds")
```
In case you do not want to run the code locally, we have saved a subset of the results in the book’s GitHub repo. They can be loaded as follows:
```{r}
result <- readRDS("svm_sp_sp_rbf_50it.rds")
```
Note that runtime depends on many aspects: CPU speed, the selected algorithm, the selected number of cores and the dataset.

### Exploring the results
Runtime in minutes
```{r}
round(result$runtime / 60, 2)
```
Even more important than the runtime is the final aggregated AUROC: the model’s ability to discriminate the two classes.

Final aggregated AUROC 
```{r}
result$aggr
mean(result$measures.test$auc)
```

It appears that the GLM (aggregated AUROC was 0.78) is slightly better than the SVM in this specific case. However, using more than 50 iterations in the random search would probably yield hyperparameters that result in models with a better AUROC (Schratz et al. 2018). On the other hand, increasing the number of random search iterations would also increase the total number of models and thus runtime.

The estimated optimal hyperparameters for each fold at the performance estimation level can also be viewed. The following command shows the best hyperparameter combination of the first fold of the first iteration (recall this results from the first 5 * 50 model runs):

Winning hyperparameters of tuning step, i.e. the best combination out of 50 * 5 models
```{r}
result$extract[[1]]$x
```

The estimated hyperparameters have been used for the first fold in the first iteration of the performance estimation level which resulted in the following AUROC value:
```{r}
result$measures.test[1, ]
```

So far spatial CV has been used to assess the ability of learning algorithms to generalize to unseen data. For spatial predictions, one would tune the hyperparameters on the complete dataset.

### Summary

This chapter used cross-validation to assess predictive performance of various models. As described in Section 11.4, observations with spatial coordinates may not be statistically independent due to spatial autocorrelation, violating a fundamental assumption of cross-validation. Spatial CV addresses this issue by reducing bias introduced by spatial autocorrelation.

The {mlr} package facilitates (spatial) resampling techniques in combination with the most popular statistical learning techniques including linear regression, semi-parametric models such as generalized additive models and machine learning techniques such as random forests, SVMs, and boosted regression trees (Bischl et al. 2016; Schratz et al. 2018). Machine learning algorithms often require hyperparameter inputs, the optimal ‘tuning’ of which can require thousands of model runs which require large computational resources, consuming much time, RAM and/or cores. mlr tackles this issue by enabling parallelization.

Machine learning overall, and its use to understand spatial data, is a large field and this chapter has provided the basics, but there is more to learn. We recommend the following resources in this direction:

The mlr tutorials on Machine Learning in R and Handling of spatial Data. An academic paper on hyperparameter tuning (Schratz et al. 2018). In case of spatio-temporal data, one should account for spatial and temporal autocorrelation when doing CV (Meyer et al. 2018).